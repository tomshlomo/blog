{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Efficient Leave One Out Cross Validation\"\n",
        "author: \"Tom Shlomo\"\n",
        "date: \"2024-02-27\"\n",
        "description: TBD\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.\n",
        "\n",
        " Leave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer  accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.\n",
        "\n",
        "For linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it's `RidgeCV` estimator. Notably, this same trick extends beyond these linear models to any problem involving quadratically regularized least squares regressionâ€”a fact not widely recognized.\n",
        "\n",
        "Taking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.\n",
        "\n",
        "In this blog post, we derive the formulas for efficient LOOCV calculations and demonstrate these results on a practical example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notation\n",
        "We use $n$ to denote the number of samples in the training dataset.\n",
        "\n",
        "The $m$-dimensional feature vectors are represented as $x_1$ to $x_n$, forming the rows of matrix $X$.\n",
        "\n",
        "Targets are denoted as $y_1$ to $y_n$, forming the vector $y$. The model's prediction for the $i$-th training sample is $\\hat{y}_i = x_i^T \\theta$, where $\\theta$ is the vector of coefficients.\n",
        "$\\hat{y} = X \\theta$ is the vector containing all predictions.\n",
        "\n",
        "We fit $\\theta$ to the training data by minimizing the combined loss and regularization terms:\n",
        "$$\n",
        "\\theta := \\arg\\min_{\\theta'} f(\\theta').\n",
        "$$ {#eq-theta-def}\n",
        "where\n",
        "$$\n",
        "f(\\theta') := \\sum_{i=1}^{n} l(x_i^T \\theta'; y_i) + r(\\theta').\n",
        "$$\n",
        "Here, $l(\\hat{y}_i; y_i)$ represents the loss function, quantifying the difference between the prediction $\\hat{y}$ and the true target $y_i$,\n",
        "while $r$ is the regularization function.\n",
        "We will assume $l$ (as a function of $\\hat{y}_i$) and $r$ are convex and twice differentiable.\n",
        "Special cases of this model include\n",
        "ordinary least squares ($l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2$, $r(\\theta') = 0$), \n",
        "ridge regression ($l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2$, $r(\\theta') = \\alpha \\| \\theta' \\|^2$), \n",
        "logistic regression ($l(\\hat{y}_i;y_i) = \\log \\left( 1 + e^{-y_i \\hat{y}_i}\\right)$ with $y_i \\in \\{ -1, 1\\}$),\n",
        "and Poisson regression ($l(\\hat{y}_i;y_i) = y_i \\hat{y}_i - e^{\\hat{y}_i}$).\n",
        "\n",
        "To denote the coefficients obtained by excluding the $j$-th example, we use $\\theta^{(j)}$:\n",
        "$$\n",
        "\\theta^{(j)} = \\arg\\min_{\\theta'} f^{(j)} (\\theta')\n",
        "$$\n",
        "where\n",
        "$$ f^{(j)}(\\theta') := \\sum_{i \\neq j} l(x_i^T \\theta'; y_i) + r(\\theta') $$\n",
        "Similarly, $X^{(j)}$ and $y^{(j)}$, are employed to represent $X$ and $y$ with the $j$-th row removed, respectively.\n",
        "We denote by $\\tilde{y}_j$ the predicted label for sample $j$ when it is left out:\n",
        "$$\n",
        "\\tilde{y}_j := x_j ^T \\theta^{(j)}\n",
        "$$ {#eq-y-tilde-j-def}\n",
        "Our goal is calculating $\\tilde{y}_j$, for all $j$, efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# LOOCV with the sum of squares loss and quadratic regularization\n",
        "In scenarios where the loss function is the sum of squares loss\n",
        "$$\n",
        "l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2,\n",
        "$$\n",
        "and the regularizer is quadaratic\n",
        "$$\n",
        "r(\\theta') = \\theta'^T R \\theta'\n",
        "$$\n",
        "where $R$ is an $m \\times m$ semi-positive definite matrix,\n",
        "the solution to optimization problem @eq-theta-def is obtained by solving the linear equation [^1]:\n",
        "$$ \n",
        "A \\theta = b. \n",
        "$$ {#eq-theta-solve}\n",
        "where\n",
        "\\begin{align*}\n",
        "    A &:= X^T X + R \\\\\n",
        "    b &:= X^T y.\n",
        "\\end{align*}\n",
        "\n",
        "[^1]: I am deliberately avoiding writing $\\theta = A^{-1} b$, as $A$ does not have to be invertible for this equation to have a solution, and it allows me to avoid the usual \"assuming full rank\" caveats people tend to use here.\n",
        "Furthermore, it can mislead people into implementations like `np.linalg.inv(A) @ b`, which are less stable and efficient than implementations like `np.linalg.solve(A, b)`.\n",
        "\n",
        "Similarly, obtaining $\\theta^{(j)}$ requires solving\n",
        "$$ \n",
        "A^{(j)} \\theta^{(j)} = b^{(j)}.\n",
        "$$ {#eq-theta-j-solve}\n",
        "where\n",
        "\\begin{align*}\n",
        "    A^{(j)} &:= X^{(j)T} X^{(j)} + R \\\\\n",
        "    b^{(j)} &:= X^{(j)T} y^{(j)}.\n",
        "\\end{align*}\n",
        "Forming and solving @eq-theta-j-solve for each $j$ has a time complexity of $O(m^3 + n m^2)$.\n",
        "Therefore, in a naive implementation, the overall complexity of LOOCV becomes $O(n m^3 + n^2 m^2)$, posing a significant computational challenge, particularly when $n$ is large.\n",
        "\n",
        "The key idea behind efficient LOOCV lies in leveraging the solution for @eq-theta-solve to calculate the solution for @eq-theta-j-solve.\n",
        "We will utilize an important idea from computational linear algebra: \n",
        "even though the complexity of solving a single $m$ by $m$ equation is $O(m^3)$, the complexity of solving $n$ such equations is not $O(nm^3)$, but $O(m^3 + nm^2)$, if all the equations share the same matrix.\n",
        "<!-- the time required to solve multiple $m$ by $m$ equations that share the same matrix is almost identical to the time it takes to solve a single $m$ by $m$ equation. -->\n",
        "Specifically, we will solve, in additional to @eq-theta-solve, the following $n$ equations:\n",
        "$$\n",
        "A t_j = x_j.\n",
        "$$\n",
        "\n",
        "We start by noting that\n",
        "\\begin{align*}\n",
        "X^TX &= X^{(j)^T} X^{(j)} + x_j x_j^T    \\\\\n",
        "X^Ty &= X^{(j)^T} y^{(j)} + x_j y_j,\n",
        "\\end{align*}\n",
        "so we can write @eq-theta-j-solve like so:\n",
        "$$\n",
        "(A - x_j x_j^T) \\theta^{(j)} = b - x_j y_j.\n",
        "$$\n",
        "The usual way forward is employing Sherman-Morrison formula, solve for $\\theta^{(j)}$ and substitute in @eq-y-tilde-j-def to get an expression for $\\tilde{y}$.\n",
        "But there is a better[^2] way: rewrite @eq-theta-j-solve as\n",
        "\\begin{align*}\n",
        "    A \\theta^{(j)} - x_j \\tilde{y}_j &= b - x_j y_j \\\\\n",
        "    \\tilde{y}_j &= x_j ^T \\theta^{(j)}\n",
        "\\end{align*}\n",
        "so instead of a single equation with one unknown ($\\theta^{(j)}$),\n",
        "we now have two equations with two \n",
        " unknowns ($\\theta^{(j)}$ and $\\tilde{y}_j$).\n",
        " At first this seems more complicated, but notice that since the coefficient of $\\theta^{(j)}$ in the first equation is $A$, we can eliminate it:\n",
        "\\begin{align*}\n",
        "\\theta^{(j)}  &= A^{-1} ( b - x_j y_j + x_j \\tilde{y}_j ) \\\\\n",
        "&= \\theta - t_j (  y_j - \\tilde{y}_j )\n",
        "\\end{align*}\n",
        "substituting in the bottom equation, we can solve for $\\tilde{y}_j$:\n",
        "\\begin{align*}\n",
        "\\tilde{y}_j &= x_j ^T \\left( \\theta - t_j (  y_j - \\tilde{y}_j ) \\right)\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\hat{y}_j - h_j (y_j - \\tilde{y}_j)\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\frac{\\hat{y}_j - h_j y_j}{1-h_j}\n",
        "% \\\\\n",
        "% \\tilde{y}_j &= \\frac{\\hat{y}_j -h_j \\hat{y}_j + h_j \\hat{y}_j - h_j y_j}{1-h_j}\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\hat{y}_j + \\frac{h_j }{1-h_j} \\left( \\hat{y}_j - y_j \\right)\n",
        "% \\\\\n",
        "% \\tilde{y}_j &= \\frac{\\hat{y}_j - y_j}{1-h_j} + y_j\n",
        "\\end{align*}\n",
        "where \n",
        "$$\n",
        "h_j := x_j ^T t_j.\n",
        "$$\n",
        "\n",
        "::: {.callout-note appearance=\"simple\"}\n",
        "## Reminder\n",
        "$y_j$ is the true label.\\\n",
        "$\\hat{y}_j$ is the prediction using all the data.\\\n",
        "$\\tilde{y}_j$ is the leave-one-out prediction.\n",
        ":::\n",
        "\n",
        "That's it! we got an expression for $\\tilde{y}_j$ that doesn't require inverting any matrix other than $A$.\n",
        "It also has a nice interpretation: the difference between the prediction and the LOO prediction is the difference between the prediction an the true label, \"amplified\" by $\\frac{h_j }{1-h_j}$.\n",
        "\n",
        "[^2]: Translates better into code, as we get the expression for $\\tilde{y}_j$ directly, without going through an expression for $\\theta^{(j)}$ first.\n",
        "I also think Sherman-Morisson is a bit too strong here and can obscure some insights, so it's nice to avoid it. But actually the other way is just halfway it's proof (see for example [here](https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python implementation\n",
        "The method above translates very well into code.\\\n",
        "Let's implement an sklearn style estimator with the usual fit and predict methods,\n",
        "and a method to get $\\tilde{y}$, the leave-one-out predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Self\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "Array = np.ndarray\n",
        "\n",
        "\n",
        "class LinearRegressionWithQuadraticRegularization:\n",
        "    def __init__(self, R: Array) -> None:\n",
        "        self.R = R\n",
        "\n",
        "    def fit(self, X: Array, y: Array) -> Self:\n",
        "        A = X.T @ X + self.R\n",
        "        b = X.T @ y\n",
        "        self.theta_ = scipy.linalg.solve(\n",
        "            A,\n",
        "            b,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: Array) -> Array:\n",
        "        return X @ self.theta_\n",
        "\n",
        "    def fit_loocv_predict(self, X: Array, y: Array) -> Array:\n",
        "        A = X.T @ X + self.R\n",
        "        b = X.T @ y\n",
        "        temp = scipy.linalg.solve(\n",
        "            A,\n",
        "            np.vstack([b, X]).T,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        self.theta_ = temp[:, 0]\n",
        "        t = temp[:, 1:]\n",
        "        h = np.einsum(\"ij,ji->i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n",
        "        y_hat = self.predict(X)\n",
        "        return y_hat + (h / (1 - h)) * (y_hat - y)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that our method for calculating the leave-one-out predictions is correct on random data, and compare it's run time to the usual leave-one-out procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from time import time\n",
        "\n",
        "def standard_loocv(model, X: Array, y: Array) -> Array:\n",
        "    y_tilde = np.empty_like(y)\n",
        "    for i, (train_index, test_index) in enumerate(LeaveOneOut().split(X)):\n",
        "        X_loo = X[train_index, :]\n",
        "        y_loo = y[train_index]\n",
        "        model.fit(X_loo, y_loo)\n",
        "        y_tilde[i] = model.predict(X[test_index, :])[0]\n",
        "    return y_tilde\n",
        "\n",
        "def gen_random_data(n, m):\n",
        "    rng = np.random.default_rng(42)\n",
        "    X = rng.standard_normal((n, m))\n",
        "    L = rng.standard_normal((m, m))\n",
        "    theta = L @ rng.standard_normal(m)\n",
        "    y = X @ theta + rng.standard_normal(n)\n",
        "    R = L @ L.T  # random positive definite matrix\n",
        "    return X, y, R\n",
        "\n",
        "X, y, R = gen_random_data(n=100, m=10)\n",
        "model = LinearRegressionWithQuadraticRegularization(R=R)\n",
        "print(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good, the two methods to calculate $\\tilde{y}$ give the same result. Let's also compare the runtime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%timeit model.fit_loocv_predict(X, y) \n",
        "%timeit standard_loocv(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice, a significant speedup. But that's quite fast to begin with. \n",
        "Let's increase `n` and `m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y, R = gen_random_data(n=500, m=50)\n",
        "model = LinearRegressionWithQuadraticRegularization(R=R)\n",
        "print(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')\n",
        "%timeit model.fit_loocv_predict(X, y) \n",
        "%timeit standard_loocv(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the standard LOOCV procedure is faster!\n",
        "Our LOOCV should be faster, in theory, so this is probably due to some python inefficiencies, not the algorithm itself.\n",
        "Let's try to improve by using JAX's just-in-time compilation feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "\n",
        "\n",
        "class JitLinearRegressionWithQuadraticRegularization:\n",
        "    def __init__(self, R: Array) -> None:\n",
        "        self.R = R\n",
        "\n",
        "    def fit(self, X: Array, y: Array) -> Self:\n",
        "        self.theta_ = self._fit(X, y, self.R)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: Array) -> Array:\n",
        "        return self._predict(X, self.theta_)\n",
        "\n",
        "    def fit_loocv_predict(self, X: Array, y: Array) -> Array:\n",
        "        self.theta_, y_tilde = self._fit_loocv_predict2(X, y, self.R)\n",
        "        return y_tilde\n",
        "    \n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _fit(X, y, R):\n",
        "        return jax.scipy.linalg.solve(\n",
        "            X.T @ X + R, \n",
        "            X.T @ y,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _predict(X, theta):\n",
        "        return X @ theta\n",
        "\n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _fit_loocv_predict(X, y, R):\n",
        "        t = jax.scipy.linalg.solve(\n",
        "            X.T @ X + R,\n",
        "            X.T,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        theta = t @ y\n",
        "        h = jax.numpy.einsum(\"ij,ji->i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n",
        "        y_hat = X @ theta\n",
        "        return theta, y_hat + (h / (1 - h)) * (y_hat - y)\n",
        "    \n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _fit_loocv_predict2(X, y, R):\n",
        "        temp = jax.scipy.linalg.solve(\n",
        "            X.T @ X + R,\n",
        "            jax.numpy.vstack([X.T @ y, X]).T,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        theta = temp[:, 0]\n",
        "        t = temp[:, 1:]\n",
        "        h = jax.numpy.einsum(\"ij,ji->i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n",
        "        y_hat = X @ theta\n",
        "        return theta, y_hat + (h / (1 - h)) * (y_hat - y)\n",
        "    \n",
        "X, y, R = gen_random_data(n=500, m=50)\n",
        "model = JitLinearRegressionWithQuadraticRegularization(R=R)\n",
        "print(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')\n",
        "%timeit model.fit_loocv_predict(X, y) \n",
        "%timeit standard_loocv(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Much better!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
