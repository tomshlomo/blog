{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Efficient Leave One Out Cross Validation\"\n",
        "author: \"Tom Shlomo\"\n",
        "date: \"2024-02-27\"\n",
        "description: TBD\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.\n",
        "\n",
        " Leave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer  accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.\n",
        "\n",
        "For linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it's `RidgeCV` estimator. Notably, this same trick extends beyond these linear models to any problem involving quadratically regularized least squares regressionâ€”a fact not widely recognized.\n",
        "\n",
        "Taking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.\n",
        "\n",
        "In this blog post, we derive the formulas for efficient LOOCV calculations and demonstrate these results on a practical example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notation\n",
        "We use $n$ to denote the number of samples in the training dataset.\n",
        "\n",
        "The $m$-dimensional feature vectors are represented as $x_1$ to $x_n$, forming the rows of matrix $X$.\n",
        "\n",
        "Targets are denoted as $y_1$ to $y_n$, forming the vector $y$. The model's prediction for the $i$-th training sample is $\\hat{y}_i = x_i^T \\theta$, where $\\theta$ is the vector of coefficients.\n",
        "$\\hat{y} = X \\theta$ is the vector containing all predictions.\n",
        "\n",
        "We fit $\\theta$ to the training data by minimizing the combined loss and regularization terms:\n",
        "$$\n",
        "\\theta := \\arg\\min_{\\theta'} f(\\theta').\n",
        "$$ {#eq-theta-def}\n",
        "where\n",
        "$$\n",
        "f(\\theta') := \\sum_{i=1}^{n} l(x_i^T \\theta'; y_i) + r(\\theta').\n",
        "$$\n",
        "Here, $l(\\hat{y}_i; y_i)$ represents the loss function, quantifying the difference between the prediction $\\hat{y}$ and the true target $y_i$,\n",
        "while $r$ is the regularization function.\n",
        "We will assume $l$ (as a function of $\\hat{y}_i$) and $r$ are strictly convex and twice differentiable.\n",
        "Special cases of this model includes \n",
        "ordinary least squares ($l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2$, $r(\\theta') = 0$), \n",
        "ridge regression ($l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2$, $r(\\theta') = 0$), \n",
        "logistic regression ($l(\\hat{y}_i;y_i) = \\log \\left( 1 + e^{-y_i \\hat{y}_i}\\right)$ with $y_i \\in \\{ -1, 1\\}$),\n",
        "and Poisson regression ($l(\\hat{y}_i;y_i) = y_i \\hat{y}_i - e^{\\hat{y}_i}$).\n",
        "\n",
        "To denote the coefficients obtained by excluding the $j$-th example, we use $\\theta^{(j)}$:\n",
        "$$\n",
        "\\theta^{(j)} = \\arg\\min_{\\theta'} f^{(j)} (\\theta')\n",
        "$$\n",
        "where\n",
        "$$ f^{(j)}(\\theta') := \\sum_{i \\neq j} l(x_i^T \\theta'; y_i) + r(\\theta') $$\n",
        "Similarly, $X^{(j)}$ and $y^{(j)}$, are employed to represent $X$ and $y$ with the $j$-th row removed, respectively.\n",
        "We denote by $\\tilde{y}_j$ the predicted label for sample $j$ when it is left out:\n",
        "$$\n",
        "\\tilde{y}_j := x_j ^T \\theta^{(j)}\n",
        "$$ {#eq-y-tilde-j-def}\n",
        "Our goal is calculating $\\tilde{y}_j$, for all $j$, efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# LOOCV with the sum of squares loss and quadratic regularization\n",
        "In scenarios where the loss function is the sum of squares loss\n",
        "$$\n",
        "l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2,\n",
        "$$\n",
        "and the regularizer is quadaratic\n",
        "$$\n",
        "r(\\theta') = \\theta'^T R \\theta'\n",
        "$$\n",
        "where $R$ is an $m \\times m$ semi-positive definite matrix,\n",
        "the solution to optimization problem @eq-theta-def is obtained by solving the linear equation [^1]:\n",
        "$$ \n",
        "A \\theta = b. \n",
        "$$ {#eq-theta-solve}\n",
        "where\n",
        "\\begin{align*}\n",
        "    A &:= X^T X + R \\\\\n",
        "    b &:= X^T y.\n",
        "\\end{align*}\n",
        "\n",
        "[^1]: I am deliberately avoiding writing $\\theta = A^{-1} b$, as $A$ does not have to be invertible for this equation to have a solution. \n",
        "Furthermore, it can mislead people into an implementation like `np.linalg.inv(A) @ b`, which is less stable and efficient than an implementation like `np.linalg.solve(A, b)`.\n",
        "\n",
        "Similarly, obtaining $\\theta^{(j)}$ requires solving\n",
        "$$ \n",
        "A^{(j)} \\theta^{(j)} = b^{(j)}.\n",
        "$$ {#eq-theta-j-solve}\n",
        "where\n",
        "\\begin{align*}\n",
        "    A^{(j)} &:= X^{(j)T} X^{(j)} + R \\\\\n",
        "    b^{(j)} &:= X^{(j)T} y^{(j)}.\n",
        "\\end{align*}\n",
        "Solving this linear equation for each $j$ has a time complexity of $O(m^2 n)$ (assuming $n > m$).\n",
        "Therefore, in a naive implementation, the overall complexity of LOOCV becomes $O(m^2 n^2)$, posing a significant computational challenge, particularly when $n$ is large.\n",
        "\n",
        "The key idea behind efficient LOOCV lies in leveraging the solution for @eq-theta-solve to calculate the solution for each @eq-theta-j-solve.\n",
        "We will utilize an important idea from computational linear algebra: the time required to solve multiple $m$ by $m$ equations that share the same matrix is almost identical to the time it takes to solve a single $m$ by $m$ equation.\n",
        "Specifically, we will solve, in additional to @eq-theta-solve, the following $n$ equations:\n",
        "$$\n",
        "A t_j = x_j.\n",
        "$$\n",
        "\n",
        "We start by noting that\n",
        "\\begin{align*}\n",
        "X^TX &= X^{(j)^T} X^{(j)} + x_j x_j^T    \\\\\n",
        "X^Ty &= X^{(j)^T} y^{(j)} + x_j y_j,\n",
        "\\end{align*}\n",
        "so we can write @eq-theta-j-solve like so:\n",
        "$$\n",
        "(A - x_j x_j^T) \\theta^{(j)} = b - x_j y_j.\n",
        "$$\n",
        "The usual way forward is employing Sherman-Morrison formula, solve for $\\theta^{(j)}$ and substitute in @eq-y-tilde-j-def to get an expression for $\\tilde{y}$.\n",
        "But there is a better way [^2]: rewrite @eq-theta-j-solve as:\n",
        "\\begin{align*}\n",
        "    A \\theta^{(j)} - x_j \\tilde{y}_j &= b - x_j y_j \\\\\n",
        "    \\tilde{y}_j &= x_j ^T \\theta^{(j)}\n",
        "\\end{align*}\n",
        "so instead of a single equation with one unknown ($\\theta^{(j)}$),\n",
        "we now have two equations with two \n",
        " unknowns ($\\theta^{(j)}$ and $\\tilde{y}_j$).\n",
        " At first this seems more complicated, but notice that since the coefficient of $\\theta^{(j)}$ in the first equation is $A$, we can eliminate it:\n",
        "\\begin{align*}\n",
        "\\theta^{(j)}  &= A^{-1} ( b - x_j y_j + x_j \\tilde{y}_j ) \\\\\n",
        "&= \\theta - t_j (  y_j - \\tilde{y}_j )\n",
        "\\end{align*}\n",
        "substituting in the bottom equation, we can solve for $\\tilde{y}_j$:\n",
        "\\begin{align*}\n",
        "\\tilde{y}_j &= x_j ^T \\left( \\theta - t_j (  y_j - \\tilde{y}_j ) \\right)\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\hat{y}_j - h_j (y_j - \\tilde{y}_j)\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\frac{\\hat{y}_j - h_j y_j}{1-h_j}\n",
        "% \\\\\n",
        "% \\tilde{y}_j &= \\frac{\\hat{y}_j -h_j \\hat{y}_j + h_j \\hat{y}_j - h_j y_j}{1-h_j}\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\hat{y}_j + \\frac{h_j }{1-h_j} \\left( \\hat{y}_j - y_j \\right)\n",
        "% \\\\\n",
        "% \\tilde{y}_j &= \\frac{\\hat{y}_j - y_j}{1-h_j} + y_j\n",
        "\\end{align*}\n",
        "where \n",
        "$$\n",
        "h_j := x_j ^T t_j = x_j^T A^{-1} x_j.\n",
        "$$\n",
        "That's it! we got an expression for $\\tilde{y}_j$ that doesn't require inverting any matrix other than $A$.\n",
        "It also has a nice interpretation: the difference between the prediction and the LOO prediction is the difference between the prediction an the true target, amplified by $\\frac{h_j }{1-h_j}$.\n",
        "\n",
        "If we wanted, we could substitute $\\hat{y}_j$ in (5) to get a concrete expression for $\\theta^{(j)}$. Although it can be useful for stuff like sensitivity analysis of the model coefficients, we won't pursuit this direction here.\n",
        "\n",
        "[^2]: For several reasons: Translates better into code; \n",
        "We get the expression for $\\tilde{y}_j$ directly, without goind through an expression for $\\theta^{(j)} first;\n",
        "Sherman-Morisson is a bit too strong here and can obscure some insights, so it's nice to avoid it. But actually the other way is just halfway it's proof (see for example [here](https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
