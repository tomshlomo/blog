{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Efficient Leave One Out Cross Validation\"\n",
        "author: \"Tom Shlomo\"\n",
        "date: \"2024-02-27\"\n",
        "description: TBD\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.\n",
        "\n",
        " Leave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer  accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.\n",
        "\n",
        "For linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it's `RidgeCV` estimator. Notably, this same trick extends beyond these linear models to any problem involving quadratically regularized least squares regressionâ€”a fact not widely recognized.\n",
        "\n",
        "Taking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.\n",
        "\n",
        "In this blog post, we derive the formulas for efficient LOOCV calculations and demonstrate these results on a practical example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notation\n",
        "We denote the number of samples in the training dataset as $n$.\n",
        "\n",
        "The $m$-dimensional feature vectors are represented as $x_1$ to $x_n$, forming the rows of matrix $X$.\n",
        "\n",
        "Targets are denoted as $y_1$ to $y_n$, forming the vector $y$. The model's prediction for the $i$-th training sample is $\\hat{y}_i = x_i^T \\theta$, where $\\theta$ is the coefficients vector.\n",
        "$\\hat{y} = X \\theta$ represents the vector containing all predictions.\n",
        "\n",
        "We fit $\\theta$ to the training data by minimizing the combined loss and regularization terms:\n",
        "$$\n",
        "\\theta := \\arg\\min_{\\theta'} f(\\theta').\n",
        "$$ {#eq-theta-def}\n",
        "where\n",
        "$$\n",
        "f(\\theta') := \\sum_{i=1}^{n} l(x_i^T \\theta'; y_i) + r(\\theta').\n",
        "$$\n",
        "Here, $l(\\hat{y}_i; y_i)$ represents the loss function, quantifying the difference between the prediction $\\hat{y}$ and the true target $y_i$,\n",
        "while $r$ is the regularization function.\n",
        "We assume $l$ (as a function of $\\hat{y}_i$) and $r$ are convex and twice differentiable.\n",
        "Special cases of this model include\n",
        "ordinary least squares ($l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2$, $r(\\theta') = 0$), \n",
        "ridge regression ($l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2$, $r(\\theta') = \\alpha \\| \\theta' \\|^2$), \n",
        "logistic regression ($l(\\hat{y}_i;y_i) = \\log \\left( 1 + e^{-y_i \\hat{y}_i}\\right)$ with $y_i \\in \\{ -1, 1\\}$),\n",
        "and Poisson regression ($l(\\hat{y}_i;y_i) = y_i \\hat{y}_i - e^{\\hat{y}_i}$).\n",
        "\n",
        "To denote the coefficients obtained by excluding the $j$-th example, we use $\\theta^{(j)}$:\n",
        "$$\n",
        "\\theta^{(j)} = \\arg\\min_{\\theta'} f^{(j)} (\\theta')\n",
        "$$\n",
        "where\n",
        "$$ f^{(j)}(\\theta') := \\sum_{i \\neq j} l(x_i^T \\theta'; y_i) + r(\\theta') $$\n",
        "Similarly, $X^{(j)}$ and $y^{(j)}$, represent $X$ and $y$ with the $j$-th row removed, respectively.\n",
        "We denote by $\\tilde{y}_j$ the predicted label for sample $j$ when it is left out:\n",
        "$$\n",
        "\\tilde{y}_j := x_j ^T \\theta^{(j)}\n",
        "$$ {#eq-y-tilde-j-def}\n",
        "Our goal is calculating $\\tilde{y}_j$, for all $j$, efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The quadratic case\n",
        "In scenarios where the loss function is the sum of squares loss,\n",
        "$$\n",
        "l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2,\n",
        "$$\n",
        "and the regularizer is quadratic\n",
        "$$\n",
        "r(\\theta') = \\theta'^T R \\theta'\n",
        "$$\n",
        "where $R$ is an $m \\times m$ semi-positive definite matrix,\n",
        "the solution to the optimization problem @eq-theta-def is obtained by solving the linear equation [^1]:\n",
        "$$ \n",
        "A \\theta = b. \n",
        "$$ {#eq-theta-solve}\n",
        "where\n",
        "\\begin{align*}\n",
        "    A &:= X^T X + R \\\\\n",
        "    b &:= X^T y.\n",
        "\\end{align*}\n",
        "\n",
        "[^1]: I am deliberately avoiding writing $\\theta = A^{-1} b$, as $A$ does not have to be invertible for this equation to have a solution, and it allows me to avoid the usual \"assuming full rank\" caveats people tend to use here.\n",
        "Furthermore, it can mislead people into implementations like `np.linalg.inv(A) @ b`, which are less stable and efficient than implementations like `np.linalg.solve(A, b)`.\n",
        "\n",
        "Similarly, obtaining $\\theta^{(j)}$ requires solving\n",
        "$$ \n",
        "A^{(j)} \\theta^{(j)} = b^{(j)}.\n",
        "$$ {#eq-theta-j-solve}\n",
        "where\n",
        "\\begin{align*}\n",
        "    A^{(j)} &:= X^{(j)T} X^{(j)} + R \\\\\n",
        "    b^{(j)} &:= X^{(j)T} y^{(j)}.\n",
        "\\end{align*}\n",
        "Forming and solving @eq-theta-j-solve for each $j$ has a time complexity of $O(m^3 + n m^2)$.\n",
        "Thus, in a naive implementation, the overall complexity of LOOCV becomes $O(n m^3 + n^2 m^2)$, posing a significant computational challenge, particularly when $n$ is large.\n",
        "\n",
        "Efficient LOOCV leverages the solution for @eq-theta-solve to calculate the solution for @eq-theta-j-solve. We exploit the idea from computational linear algebra that solving multiple $m$ by $m$ equations with the same matrix has a time complexity similar to solving a single such equation. Thus, we solve, in addition to @eq-theta-solve, the following $n$ equations:\n",
        "$$\n",
        "A t_j = x_j.\n",
        "$$\n",
        "\n",
        "<!-- The key idea behind efficient LOOCV lies in leveraging the solution for @eq-theta-solve to calculate the solution for @eq-theta-j-solve.\n",
        "We will utilize an important idea from computational linear algebra: \n",
        "even though the complexity of solving a single $m$ by $m$ equation is $O(m^3)$, the complexity of solving $n$ such equations is not $O(nm^3)$, but $O(m^3 + nm^2)$, if all the equations share the same matrix. -->\n",
        "<!-- the time required to solve multiple $m$ by $m$ equations that share the same matrix is almost identical to the time it takes to solve a single $m$ by $m$ equation. -->\n",
        "<!-- Specifically, we will solve, in additional to @eq-theta-solve, the following $n$ equations: -->\n",
        "\n",
        "\n",
        "We start by noting that\n",
        "\\begin{align*}\n",
        "X^TX &= X^{(j)^T} X^{(j)} + x_j x_j^T    \\\\\n",
        "X^Ty &= X^{(j)^T} y^{(j)} + x_j y_j,\n",
        "\\end{align*}\n",
        "so we can write @eq-theta-j-solve like so:\n",
        "$$\n",
        "(A - x_j x_j^T) \\theta^{(j)} = b - x_j y_j.\n",
        "$$\n",
        "The usual way forward involves employing Sherman-Morrison formula, solving for $\\theta^{(j)}$ and substituting it in @eq-y-tilde-j-def to obtain an expression for $\\tilde{y}$.\n",
        "However, there's a better approach [^2]:\n",
        "We rewrite @eq-theta-j-solve as\n",
        "\\begin{align*}\n",
        "    A \\theta^{(j)} - x_j \\tilde{y}_j &= b - x_j y_j \\\\\n",
        "    \\tilde{y}_j &= x_j ^T \\theta^{(j)}\n",
        "\\end{align*}\n",
        "so instead of a single equation with one unknown ($\\theta^{(j)}$),\n",
        "we now have two equations with two \n",
        " unknowns ($\\theta^{(j)}$ and $\\tilde{y}_j$).\n",
        " At first this seems more complicated, but notice that since the coefficient of $\\theta^{(j)}$ in the first equation is $A$, we can eliminate it:\n",
        "\\begin{align*}\n",
        "\\theta^{(j)}  &= A^{-1} ( b - x_j y_j + x_j \\tilde{y}_j ) \\\\\n",
        "&= \\theta - t_j (  y_j - \\tilde{y}_j )\n",
        "\\end{align*}\n",
        "substituting in the bottom equation, we can solve for $\\tilde{y}_j$:\n",
        "\\begin{align*}\n",
        "\\tilde{y}_j &= x_j ^T \\left( \\theta - t_j (  y_j - \\tilde{y}_j ) \\right)\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\hat{y}_j - h_j (y_j - \\tilde{y}_j)\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\frac{\\hat{y}_j - h_j y_j}{1-h_j}\n",
        "% \\\\\n",
        "% \\tilde{y}_j &= \\frac{\\hat{y}_j -h_j \\hat{y}_j + h_j \\hat{y}_j - h_j y_j}{1-h_j}\n",
        "\\\\\n",
        "\\tilde{y}_j &= \\hat{y}_j + \\frac{h_j }{1-h_j} \\left( \\hat{y}_j - y_j \\right)\n",
        "% \\\\\n",
        "% \\tilde{y}_j &= \\frac{\\hat{y}_j - y_j}{1-h_j} + y_j\n",
        "\\end{align*}\n",
        "where \n",
        "$$\n",
        "h_j := x_j ^T t_j.\n",
        "$$\n",
        "\n",
        "::: {.callout-note appearance=\"simple\"}\n",
        "## Reminder\n",
        "$y_j$ is the true label.\\\n",
        "$\\hat{y}_j$ is the prediction using all the data.\\\n",
        "$\\tilde{y}_j$ is the leave-one-out prediction.\n",
        ":::\n",
        "\n",
        "That's it! we got an expression for $\\tilde{y}_j$ that doesn't require inverting any matrix other than $A$.\n",
        "It also has a nice interpretation: the difference between the prediction and the LOO prediction is the difference between the prediction an the true label, \"amplified\" by $\\frac{h_j }{1-h_j}$.\n",
        "\n",
        "[^2]: This approach translates better into code, as we get the expression for $\\tilde{y}_j$ directly, without going through an expression for $\\theta^{(j)}$ first.\n",
        "I also think Sherman-Morisson is a bit too strong here and can obscure some insights, so it's nice to avoid it. But actually the other approach is just halfway it's proof (see for example [here](https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python implementation\n",
        "The approach outlined above adapts seamlessly into code.\n",
        "We'll construct an estimator resembling the sklearn style, featuring standard fit and predict methods, alongside a function to compute $\\tilde{y}$, the leave-one-out predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Self\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "class LinearRegressionWithQuadraticRegularization:\n",
        "    def __init__(self, R) -> None:\n",
        "        self.R = R\n",
        "\n",
        "    def fit(self, X, y) -> Self:\n",
        "        A = X.T @ X + self.R\n",
        "        b = X.T @ y\n",
        "        self.theta_ = scipy.linalg.solve(\n",
        "            A,\n",
        "            b,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        return X @ self.theta_\n",
        "\n",
        "    def fit_loocv_predict(self, X, y) -> np.ndarray:\n",
        "        A = X.T @ X + self.R\n",
        "        b = X.T @ y\n",
        "        temp = scipy.linalg.solve(\n",
        "            A,\n",
        "            np.vstack([b, X]).T,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        self.theta_ = temp[:, 0]\n",
        "        t = temp[:, 1:]\n",
        "        h = np.einsum(\"ij,ji->i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n",
        "        y_hat = self.predict(X)\n",
        "        return y_hat + (h / (1 - h)) * (y_hat - y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that our method for calculating the leave-one-out predictions is correct on random data, and compare it's run time to the usual leave-one-out procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max absolute error: 1.243e-14\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from time import time\n",
        "\n",
        "\n",
        "def standard_loocv(model, X, y) -> np.ndarray:\n",
        "    y_tilde = np.empty_like(y)\n",
        "    for i, (train_index, test_index) in enumerate(LeaveOneOut().split(X)):\n",
        "        X_loo = X[train_index, :]\n",
        "        y_loo = y[train_index]\n",
        "        model.fit(X_loo, y_loo)\n",
        "        y_tilde[i] = model.predict(X[test_index, :])[0]\n",
        "    return y_tilde\n",
        "\n",
        "\n",
        "def gen_random_data(n: int, m: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(42)\n",
        "    X = rng.standard_normal((n, m))\n",
        "    L = rng.standard_normal((m, m))\n",
        "    theta = L @ rng.standard_normal(m)\n",
        "    y = X @ theta + rng.standard_normal(n)\n",
        "    R = L @ L.T  # random positive definite matrix\n",
        "    return X, y, R\n",
        "\n",
        "\n",
        "X, y, R = gen_random_data(n=100, m=10)\n",
        "model = LinearRegressionWithQuadraticRegularization(R=R)\n",
        "print(\n",
        "    f\"max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good, the two methods to calculate $\\tilde{y}$ give the same result. Let's also compare the runtime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.6 Âµs Â± 1.35 Âµs per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)\n",
            "2.39 ms Â± 10.7 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit model.fit_loocv_predict(X, y) \n",
        "%timeit standard_loocv(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice, a significant speedup. But that's quite fast to begin with. \n",
        "Let's increase `n` and `m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max absolute error: 8.527e-14\n",
            "138 ms Â± 16.4 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
            "The slowest run took 4.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "822 ms Â± 461 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "X, y, R = gen_random_data(n=1000, m=50)\n",
        "model = LinearRegressionWithQuadraticRegularization(R=R)\n",
        "print(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')\n",
        "%timeit model.fit_loocv_predict(X, y) \n",
        "%timeit standard_loocv(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hmm... Much less impressive. In theory the speedup should improve as the problem size increases. This is likely due to some python inefficiencies, not the algorithm itself.\n",
        "Let's try to improve by using JAX's just-in-time compilation feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max absolute error: 4.780e-05\n",
            "1.75 ms Â± 232 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
            "353 ms Â± 11.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "\n",
        "class JitLinearRegressionWithQuadraticRegularization:\n",
        "    def __init__(self, R) -> None:\n",
        "        self.R = R\n",
        "\n",
        "    def fit(self, X, y) -> Self:\n",
        "        self.theta_ = self._fit(X, y, self.R)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        return self._predict(X, self.theta_)\n",
        "\n",
        "    def fit_loocv_predict(self, X, y) -> np.ndarray:\n",
        "        self.theta_, y_tilde = self._fit_loocv_predict(X, y, self.R)\n",
        "        return y_tilde\n",
        "    \n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _fit(X, y, R) -> np.ndarray:\n",
        "        return jax.scipy.linalg.solve(\n",
        "            X.T @ X + R, \n",
        "            X.T @ y,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _predict(X, theta) -> np.ndarray:\n",
        "        return X @ theta\n",
        "\n",
        "    @staticmethod\n",
        "    @jax.jit\n",
        "    def _fit_loocv_predict(X, y, R) -> np.ndarray:\n",
        "        temp = jax.scipy.linalg.solve(\n",
        "            X.T @ X + R,\n",
        "            jax.numpy.vstack([X.T @ y, X]).T,\n",
        "            overwrite_a=True,\n",
        "            overwrite_b=True,\n",
        "            assume_a=\"pos\",\n",
        "        )\n",
        "        theta = temp[:, 0]\n",
        "        t = temp[:, 1:]\n",
        "        h = jax.numpy.einsum(\"ij,ji->i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n",
        "        y_hat = X @ theta\n",
        "        return theta, y_hat + (h / (1 - h)) * (y_hat - y)\n",
        "    \n",
        "model = JitLinearRegressionWithQuadraticRegularization(R=R)\n",
        "print(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')\n",
        "%timeit model.fit_loocv_predict(X, y).block_until_ready()\n",
        "%timeit standard_loocv(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Much better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The non-quadratic case\n",
        "In this section, we extend our approach to scenarios where $l$ or $r$ are not quadratic. Although solving equation @eq-theta-def is not simplified to solving a linear equation in this case, we can resort to the following approximation:\n",
        "$$\n",
        "H^{(j)} (\\theta^{(j)} - \\theta) \\approx -g^{(j)}\n",
        "$$ {#eq-newton-approx}\n",
        "where $H^{(j)}$ and $g^{(j)}$ represent the Hessian and gradient of $f^{(j)}$ at $\\theta$, respectively.\n",
        "The rationale here is that $\\theta$ and $\\theta^{(j)}$ should be relatively close (and closer as $n$ increases), making it likely that Newton's method on $f^{(j)}$ converges in a single iteration when initialized on $\\theta$. \n",
        "\n",
        "Similar to the quadratic case, we can relate $H^{(j)}$ and $g^{(j)}$ to $H$ and $g$, the Hessian and gradient of $f$ at $\\theta$:\n",
        "\\begin{align*}\n",
        "H^{(j)} &= H - x_j l''(\\hat{y}_i ; y_i) x_j^T\n",
        "\\\\\n",
        "g^{(j)} &= g - x_j l'(\\hat{y}_i ; y_i) = - x_j l'(\\hat{y}_i ; y_i)\n",
        "\\end{align*}\n",
        "allowing us to rewrite @eq-newton-approx as:\n",
        "$$\n",
        "\\left(\n",
        "    H - x_j l''\\left(\\hat{y}_i ; y_i\\right) x_j^T\n",
        "\\right) \n",
        "\\left( \\theta^{(j)} - \\theta \\right) \n",
        "\\approx  x_j l'(\\hat{y}_i ; y_i).\n",
        "$$\n",
        "Next, we introduce the second equation:\n",
        "\\begin{align*}\n",
        "H \\theta^{(j)} \n",
        "    - x_j l''(\\hat{y}_i ; y_i) \\tilde{y}_j\n",
        "    - H \\theta\n",
        "    + x_j l''(\\hat{y}_i ; y_i) \\hat{y}_j\n",
        "    &\\approx\n",
        "    x_j l'(\\hat{y}_i ; y_i)\n",
        "    \\\\\n",
        "    \\tilde{y}_j &= x_j ^T \\theta^{(j)}.\n",
        "\\end{align*}\n",
        "Now, we can eliminate $\\theta^{(j)}$ and solve for $\\tilde{y}_j$:\n",
        "\\begin{align*}\n",
        "\\theta^{(j)} &\\approx \\theta + t_j (l'(\\hat{y}_i ; y_i) +  l''(\\hat{y}_i ; y_i) (\\tilde{y}_j - \\hat{y}_j))\n",
        "\\\\\n",
        "\\tilde{y}_j &\\approx x_j ^T \\left(\n",
        "    \\theta + t_j (l'(\\hat{y}_i ; y_i) +  l''(\\hat{y}_i ; y_i) (\\tilde{y}_j - \\hat{y}_j))\n",
        "    \\right)\n",
        "\\\\\n",
        "\\tilde{y}_j &\\approx \n",
        "     \\hat{y}_j \n",
        "    + \\frac{h_j}{1 - h_j l''(\\hat{y}_i ; y_i)}  l'(\\hat{y}_i ; y_i) \n",
        "\\end{align*}\n",
        "where $t_j := H^{-1} x_j$ and $h_j := x_j^T t_j$.\n",
        "\n",
        "It's worth noting the resemblance between the expression for $\\tilde{y}_j$ here and the expression obtained for the quadratic case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python implementation\n",
        "Once more, we'll turn to jax, leveraging its automatic differentiation capabilities.\n",
        "Our estimator will take as inputs the loss and regularization functions, along with an optional \"inverse link\" function. This function can be employed to transform the predicted labels (e.g. a sigmoid to convert log-odds to probabilities in logistic regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
