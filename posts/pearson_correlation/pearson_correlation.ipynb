{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A practical interpertation of the Pearson correlation coefficient\"\n",
        "author: \"Tom Shlomo\"\n",
        "date: \"2024-01-20\"\n",
        "description: $\\rho=1$ means perfect positive correlation, $\\rho=-1$ means perfect negative correlation, $\\rho=0$ means no correlation. But what does $\\rho=0.72$ mean?\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\renewcommand{\\E}[1]{\\operatorname{E}\\left[#1\\right]}\n",
        "\\renewcommand{\\var}[1]{\\operatorname{Var} \\left[#1 \\right]}\n",
        "\\renewcommand{\\cov}[1]{\\operatorname{Cov} \\left[#1 \\right] }\n",
        "$$\n",
        "My goal is to explain the Pearson correlation coefficient without using the word correlation, which is often used to describe it.\n",
        "\\\n",
        "The Pearson correlation coefficient of two random variables $X$ and $Y$ is\n",
        "$$\n",
        "\\rho := \\frac{\\sigma_{XY}}{\\sigma_X \\sigma_Y},\n",
        "$$\n",
        "where \n",
        "$\\sigma_X, \\sigma_Y$ are the standard deviation of $X$ and $Y$,\n",
        "and $\\sigma_{XY}$ is their covariance.\n",
        "\n",
        "A motivation for the definition $\\rho$ comes from the problem of estimating $Y$ from an observation of $X$.\n",
        "It turns out that in the optimal (lowest MSE) linear estimation, *the number of standard deviations $Y$ is above it's mean is \n",
        "$\\rho$ times\n",
        "the number of standard deviations $X$ is above it's mean.*\n",
        "\\\n",
        "For example, consider a population of people where\n",
        "height and weight are correlated with $\\rho=0.72$,\n",
        "heights are distributed with mean $170$cm and a standard deviation of $10$cm,\n",
        "weights are distributed with mean $70$Kg and a standard deviation of $20$Kg.\n",
        "If you know that the height of a certain person is $190$cm,\n",
        "a good guess for it's weight is\n",
        "$70 + 2 \\cdot 0.72 \\cdot 20 = 98.8$Kg.\n",
        "\n",
        "The proof is very simple. Since we are dealing with linear (actually, affine) estimators, we need to show that the $a$ and $b$ that would minimize \n",
        "$$\n",
        "\\text{MSE} := \\E{ \\left( \\hat{Y} - Y \\right) ^2},\n",
        "$$\n",
        "where $\\hat{Y} := a (X - \\mu_x) + b$,\n",
        "are $\\rho \\sigma_Y / \\sigma_X$ and $\\mu_Y$.\n",
        "\\\n",
        "The MSE is the sum of bias squared and variance.\n",
        "The variance doesn't depend on $b$, and the bias is\n",
        "$\\E{  \\hat{Y} - Y } = b - \\mu_Y$\n",
        "which doesn't depend on $a$.\n",
        "so $b=\\mu_Y$.\n",
        "To minimize the variance, we simplify\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\var{\\hat{Y} - Y}\n",
        "&= \\var{\\hat{Y}} + \\var{Y} - 2 \\cov{\\hat{Y}, Y}\n",
        "\\\\&= \\sigma_x ^ 2 a^2 \n",
        "   + \\sigma_Y ^2\n",
        "   -2  \\sigma_{XY} a\n",
        "\\end{align*}\n",
        "$$\n",
        "This is just a parabola in $a$, so the optimal $a$ is\n",
        "$$\n",
        "a=\\frac{2 \\sigma_{XY}} {2 \\sigma_X ^2}\n",
        "=\n",
        "\\rho \\frac{\\sigma_Y } {\\sigma_X }\n",
        "$$\n",
        "(which is what we wanted to show).\n",
        "\n",
        "The estimator is unbiased, so it's MSE is equal to it's variance:\n",
        "$$\n",
        "\\text{MSE} = \\sigma_Y ^2 (1 - \\rho ^ 2).\n",
        "$$\n",
        "This equation gives another concrete interpretation of $\\rho$:\n",
        "*If $X$ and $Y$ are correlated with coefficient $\\rho$, observing $X$ will decrease the standard deviation of a $Y$ estimate by a factor of at least $\\sqrt{1 - \\rho^2}$.*\n",
        "\\\n",
        "\"at least\" since the the optimal linear estimator is equal or worse than the optimal estimator.\n",
        "\\\n",
        "In the example above, knowing the height decreases weight estimation standard deviation from 20Kg to $20  (1 - 0.72^2) = 9.6$Kg.\n",
        "\n",
        "Randomly ordered notes:\n",
        "\n",
        "1. If $X$ and $Y$ are jointly Gaussian, the optimal linear estimator is also the optimal estimator.\n",
        "\n",
        "2. The \"mean\" in \"MSE\" is an average over the joint distribution of $X$ and $Y$, which is different than over the distribution of $Y$ given $X$, for which\n",
        "our estimator is not the optimal linear estimator (and biased).\\\n",
        "In our example, we estimated the weight to be $98.8$Kg with variance $9.6^2$.\n",
        "It doesn't mean that if we will sample random people with height $190$cm, we would get a mean weight of $98.8$Kg and variance smaller than $9.6^2$.\n",
        "It means that if we sample random people, and estimate their weight from their height using the optimal linear estimator, our error will be zero on average, and with variance $9.6^2$.\n",
        "If we use the optimal estimator, the $9.6^2$ is an upper bound on the variance.\n",
        "\n",
        "3. The sentence \"$X$ and $Y$ are not correlated\" now has a concrete meaning: it means that the optimal linear estimator of $Y$ from $X$ will be the mean of $Y$, ignoring $X$ completely.\n",
        "\n",
        "4. The discussion above is \"Bayesian\", in the sense that it assumes you have some knowledge about the distribution of $X$ and $Y$.\n",
        "In practice we usually get $n$ samples of $X$ and $Y$ pairs, and we use plug-in estimators to estimate the means, variances, and covariance, which we will then use build our $Y$ from $X$ linear estimator.\n",
        "\\\n",
        "Machine learning people would say: we can use the samples to train a linear regression model to predict $Y$ from $X$ directly. \n",
        "Sounds better, more \"end-to-end\"y, but actually it gives exactly the same result\n",
        "(assuming we don't use [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)).\\\n",
        "Proof:\\\n",
        "We denote by $x$ and $y$ be the vectors of samples of $X$ and $Y$,\n",
        "by $\\mathbf{1}$ a vector of ones,\n",
        "and by $A$ the matrix whose first column is $x$ and the second column is $\\mathbf{1}$.\n",
        "The coefficients of the linear model are given by:\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\begin{bmatrix}\n",
        "        \\theta_{\\text{slope}} \\\\\n",
        "        \\theta_{\\text{intercept}}\n",
        "    \\end{bmatrix}\n",
        "    &:=\n",
        "    \\text{argmin}_\\theta \\| A \\theta - y \\|^2\n",
        "    \\\\&=\n",
        "    \\left( A ^T A \\right)^{-1} A^T y\n",
        "    \\\\&=\n",
        "    \\begin{bmatrix}\n",
        "        \\|x\\|^2 && \\mathbf{1}^Tx \\\\\n",
        "        \\mathbf{1}^T x  && \\mathbf{1}^T \\mathbf{1}\n",
        "    \\end{bmatrix} \n",
        "    ^{-1}\n",
        "    \\begin{bmatrix}\n",
        "        x^T y \\\\\n",
        "        \\mathbf{1} ^T y\n",
        "    \\end{bmatrix}\n",
        "    \\\\&=\n",
        "    \\begin{bmatrix}\n",
        "        \\sigma_X^2 + \\mu_X^2 && \\mu_X \\\\\n",
        "        \\mu_X  && 1\n",
        "    \\end{bmatrix} \n",
        "    ^{-1}\n",
        "    \\begin{bmatrix}\n",
        "        \\sigma_{XY} + \\mu_X \\mu_Y \\\\\n",
        "        \\mu_Y\n",
        "    \\end{bmatrix}\n",
        "    \\\\&=\n",
        "    \\frac{1}{\\sigma_X ^2}\n",
        "    \\begin{bmatrix}\n",
        "        1 && -\\mu_X \\\\\n",
        "        -\\mu_X  && \\sigma_X^2 + \\mu_X^2\n",
        "    \\end{bmatrix} \n",
        "    \\begin{bmatrix}\n",
        "        \\sigma_{XY} + \\mu_X \\mu_Y \\\\\n",
        "        \\mu_Y\n",
        "    \\end{bmatrix}\n",
        "    \\\\&=\n",
        "    \\frac{1}{\\sigma_X ^2}\n",
        "    \\begin{bmatrix}\n",
        "        \\sigma_{XY} \\\\\n",
        "        -\\mu_X \\sigma_{XY} + \\sigma_X^2 \\mu_Y \n",
        "    \\end{bmatrix}\n",
        "    \\\\&=\n",
        "    \\begin{bmatrix}\n",
        "        a \\\\\n",
        "        -\\mu_X a + b\n",
        "    \\end{bmatrix}.\n",
        "\\end{align*}\n",
        "$$\n",
        "Note also that the r2-score of this fit is equal to $\\rho^2$:\n",
        "$$\n",
        "r^2 := 1 - \\frac{\\text{MSE}}{\\sigma_Y^2} = 1 - \\frac{\\sigma_Y ^2 \\left(1-\\rho^2\\right)}{\\sigma_Y ^2} = \\rho^2.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<!-- \n",
        "The optima\n",
        "The optimal $a=\\frac{2 \\sigma_{XY}} {2 \\sigma_X ^ 2}$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "a \n",
        "&= \\text{argmin}_{a'} \\var{\\hat{Y} - Y\\right] \\\n",
        "&= \\text{argmin}_{a'} \\var{a (X - \\mu_X) - Y\\right] \\\n",
        "&= \\text{argmin}_{a'} \n",
        "    \\var{a \\left(X - \\mu_X\\right) \\right] \n",
        "    + \\var{ Y\\right] \n",
        "    -2 \\mathrm{Cov}\\left[a \\left(X - \\mu_X\\right), Y\\right] \\\n",
        "&= \\text{argmin}_{a'} \n",
        "    a^2 \\sigma_x ^ 2\n",
        "    + \\sigma_Y ^2\n",
        "    -2 a \\sigma_{XY}\n",
        "\\end{align*}\n",
        "$$\n",
        "and the variance do\n",
        "We with expanding the MSE as the sum of the squared bias and variance\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{MSE} &=\n",
        "\\left(\\E{ \\left[\\hat{Y} - Y \\right] \\right)^2\n",
        "+ \\mathrm{Var} \\left[\\hat{Y} - Y \\right]\n",
        "\\&=\n",
        "b ^ 2\n",
        "+ \\mathrm{Var} \\left[a (x - \\mu_X) - Y \\right]\n",
        "\\end{align*}\n",
        "$$ -->\n",
        "\n",
        "<!-- \n",
        "Suppose the Pearson correlation coefficient is $\\rho$, \n",
        "and you wish to estimate $Y$ based on a given observation of $X$\n",
        "that is $n$ standard deviations away from the mean.\n",
        "The optimal linear estimate is $n \\rho$ standard deviations away from the mean.\n",
        "\n",
        "If the observation of $X$ is $n$\n",
        "It turns out that the optimal linear estimation is $\\rho$ \n",
        "then the optimal linear estimation of $Y$ given a sample of $X$ is\n",
        "then the optimal linear estimation of $Y$ from $X$ is obtained by \n",
        "1. Calculate by how many standard deviations the sampled $X$ is above it's mean.\n",
        "2. multiply by $\\rho$.\n",
        "3. This is by how many standard deviations the estimate of $Y$ is above it's mean. -->\n",
        "<!-- $$\n",
        "\\begin{align*}\n",
        "\\E{ \\left[ \\left(a (x - \\mu_x) + b - y \\right) ^ 2 \\right]\n",
        "&= \n",
        "a ^ 2 \\E{ \\left[ \\left( x - \\mu_x \\right) ^ 2 \\right]\n",
        "+\n",
        "\\E{ \\left[ \\left(b - y \\right) ^ 2 \\right]\n",
        "+\n",
        "a \\E{ \\left[ \\left(x - \\mu_x\\right) \\left(b - y \\right) \\right]\n",
        "\\&=\n",
        "a ^ 2 \\sigma_x ^2\n",
        "+\n",
        "\\sigma_y ^ 2\n",
        "+\n",
        "a \\, \\sigma_{xy}\n",
        "\\end{align*}\n",
        "$$ -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
