<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tom Shlomo&#39;s Blog</title>
<link>https://tomshlomo.github.io/blog/</link>
<atom:link href="https://tomshlomo.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Tom Shlomo&#39;s blog</description>
<generator>quarto-1.4.543</generator>
<lastBuildDate>Fri, 06 Dec 2024 22:00:00 GMT</lastBuildDate>
<item>
  <title>Teaching Rust to Python Developers</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/rust_with_via/rust_with_via.html</link>
  <description><![CDATA[ 





<p>Over the last two years, I have developed a strong interest in Rust.<br>
While my day-to-day work primarily involves Python, I have been exploring ways to leverage Rust in our projects.</p>
<p>A few months ago, I had the opportunity to teach Rust to our algorithms team in a 4-day workshop.<br>
We based the workshop on Google’s excellent <a href="https://google.github.io/comprehensive-rust/">Comprehensive Rust</a> course but made several modifications.<br>
Most notably, we replaced some exercises and added a new section on using PyO3 to write Python extensions in Rust.</p>
<p>I was very happy with the results, and I think the team was too.</p>
<p>We have since <a href="https://tomshlomo.github.io/rust-with-via/index.html">published all the materials</a> and shared a <a href="https://medium.com/@tomshlomo/teaching-rust-to-python-developers-65709cf4babb">blog post</a> detailing the experience.</p>
<p>I hope you find these resources valuable!</p>



 ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/rust_with_via/rust_with_via.html</guid>
  <pubDate>Fri, 06 Dec 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The sparse approximation algorithm no one talks about</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/omp/omp.html</link>
  <description><![CDATA[ 





<p>In the problem of sparse approximation, we are trying to approximate a given vector <img src="https://latex.codecogs.com/png.latex?y"> as a linear combination of few columns of a given matrix <img src="https://latex.codecogs.com/png.latex?A">. It is useful in many applications, such as machine learning (feature selection), image processing (denoising, inpainting, deblurring, compression), and signal processing (spectral estimation, DOA estimation, compressive sensing).</p>
<p>In this post, we will discuss greedy algorithms for solving the sparse approximation problem, focusing on the well-known Matching Pursuit (MP) and Orthogonal Matching Pursuit (OMP) algorithms, as well as a “new” algorithm that I call the Most Obvious Matching Pursuit (MOMP).</p>
<section id="notation" class="level1">
<h1>Notation</h1>
<p>We will consider the following flavor of the sparse approximation problem: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20%5Cbegin%7Barray%7D%7Bll%7D%0A%20%20%20%20%20%20%20%20%5Cunderset%7Bx%7D%7B%5Cmbox%7Bminimize%7D%7D%20%20&amp;%20%5C%7C%20Ax%20-%20y%5C%7C%5E2%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Cmbox%7Bsubject%20to%7D%20&amp;%20%5Ctext%7B$x$%20has%20at%20most%20$k$%20non-zero%20entries%7D.%0A%20%20%20%20%5Cend%7Barray%7D%0A%20%20%20%20%5Clabel%7Be-opt-prob%7D%0A%5Cend%7Bequation%7D"> Where</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?A"> is the <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> dictionary matrix with columns <img src="https://latex.codecogs.com/png.latex?a_1,%20%5Cldots,%20a_n">,</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?y"> is the <img src="https://latex.codecogs.com/png.latex?m">-vector we are trying to approximate as the linear combination of the columns of <img src="https://latex.codecogs.com/png.latex?A">, and</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?x"> is the <img src="https://latex.codecogs.com/png.latex?n">-vector of coefficients.</p></li>
</ul>
<p>When <img src="https://latex.codecogs.com/png.latex?S"> is an ordered subset of <img src="https://latex.codecogs.com/png.latex?1,%20%5Cldots,%20n">, we denote by <img src="https://latex.codecogs.com/png.latex?A_S"> the sub-matrix of <img src="https://latex.codecogs.com/png.latex?A"> obtained by keeping only the columns in <img src="https://latex.codecogs.com/png.latex?S">, and by <img src="https://latex.codecogs.com/png.latex?x_S"> the sub-vector of <img src="https://latex.codecogs.com/png.latex?x"> obtained by keeping only entries in <img src="https://latex.codecogs.com/png.latex?S">.</p>
<p>For a matrix <img src="https://latex.codecogs.com/png.latex?M">, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(M)"> denotes it’s column space, and <img src="https://latex.codecogs.com/png.latex?M%5EH"> denotes it’s conjugate transpose (if you don’t care about complex data, you can think of it as the transpose).</p>
</section>
<section id="support-recovery" class="level1">
<h1>Support Recovery</h1>
<p>If the support of <img src="https://latex.codecogs.com/png.latex?x"> is known and denoted by <img src="https://latex.codecogs.com/png.latex?S">, then since <img src="https://latex.codecogs.com/png.latex?A%20x%20=%20A_S%20x_S">, finding the optimal <img src="https://latex.codecogs.com/png.latex?x_S"> is a simple least squares problem: <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderset%7Bx_S%7D%7B%5Ctext%7Bminimize%7D%7D%20%20%5C%7C%20A_S%20x_S%20-%20y%20%5C%7C%5E2.%0A"> This means that in practice, the problem is to recover the support of <img src="https://latex.codecogs.com/png.latex?x">. The following results will be useful in this context:</p>
<ul>
<li><p>An optimal support <img src="https://latex.codecogs.com/png.latex?S"> maximizes <img src="https://latex.codecogs.com/png.latex?y%5EH%20P_%7BA_S%7D%20y">, where <img src="https://latex.codecogs.com/png.latex?P_%7BA_S%7D"> is the projection matrix onto <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)">. <!-- (The intution here is that with an optimal support, $y$ should be approximately in $\text{Range}(A_s)$, so projecting onto it should have little effect). --></p></li>
<li><p>An optimal <img src="https://latex.codecogs.com/png.latex?x_S"> is given by any solution to <img src="https://latex.codecogs.com/png.latex?A_S%5EH%20A_S%20x_S%20=%20A_S%5EH%20y"> (aka the normal equations).</p></li>
</ul>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof <sup>1</sup>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>First, we rewrite <img src="https://latex.codecogs.com/png.latex?y"> as <img src="https://latex.codecogs.com/png.latex?y%5E%7B%7C%7C%7D%20+%20y%5E%7B%5Cperp%7D">, where <img src="https://latex.codecogs.com/png.latex?y%5E%7B%7C%7C%7D"> is in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)"> and <img src="https://latex.codecogs.com/png.latex?y%5E%7B%5Cperp%7D"> is orthogonal to it. Since <img src="https://latex.codecogs.com/png.latex?A_S%20x_S"> is in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)">, we can apply the Pythagorean theorem (twice) to get: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5C%7C%20A_S%20x_S%20-%20y%20%5C%7C%5E2%20&amp;=%0A%5C%7C%20%5Cleft(A_S%20x_S%20-%20y%5E%7B%7C%7C%7D%5Cright)%20-%20y%5E%7B%5Cperp%7D%20%5C%7C%5E2%0A%5C%5C&amp;=%0A%20%20%5C%7C%20A_S%20x_S%20-%20y%5E%7B%7C%7C%7D%20%5C%7C%5E2%20+%20%5C%7C%20y%5E%7B%5Cperp%7D%20%5C%7C%5E2%0A%20%20%5C%5C&amp;=%0A%20%20%5C%7C%20A_S%20x_S%20-%20y%5E%7B%7C%7C%7D%20%5C%7C%5E2%20+%20%5C%7C%20y%20%5C%7C%5E2%20-%20%5C%7C%20y%5E%7B%7C%7C%7D%20%5C%7C%5E2%20.%0A%5Cend%7Balign%7D"> Minimizing across <img src="https://latex.codecogs.com/png.latex?x_S">, the first term vanishes (since <img src="https://latex.codecogs.com/png.latex?y%5E%7B%7C%7C%7D"> is in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)">, there is by definition an <img src="https://latex.codecogs.com/png.latex?x_S"> such that <img src="https://latex.codecogs.com/png.latex?y%5E%7B%7C%7C%7D%20=%20A_S%20x_S">).<br>
Thus, an optimal <img src="https://latex.codecogs.com/png.latex?S"> maximizes <img src="https://latex.codecogs.com/png.latex?%5C%7C%20y%5E%7B%7C%7C%7D%20%5C%7C%5E2">.<br>
Since <img src="https://latex.codecogs.com/png.latex?P_%7BA_S%7D"> is a projection matrix, it is symmetric and idempotent, so <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%20y%5E%7B%7C%7C%7D%20%5C%7C%5E2%20=%20%5C%7C%20P_%7BA_S%7D%20y%20%5C%7C%5E2%20=%20y%5EH%20P_%7BA_S%7D%5EH%20P_%7BA_S%7D%20y%20=%20y%5EH%20P_%7BA_S%7D%20y%0A"> which proves the first item.</p>
<p>An optimal <img src="https://latex.codecogs.com/png.latex?x_S"> satisfies <img src="https://latex.codecogs.com/png.latex?y%5E%7B%7C%7C%7D%20=%20A_S%20x_S">.<br>
As the projection of <img src="https://latex.codecogs.com/png.latex?y"> onto <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)">, <img src="https://latex.codecogs.com/png.latex?y%5E%7B%7C%7C%7D"> is characterized by <img src="https://latex.codecogs.com/png.latex?A_S%20%5EH%20%5Cleft(y%20-%20y%5E%7B%7C%7C%7D%20%5Cright)%20=%200">.<br>
Combining the two, we get the second result.</p>
</div>
</div>
</div>
</section>
<section id="greedy-algorithms" class="level1">
<h1>Greedy Algorithms</h1>
<p>Exactly Solving the sparse approximation problem is NP-hard. It turns out there isn’t a significantly better way than brute force checking all <img src="https://latex.codecogs.com/png.latex?n%20%5Cchoose%20k"> possible supports. Thus we often turn to greedy algorithms<sup>2</sup>, which are faster but not guaranteed to find the optimal solution. We will present 3 such algorithms and their Python implementations. We will rely on the following helper functions:</p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> project_y_onto_range_A_S(A: np.ndarray, y: np.ndarray, S: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Returns $P_{A_S} y$</span></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb1-8">    A_S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A[:, S]</span>
<span id="cb1-9">    Q, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.qr(A_S)</span>
<span id="cb1-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Q.T.conj() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb1-11"></span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> score_S(A: np.ndarray, y: np.ndarray, S: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb1-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Returns $y^H P_{A_S} y$</span></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb1-17">    A_S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A[:, S]</span>
<span id="cb1-18">    Q, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.qr(A_S)</span>
<span id="cb1-19">    z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q.T.conj() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb1-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.real(z.T.conj() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> z)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># real is needed only because of numerical errors</span></span></code></pre></div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why QR?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You may wonder why we are using the QR decomposition to compute the projection, and not simply solving the normal equations directly to get <img src="https://latex.codecogs.com/png.latex?x_S"> and then computing <img src="https://latex.codecogs.com/png.latex?A_S%20x_S"> to get the projection. The reason is that the normal equations are not always well-conditioned, which may lead to numerical errors in <img src="https://latex.codecogs.com/png.latex?x_S">, which in turn may lead to numerical errors in the projection. But we don’t really care about <img src="https://latex.codecogs.com/png.latex?x_S">, we only care about the projection. The QR decomposition is used to compute the projection directly in a more numerically stable way, without the need to compute the intermediate <img src="https://latex.codecogs.com/png.latex?x_S">.</p>
</div>
</div>
</div>
<section id="matching-pursuit-mp" class="level2">
<h2 class="anchored" data-anchor-id="matching-pursuit-mp">Matching Pursuit (MP)</h2>
<p>MP is a simple and popular iterative algorithm for the sparse approximation problem. We start with an empty support. Then, at each iteration:</p>
<ol type="1">
<li><p>Add the column <img src="https://latex.codecogs.com/png.latex?i"> that maximizes <img src="https://latex.codecogs.com/png.latex?y%5EH%20P_%7Ba_i%7D%20y"> to the support.</p></li>
<li><p>Update <img src="https://latex.codecogs.com/png.latex?y"> by projecting it onto the space orthogonal to <img src="https://latex.codecogs.com/png.latex?a_i">.</p></li>
</ol>
<p>In Python, it looks like this:</p>
<div id="cell-8" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> mp(A: np.ndarray, y: np.ndarray, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]:</span>
<span id="cb2-2">    S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>()</span>
<span id="cb2-3">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(S) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> k:</span>
<span id="cb2-5">        i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n), key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> i: score_S(A, y, [i]))</span>
<span id="cb2-6">        S.add(i)</span>
<span id="cb2-7">        y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> project_y_onto_range_A_S(A, y, [i])</span>
<span id="cb2-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> S</span></code></pre></div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A more efficient implementation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In MP, we are always considering a support of size 1, so the normal equations are actually scalar equations: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Aa_i%5EH%20a_i%20x_i%20=%20a_i%5EH%20y%0A%5Cend%7Balign*%7D"> If we normalize the columns of <img src="https://latex.codecogs.com/png.latex?A"> to have unit norm, we get very simple equations for <img src="https://latex.codecogs.com/png.latex?x_i"> and <img src="https://latex.codecogs.com/png.latex?y%5EH%20P_%7Ba_i%7D%20y">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Ax_i%20&amp;=%20a_i%5EH%20y,%0A%5C%5C%20y%5EH%20P_%7Ba_i%7D%20y%20&amp;=%20%7C%20x_i%20%7C%5E2.%0A%5Cend%7Balign*%7D"> which can be utilized to make the algorithm more efficient. In Python, this can be implemented as follows:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> mp(A: np.ndarray, y: np.ndarray, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]:</span>
<span id="cb3-2">    S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>()</span>
<span id="cb3-3">    A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.linalg.norm(A, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(S) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> k:</span>
<span id="cb3-5">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A.T.conj() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb3-6">        i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argmax(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(x))</span>
<span id="cb3-7">        S.add(i)</span>
<span id="cb3-8">        y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> A[:, i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x[i]</span>
<span id="cb3-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> S</span></code></pre></div>
</div>
</div>
</div>
</section>
<section id="orthogonal-matching-pursuit-omp" class="level2">
<h2 class="anchored" data-anchor-id="orthogonal-matching-pursuit-omp">Orthogonal Matching Pursuit (OMP)</h2>
<p>OMP is a popular variant of MP. While in MP we project <img src="https://latex.codecogs.com/png.latex?y"> onto the space orthogonal to the selected column in the current iteration, in OMP we project <img src="https://latex.codecogs.com/png.latex?y"> onto the space orthogonal to all the selected columns so far:</p>
<div id="cell-11" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> omp(A: np.ndarray, y: np.ndarray, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]:</span>
<span id="cb4-2">    S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>()</span>
<span id="cb4-3">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(S) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> k:</span>
<span id="cb4-5">        i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n), key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> i: score_S(A, y, [i]))</span>
<span id="cb4-6">        S.add(i)</span>
<span id="cb4-7">        y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> project_y_onto_range_A_S(A, y, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(S))</span>
<span id="cb4-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> S</span></code></pre></div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A more efficient implementation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Like in MP, if the columns are normalized, the selected column is the one with the largest absolute inner product with <img src="https://latex.codecogs.com/png.latex?y">. Unlike MP, we can’t reuse the inner product to compute the projection of <img src="https://latex.codecogs.com/png.latex?y"> onto the space orthogonal to the selected columns. However, there are ways to speed up the computation of the projection by using the solution of the previous iteration, using e.g.&nbsp;incremental QR factorizations. I plan to discuss this in a future post.</p>
</div>
</div>
</div>
</section>
<section id="the-most-obvious-matching-pursuit-momp" class="level2">
<h2 class="anchored" data-anchor-id="the-most-obvious-matching-pursuit-momp">The Most Obvious Matching Pursuit (MOMP)</h2>
<p>Start with and empty support. At each iteration add the column <img src="https://latex.codecogs.com/png.latex?i"> that maximizes <img src="https://latex.codecogs.com/png.latex?y%5EH%20P_%7BA_%7BS%20%5Ccup%20%5Cleft%5C%7B%20i%20%5Cright%5C%7D%7D%7D%20y">:</p>
<div id="cell-14" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> momp(A: np.ndarray, y: np.ndarray, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]:</span>
<span id="cb5-2">    S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>()</span>
<span id="cb5-3">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(S) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> k:</span>
<span id="cb5-5">        S.add(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> S, key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> i: score_S(A, y, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(S) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [i])))</span>
<span id="cb5-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> S</span></code></pre></div>
</div>
<p>That’s it, there are no <img src="https://latex.codecogs.com/png.latex?y"> updates here.</p>
<p>OMP aims to enhance MP by optimizing the coefficients of all the selected columns at each iteration. However, this optimization occurs only after a new column has been added. During the selection of the new column, OMP—like MP—assumes <img src="https://latex.codecogs.com/png.latex?k=1">, optimizing only the coefficient of the candidate column while keeping the coefficients of the previously selected columns fixed.</p>
<p>In contrast, MOMP simultaneously optimizes the coefficients of all selected columns, including the new candidate.</p>
<p>To clarify this distinction, consider the j’th iteration:</p>
<ul>
<li>MOMP adds to the support <img src="https://latex.codecogs.com/png.latex?S"> the column <img src="https://latex.codecogs.com/png.latex?i"> that minimizes <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderset%7Bz%7D%7B%5Ctext%7Bmin%7D%7D%20%5Cleft%5C%7B%0A%20%20%5C%7C%0A%20%20%20%20%20%20A_%7BS%20%5Ccup%20%5Cleft%5C%7B%20i%20%5Cright%5C%7D%7D%20z%0A%20%20%20%20%20%20-%20y%0A%20%20%20%5C%7C%5E2%0A%5Cright%5C%7D%0A"></li>
<li>Meanwhile, OMP adds to the support <img src="https://latex.codecogs.com/png.latex?S"> the column <img src="https://latex.codecogs.com/png.latex?i"> that minimizes <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderset%7Bz%7D%7B%5Ctext%7Bmin%7D%7D%20%5Cleft%5C%7B%0A%20%20%5C%7C%0A%20%20%20%20%20%20A_%7BS%20%5Ccup%20%5Cleft%5C%7B%20i%20%5Cright%5C%7D%7D%20z%0A%20%20%20%20%20%20-%20y%0A%20%20%20%5C%7C%5E2%0A%20%20%20%5C:%20%7C%20%5C:%0A%20%20%20z_S%20=%20%5Ctext%7Bargmin%7D_%7Bz'%7D%20%5C%7C%20A_S%20z'%20-%20y%20%5C%7C%5E2%0A%5Cright%5C%7D,%0A"> which imposes additional constraints.</li>
</ul>
<p>MP and OMP are well-known and widely used algorithms. However, I have not encountered MOMP in the literature (though I’d be happy to be proven wrong). Despite this, I believe MOMP is the most natural greedy algorithm to consider.</p>
<p>A possible reason for MOMP’s absence in the literature could be its computational complexity. At each iteration, MOMP requires solving a least squares problem for every candidate column, whereas OMP solves it only for the newly added column.</p>
<p>That said, there are ways to leverage the solution from the previous iteration to expedite these computations — for example, by using incremental Cholesky or QR factorizations. I plan to explore this in a future post.</p>
</section>
</section>
<section id="numerical-experiment" class="level1">
<h1>Numerical Experiment</h1>
<p>In this section, we compare the performance of the three algorithms on the classic problem of decomposing a signal into a sum of sinusoids.</p>
<p>For each value of <img src="https://latex.codecogs.com/png.latex?k">, we generate 25 samples of a signal composed of <img src="https://latex.codecogs.com/png.latex?k"> sinusoids with random frequencies, phases, and amplitudes. Noise is added to the signal, and the three algorithms are then used to approximate it. We evaluate their performance by measuring the root mean squared error (RMSE) of the approximation over 1000 repetitions.</p>
<div id="cell-16" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> product</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> tqdm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tqdm</span>
<span id="cb6-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb6-4"></span>
<span id="cb6-5">rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb6-6"></span>
<span id="cb6-7"></span>
<span id="cb6-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> generate_data(m: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, n: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[np.ndarray, np.ndarray]:</span>
<span id="cb6-9">    t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(m)</span>
<span id="cb6-10">    f <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(n) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n</span>
<span id="cb6-11">    A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">2j</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.outer(t, f)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(m)</span>
<span id="cb6-12">    S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.choice(n, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>k, replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb6-13">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (rng.standard_normal(k) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">1j</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> rng.standard_normal(k)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> k)</span>
<span id="cb6-14">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A[:, S] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (</span>
<span id="cb6-15">        rng.standard_normal(m) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">1j</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> rng.standard_normal(m)</span>
<span id="cb6-16">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb6-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> A, y</span>
<span id="cb6-18"></span>
<span id="cb6-19"></span>
<span id="cb6-20">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span></span>
<span id="cb6-21">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span></span>
<span id="cb6-22">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb6-23"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> trial, k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> tqdm(product(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>), [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>]), total<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3000</span>):</span>
<span id="cb6-24">    A, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_data(m, n, k)</span>
<span id="cb6-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> algo <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> [mp, omp, momp]:</span>
<span id="cb6-26">        S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> algo(A, y, k)</span>
<span id="cb6-27">        y_est <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> project_y_onto_range_A_S(A, y, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(S))</span>
<span id="cb6-28">        rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.norm(y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_est) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(m)</span>
<span id="cb6-29">        results.append((trial, k, algo.<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span>, rmse))</span>
<span id="cb6-30"></span>
<span id="cb6-31"></span>
<span id="cb6-32">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(results, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"trial"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"algo"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rmse"</span>])</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 3000/3000 [07:39&lt;00:00,  6.52it/s]</code></pre>
</div>
</div>
<p>We now plot the distribution of the RMSE for each algorithm, for each value of <img src="https://latex.codecogs.com/png.latex?k">.</p>
<div id="cell-18" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.express <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> px</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.io <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pio</span>
<span id="cb8-3"></span>
<span id="cb8-4">pio.renderers.default <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"notebook"</span></span>
<span id="cb8-5"></span>
<span id="cb8-6">px.ecdf(</span>
<span id="cb8-7">    df,</span>
<span id="cb8-8">    x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rmse"</span>,</span>
<span id="cb8-9">    color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"algo"</span>,</span>
<span id="cb8-10">    facet_row<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>,</span>
<span id="cb8-11">    title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ECDF of the sparse approximation RMSE"</span>,</span>
<span id="cb8-12">    height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">700</span>,</span>
<span id="cb8-13">)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="3dbb5e4c-5006-4b0a-827e-daaab2222511" class="plotly-graph-div" style="height:700px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("3dbb5e4c-5006-4b0a-827e-daaab2222511")) {                    Plotly.newPlot(                        "3dbb5e4c-5006-4b0a-827e-daaab2222511",                        [{"hovertemplate":"algo=mp\u003cbr\u003ek=10\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"mp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#636efa","symbol":"circle"},"mode":"lines","name":"mp","showlegend":true,"x":[0.03056728339849537,0.03265687599930859,0.033849590795741936,0.034964948276372816,0.035396430870465026,0.03551634832802999,0.0364742716881324,0.03684885553516027,0.03686490336239541,0.0371494281997881,0.03746339895466654,0.037629178137708955,0.03774773812668084,0.03781188630629391,0.037827793119056916,0.03840663406258753,0.039674200352486866,0.03983512271193134,0.039941271923397595,0.04008592450388148,0.0401013099309182,0.0401150474882143,0.04014963556240673,0.040318394214934036,0.040382761627486906,0.04038278381327162,0.040557257175663296,0.04063410591072468,0.04064725621864961,0.040768277711588544,0.04104547640780012,0.04104632950925031,0.04107633567723044,0.041162442772673065,0.04119397098276141,0.0413991450477033,0.041557224784126365,0.04159113054892156,0.04160278982105821,0.041712507673158886,0.04185522541256241,0.04197112259530353,0.04204282100099156,0.042176426353804676,0.042222953024758106,0.04228803681407392,0.04229729885665386,0.04233976351412549,0.042375989785728256,0.04238874890743009,0.04257506024347722,0.04264983024318762,0.04283458318546028,0.04296539045672679,0.04301681053008125,0.043046204859107325,0.04310052799449013,0.043159311479968286,0.043221553613498206,0.04328709930082631,0.04338372527388304,0.04340870666582348,0.04341331366592025,0.043420526852660374,0.04342390547097276,0.0434645500525795,0.04348818085167029,0.04349892715588914,0.04352460459389054,0.043608765773648285,0.043630834774073146,0.04366474164369488,0.043705660018396694,0.04371082652663993,0.043719965791392326,0.04374195839350664,0.043750447872117654,0.043760071689201416,0.043766838275480294,0.04377604282445502,0.043856304769488805,0.04391540683064029,0.044010668403684824,0.044061746394238696,0.04407469381464905,0.044082313124469426,0.04421253540450522,0.04427188985417931,0.044378592223612914,0.04439528782178145,0.044421128908816464,0.04443498107812162,0.044501532428462394,0.04453781578728035,0.044560873486117675,0.0446634636095152,0.04467383900386455,0.04470344544079178,0.04473178026087718,0.044737045690885845,0.04474698285412103,0.04476329859247304,0.04487368214436742,0.04496643067791206,0.04496777044957398,0.045028991011730954,0.0451523517264435,0.04533249094400831,0.04535501627140644,0.0454595770801033,0.04555933964926538,0.045577947069141166,0.04558003619972706,0.04558732953158721,0.0456016135299706,0.04561961752675876,0.04563415656448005,0.04567842142011228,0.04572641400365736,0.04573304787496805,0.04577164526628759,0.04578513673692719,0.04587421707813373,0.045879805514129166,0.0459201035266519,0.045935545317994805,0.04603288073323562,0.0460354903040623,0.04610830572845171,0.04610912272924989,0.0462228195223789,0.046234101370230034,0.04626502132096365,0.046346424163105936,0.04641776762480362,0.04642795810906904,0.046438320816489555,0.04648575525285805,0.04661055650465916,0.046644819833885805,0.046746231425784136,0.046791672564894166,0.04679904459312328,0.046809378194513,0.04682964949156712,0.046872227108412864,0.046892984103622935,0.04692805544102409,0.046959649869897015,0.046964970229477834,0.04697347735533041,0.04699211743086938,0.04700303683107562,0.04700918292973214,0.047069048792340494,0.04709408287657808,0.04709623051238809,0.047102606254987266,0.04716496095824297,0.047186381688294955,0.04726547566584571,0.04727073169360006,0.04729003476582734,0.047338882453211065,0.047340511692539264,0.04736433664339819,0.047369576187284754,0.04738248720071146,0.04751822605237351,0.04752056954617126,0.04755385205234076,0.047561180508553236,0.04757287831099903,0.04763675512410042,0.04767182972929686,0.047676879900397456,0.04767808147317728,0.04772669299991436,0.04774358537389052,0.04774579433785553,0.04775450437891668,0.047793266986611606,0.04782546320796553,0.047853337453871755,0.047875178259565776,0.04790502329099011,0.04791635229728588,0.04792923557425168,0.047937307597861974,0.04794469584165322,0.04794778262964327,0.04797218328095788,0.047972658678626094,0.047998615042902745,0.04802308575685834,0.04809644494598277,0.048111027635115544,0.048128301379488134,0.048136989045736434,0.048137596776254496,0.04817801850860268,0.04819406617703233,0.0482670293924071,0.04829055334411327,0.04829896049321439,0.04830326587480081,0.04830523141571943,0.048306355866266124,0.04833791615865969,0.04835345034027802,0.048385432302335885,0.04842520576124764,0.0484444057999226,0.04844601585689266,0.048495062639327555,0.048551257965693324,0.048553292907578496,0.04856554697767876,0.04857256732349917,0.048658087372551465,0.0486817220856281,0.04868633991016785,0.04869442202789048,0.04873400504556652,0.04878460823947721,0.04886626098230132,0.04890398855498395,0.048922988447407764,0.04897456512698499,0.0490102555208539,0.04902511184766768,0.04904926946953887,0.049068937674633814,0.04908737915364521,0.04911761796884549,0.049127819089764976,0.04913514475368919,0.0491440781731483,0.0491557589145599,0.0491652670137868,0.04919282797481122,0.04921666939296708,0.04925948797274646,0.04926387486557242,0.049276237142044925,0.04927849077333918,0.049333642053990955,0.04940762106429766,0.04941062935268876,0.04943698970639453,0.04944043234854928,0.04944510664105724,0.049447730291991024,0.04945241281797938,0.049647872941493594,0.04965623626817235,0.049657619045288814,0.04970737088622681,0.04972132327682636,0.04972348876132482,0.049782704804426835,0.049815176195610454,0.04981993594773361,0.04984254825386676,0.04984417145128846,0.04986449329177974,0.04988290750550651,0.049887856221478295,0.049900448498039124,0.04994106041044011,0.04994789089215247,0.04998146183218958,0.050096721209896646,0.05011958157748185,0.05014618186989327,0.05019142738583726,0.05019486256323913,0.050220554987328095,0.05024495183640278,0.0502509936992271,0.05029674860466419,0.05035792438787098,0.05038363022544838,0.05039431165823177,0.050433606038369695,0.05045915530426835,0.05050229998205881,0.05054358012473277,0.05059178106147162,0.050596693329023404,0.05062489305139711,0.05063049017272685,0.05074680293054087,0.05074893922453677,0.05075589177828675,0.050770339414640264,0.050780712993452125,0.05081821080809995,0.05082141340887654,0.050826494742473874,0.05084576164461825,0.050867928774337455,0.050895938138323896,0.050907634095279396,0.050949775477348705,0.05095467302411229,0.05096688478864334,0.050968982305425514,0.05097984829232803,0.05099176881671148,0.05100921661173837,0.05101265164230193,0.05102616326873318,0.051036232817736324,0.051055485255156186,0.051105590389467916,0.05114280695088239,0.05114378055164594,0.05123311554703451,0.051273382959705315,0.05132126653461381,0.05136240294984895,0.05136782154905667,0.051387174168697106,0.0514247456350073,0.05143811532870053,0.05144763470406436,0.051483295087964495,0.05151505457602652,0.05152087147935146,0.05159683838920985,0.05162355202251047,0.0516537167807274,0.05172590696177739,0.051773201392176525,0.05184207687953414,0.05186617480282836,0.051911133577599744,0.05192798133171903,0.05194103105667257,0.05194823714147199,0.0519581743181504,0.05199151927675684,0.052039491982113176,0.052044942923618887,0.052093998447994116,0.0521201193585473,0.052157481905350266,0.05217253089853169,0.05220427365363829,0.05223539557323783,0.05227673496630506,0.05227792117074166,0.052289122830312174,0.052289462259851004,0.0522939242514022,0.05230008587262898,0.052329698178714426,0.05233016634059558,0.05234058028399036,0.05235611471722327,0.05241365381604379,0.052437256908511476,0.05246076738269412,0.052473435495035914,0.05247920862104064,0.052500612205038245,0.05255849368400458,0.052582058805052914,0.05264114395435486,0.0526534087230764,0.05265900205282855,0.05267271000534496,0.052697373952251145,0.05273257992594247,0.05274403803883122,0.05276625885706967,0.052779316011531095,0.0528711141684577,0.05289356584555185,0.052896675345752375,0.05290678253603433,0.052967231515770194,0.05297786546142993,0.05299017144789849,0.05299420421329032,0.05300113896171921,0.05302202975231139,0.053044727249829796,0.05305176450586478,0.05308711252030531,0.0531173170101123,0.05318277874540909,0.05319952902289741,0.05325327310572141,0.05336536509084021,0.05336724666788846,0.05338410465958389,0.05347923331692631,0.053483031089565834,0.05348983456688643,0.053516953865218465,0.05357394983157467,0.05357828348710292,0.053599867753571506,0.05367558542417189,0.05372130867779634,0.05373779042691105,0.053738075502926454,0.05373940731497852,0.05377788644182537,0.05380134479903601,0.053885294433081134,0.05389437685042668,0.0539051512166054,0.053909534408855764,0.05390987355969111,0.053925286338665536,0.05393302307078115,0.053934701792076545,0.053954327464965944,0.05396052570023376,0.05396656609121595,0.05399074288057356,0.05399189950974139,0.05401888594464201,0.054065302022281304,0.05410792919867724,0.05412081172439144,0.05417224844647409,0.054201603938071906,0.05420924395180654,0.054224974268391325,0.05425772181855267,0.05426106766793357,0.05426115805613193,0.05428946663069145,0.05429302119764247,0.054306174934492725,0.054314865585340946,0.054331430211426904,0.054340419511294355,0.054345123138533014,0.054348732820147815,0.054351774443812985,0.05438339432956056,0.05438889401413057,0.05441883554631969,0.054435803796412975,0.05452176689779477,0.05455300001820952,0.05457025519739622,0.05461641809088276,0.05462205805790889,0.05463058456407097,0.054659313312221236,0.054662884908365204,0.05467720640616817,0.054755797800920304,0.05479125860806663,0.05480379421959873,0.054879489788051995,0.054900802780441224,0.05492990183352461,0.05493172551693114,0.054946109019369097,0.05500711897921436,0.05501545560675419,0.05505522546244405,0.05506941203032216,0.055085609954183636,0.055113308969777876,0.055126882745166514,0.05513800759970953,0.05515905654209556,0.055204926235085064,0.05521204004477328,0.05524161061379938,0.05524243884195364,0.055271147521574616,0.05528540327601424,0.055293645501345946,0.05533049130472302,0.055334051665961045,0.05533823105964138,0.055353267189523415,0.05535719252454489,0.05543718569803885,0.05545042042939904,0.055524041247483114,0.05557809325754961,0.05558364550856276,0.05561246788692229,0.05562159995993791,0.05563707111028574,0.05566307871614479,0.05567293885718887,0.05571623176324388,0.05575314730926996,0.0557677063515559,0.055777759060339015,0.05578396674795387,0.05579502159139458,0.05579519356588013,0.0558149892394353,0.055885353513089445,0.055895226831407796,0.05593003472936396,0.05593495402034254,0.055980973775571964,0.05598563127366437,0.05599096041954219,0.056022768686907254,0.05602575575680404,0.05602907247513063,0.056030329908774934,0.05603164259342445,0.05612471101991874,0.056141574026408056,0.05614769783131529,0.05615440262862481,0.05618721110180702,0.05619714073498504,0.056253854331733834,0.05626850583427821,0.05629776165721687,0.05632295481018838,0.05636492987761756,0.056380758519031224,0.05639592776648916,0.05640370563994533,0.05641215987886612,0.056416199257773546,0.05645791179100239,0.05647228549190749,0.05652970970645045,0.05653465543630004,0.05654714580653928,0.05655426468068401,0.05658478592545413,0.05659008372811053,0.05661400986138021,0.056627414940960144,0.05665404601602654,0.05668502417966761,0.056706919981284,0.05671251930360714,0.056714292819666036,0.05672186218714944,0.05675232305048692,0.05677229204828912,0.05678535387791341,0.05679347185030037,0.05679945725921217,0.05681730558229604,0.05682509837820392,0.05685283977742874,0.056886787130703644,0.05690926012733414,0.05691483224218843,0.056928270059898534,0.05694017908051839,0.05698206684601109,0.05700355961236818,0.05700914771560835,0.0570159678562616,0.05702766965488051,0.05709333352445585,0.0571502474071071,0.05718435077746681,0.057248912566064944,0.05725460062260816,0.057260312216036544,0.057280146496850905,0.057294690046805465,0.057326098699712155,0.05732741018302664,0.05732907996777675,0.057340352775572326,0.0573754319449266,0.057381686616270536,0.05739497287470526,0.05743703340344745,0.05744821666046294,0.057460112709000664,0.057489187763943496,0.05749137840057643,0.05753441426978079,0.05755123208880616,0.05755785647476487,0.05757102032554225,0.05759149808863133,0.05759541995775208,0.05759724192775505,0.057597671543707075,0.05762317493925199,0.057654410510367474,0.05766332811449153,0.05766570319151628,0.057678153351770844,0.05768222031890902,0.05768770207971972,0.05769420994710857,0.05769514763305565,0.05771512989272067,0.0577171404967175,0.057815159718097565,0.05782249089342274,0.0578259668064092,0.05788560669197117,0.05789747068117057,0.05793221897600924,0.057959568143489126,0.05797223331938077,0.05799127919623599,0.05800037784470015,0.05801365729783672,0.058021194787962915,0.05806747755935919,0.058090279527277855,0.058125023531360176,0.05815023070153095,0.0581997840766589,0.05820743390384601,0.05821588571444666,0.05823254789314206,0.05823345460798991,0.05823555791252634,0.05824090983885301,0.058242213265561296,0.05828110816062092,0.058339427759543226,0.058340246564776,0.05837853142859709,0.05838885412782761,0.05840903254801129,0.058456243342333014,0.05846715071107507,0.05848786201669805,0.058489313121948786,0.058500957466020374,0.058537641344283534,0.05853833317649983,0.058544125527749256,0.058660199967030424,0.05875569515392125,0.0587833561126645,0.0588192640203957,0.05882847390724767,0.058906552558530015,0.05892845889647615,0.058987172158439495,0.058987727655410195,0.05899424718595042,0.05900449977027207,0.05901778079198601,0.059053149687573504,0.059059514394449376,0.05910445600663296,0.05915469345455129,0.059219108213542794,0.059236854624233434,0.0592619344466339,0.059266053978245414,0.059266132275227555,0.05928175787904523,0.05928605834073582,0.05930929033755398,0.05932634823255032,0.05939431498520735,0.059398086774450465,0.05947055094878956,0.05947301859688057,0.05949609452014287,0.05951093419300084,0.059537862555677966,0.05954208481156659,0.05956661837763931,0.05957325176333435,0.0595899924914474,0.05974866102695979,0.05977300886538363,0.05981419414737736,0.059835044050058,0.05990173644356668,0.05992351467751188,0.05993077612724111,0.060042471381880756,0.060052527098539664,0.060103104359287285,0.06011305880123706,0.06012819557388612,0.060277451631474854,0.060280460834211125,0.060284845829833754,0.060345871411574215,0.06037529843985321,0.06041431102138314,0.06044044381619308,0.0604826813175706,0.060500837466604264,0.06056084970674528,0.06062741387922084,0.0606735960192427,0.06076028389573977,0.060763090256310257,0.06079216310830289,0.060797169903949635,0.060824174327462434,0.06086212773169279,0.060872648509944983,0.06089366290585776,0.06090099869429375,0.06092875565564796,0.060937947903268244,0.060981205207262135,0.060992513271899064,0.06101216016357963,0.06103281710388464,0.06104585397654479,0.06106123383737725,0.06106813388927723,0.0610720596579478,0.0610911966703076,0.061100408923071024,0.06110406546293838,0.061144407347767896,0.061153933068126456,0.06115584960681787,0.061181738383717835,0.061195055539065055,0.06124408662481858,0.061261907633835544,0.06127598526575122,0.06129467825557944,0.06131379062231512,0.061328573860426325,0.06136076931550816,0.06136324130851659,0.061384336947172456,0.06139510817541173,0.06142320257289123,0.06145615967327467,0.06148438099411777,0.06148977880259078,0.06159989844764102,0.06162294952918642,0.061648379090858366,0.061664994097992976,0.06167118253080761,0.061680515260616874,0.06178407673059374,0.061794173874473265,0.061804121405643474,0.06185313649615369,0.061870556635836405,0.06191914487190886,0.06195210440005387,0.061958788426250924,0.061969503709202546,0.06196997031490398,0.061982897484712564,0.06202170955476856,0.06202730072982786,0.062030271758974666,0.06208948563890669,0.062097538932212704,0.062100891709627605,0.062112668705446274,0.06214715588343404,0.06218104726107967,0.062208000026113174,0.062219428971368076,0.062233382631482356,0.062263990276881676,0.0622711658038119,0.06232087875140856,0.062350596881128165,0.06236897659984075,0.062391741345327045,0.06239400030449114,0.06239438051590016,0.0624020228625914,0.062421032247271704,0.06242692503030902,0.062446249783240114,0.06249035335801888,0.062495965723992505,0.0624971564494637,0.06254932057840282,0.06257165894820198,0.06260865040174293,0.06263519612965343,0.06267285623641125,0.06269717357584156,0.06292311851134433,0.06292358777960619,0.06293938972481346,0.06294753339254598,0.06296863530388497,0.06299449604390919,0.06303475640106194,0.06319187971551302,0.06319623180351301,0.06322250984124718,0.063258531985906,0.06330992795196996,0.06334036354248607,0.06334824906421352,0.06335745296175253,0.06339507881954506,0.06349021610391467,0.06349285839674117,0.0635357985253966,0.0635467770020258,0.06358073210114192,0.06361400496158229,0.06362219789963669,0.06367738360974888,0.06368458368688776,0.06371388741002267,0.06373323601062958,0.06374650773137537,0.06378381037179097,0.06383065326523105,0.06394068483180436,0.06397573977656187,0.0640009204074222,0.06400392249755618,0.06403642177020237,0.06403878540146687,0.06405963952949235,0.06410725984258484,0.06410978718491063,0.06413824808794877,0.06417168845549609,0.06422340137216712,0.06422915419744625,0.06433449443025344,0.06435006840322272,0.0644070172067011,0.06448941441339907,0.06450750976286225,0.0645166868905194,0.06451785701725857,0.06453469176382029,0.06457066957333654,0.06457230943513494,0.06461317121493405,0.0646522400315496,0.06465333134108711,0.06472180428571411,0.06472705199940182,0.06474879227363152,0.06475558688503505,0.06478428887251933,0.06478812640601767,0.06489763461212475,0.06489780530656847,0.06491257139533325,0.06496552152615101,0.06499199485661977,0.06505288296271561,0.06507723617117925,0.06513149930096314,0.06523691316389288,0.06526263610889803,0.06533572570281734,0.06535471689251413,0.06536692700108812,0.06541817321643693,0.06550488710243513,0.06551843410969321,0.06556781624649546,0.06558806379949447,0.0656004156560318,0.06567619675267825,0.0657290254933143,0.06583630848571277,0.06590805769201429,0.06596956405133778,0.06597872426411777,0.06601392954100489,0.06601431972962181,0.06607052056312583,0.06607160801863507,0.06615786199353185,0.06621637105438037,0.06625056935563636,0.0662517397976657,0.0662558553203803,0.0662954757872596,0.06635468420977066,0.06636454987544302,0.06667046948319415,0.06674037347659893,0.06675618190077778,0.06678536908130474,0.06679187748597085,0.06681613311603862,0.06687628345939531,0.06688783061017217,0.066983881784122,0.06698463906405272,0.06699052039536753,0.0670556983336348,0.0670757842754117,0.06711452975627961,0.0671293628809066,0.06714879182596654,0.06717734428810981,0.06738074495439159,0.0675052897148208,0.06750704973650763,0.06771557994964061,0.06772515812535086,0.06789838769077912,0.0679476155821884,0.06795106135598243,0.06796576065109657,0.06796924692963337,0.06810544819227261,0.06812205551348877,0.06814750920676457,0.06847352975197762,0.06855711080557593,0.06857675857425022,0.06869092281014552,0.06872518283129583,0.06873311159360354,0.06890922024172799,0.06896198789802799,0.06914356184746022,0.0692620798165545,0.06930214460702112,0.06941553606756462,0.06947649808945977,0.06954035438410996,0.06954652038312217,0.06959326816945312,0.07001927285480362,0.07018659784225614,0.07019022922566188,0.07029766880190726,0.0703284568491352,0.07034066656020024,0.07042967676772849,0.07053033764308145,0.07055073102284927,0.07061024241865974,0.07074651036822781,0.07096568774432413,0.07106843010461103,0.07115789907684564,0.07116316453692871,0.0714513025891849,0.07175025068059057,0.07197056612926779,0.07207501678389167,0.0724181235935872,0.07249203708441007,0.07272698471926418,0.07273056583097458,0.07278983494939098,0.07293165966511436,0.07307028531031581,0.07317108618722093,0.07322676186371282,0.07336438413249216,0.07337625159602594,0.0734307416399778,0.07343594302306711,0.07355657577432453,0.07392043267530991,0.0739743670544311,0.07411109051480182,0.07425137331639081,0.07463301854242757,0.07474013552834366,0.0752031196368365,0.07549490823314858,0.07554320757341355,0.07573361645307161,0.07588009208168794,0.07598219032748668,0.07616960805596933,0.07724387811310687,0.07764489905300641,0.0780542770434027,0.07894058573544216,0.07981360763268934,0.08190046381383945,0.08274894406975902,0.08382406414357714,0.08451388148045466,0.08602617031547176,0.08667862121851413,0.08699072836686549,0.08730032895268398,0.09361534582250254],"xaxis":"x3","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y3","type":"scattergl"},{"hovertemplate":"algo=mp\u003cbr\u003ek=15\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"mp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#636efa","symbol":"circle"},"mode":"lines","name":"mp","showlegend":false,"x":[0.012915762972378711,0.014847909634603303,0.016371322202676723,0.016764057276244713,0.017076710774702172,0.0173273199930388,0.01811239068712608,0.018221134282723837,0.01854616889219903,0.018599036774243457,0.01863114509057586,0.018789369917029265,0.01909109045583559,0.01910391185376033,0.019105484670559468,0.0191434733332715,0.01916094233597887,0.019169983195080512,0.019235773922393416,0.019376016943885722,0.01938512660498763,0.019446944776123423,0.019596671101026687,0.01985043310324374,0.01994660307055844,0.01997986642544553,0.01998196292581604,0.020139284357321163,0.020205185142811697,0.020218298476133482,0.02026539416399659,0.020318497644396187,0.02034759771789504,0.020356676298233796,0.02073110398420331,0.020913572981904528,0.02091876764295658,0.02096716633514013,0.021044813604018005,0.02115367246519247,0.021190642422403202,0.021195589433027514,0.021308035196256646,0.02152260489373383,0.02160757094567289,0.02167565324816646,0.021682359618595735,0.021753332673926644,0.021788575590069373,0.02180700571310609,0.021907977285452775,0.021982290228766865,0.02201271802908511,0.022017194264467584,0.022120853382469007,0.022122024938509757,0.02218103854080595,0.02247025556227557,0.02251926118092108,0.022621279960589727,0.022648637083128042,0.022666709734319862,0.022893046041142506,0.022893405156055355,0.02289803062318681,0.02290619488703414,0.0229253746311994,0.023049764683909135,0.023101467326047634,0.023130549808204136,0.023229781773329305,0.023234439458027813,0.02323803733198189,0.023304372503843398,0.023314879605359,0.023319081187048892,0.02334728468206599,0.023390532821852852,0.023391373503999596,0.02344089804845339,0.02345049030491188,0.023452755326490413,0.023476176642037418,0.023492022725978962,0.02349526118547385,0.023504105092672426,0.023538614738927315,0.023583340661614452,0.023642671635602377,0.023730615613421255,0.023825603280982988,0.02385714255417755,0.023857415531085186,0.023874657430652875,0.023879706001985,0.023902558258328856,0.023940253918734326,0.0239596402745701,0.023962890884051236,0.023983454338385456,0.02399148427613076,0.024006157806251042,0.024064672172757286,0.024065768839842214,0.024118422144335485,0.02412611972385184,0.024198444966206968,0.02420275879043161,0.024212661922201555,0.02422846270225413,0.024231415248396403,0.02423434729460555,0.024249384902001563,0.024253117806013323,0.024299151782334674,0.024349765898161922,0.024372451746739722,0.024401067527357843,0.024424120541013037,0.024429826724720934,0.024433822572131517,0.024476951250528654,0.024493718514729045,0.024534320320430514,0.024568786550453157,0.024609612933567074,0.024718745467002028,0.02473254418033828,0.02475321021871472,0.02475768254114011,0.024765291555603103,0.024768132338472984,0.024801963638162706,0.02481005149830823,0.024814081334767913,0.024854554812262515,0.024860725308507872,0.024876461630195544,0.024901577729083444,0.024931802728165108,0.02498360368029496,0.0250117637474978,0.025026286091670225,0.025055058161616857,0.025060072468541257,0.025082070165300358,0.025093984496291417,0.02511229498740829,0.025138162788341124,0.025169351835792865,0.025194111167658732,0.025227739605367134,0.025289138094749364,0.025294800384961773,0.025300980864391236,0.02530855230489747,0.0253095289792238,0.025353959956309586,0.025382984630754158,0.025407785996055886,0.025410570416371125,0.025415917386984503,0.025444369162304505,0.025446926599037894,0.02546778690767943,0.025489086176702473,0.025494024287694577,0.025498398990476533,0.025498805962869513,0.025534835106900734,0.025542076364523925,0.0255481998032352,0.025553452922974885,0.025559997293217264,0.02558936408552388,0.02561136006935094,0.02566423444519781,0.02569040661240482,0.025704878728804982,0.02570958404462646,0.02571676657612724,0.02572180333390801,0.025732497382501912,0.02574854897369862,0.02577270853778486,0.025775412209160457,0.02580603332952222,0.025807786352558138,0.02581665905126424,0.025847528991563666,0.0258556840620641,0.02589127111298036,0.025893388956814294,0.025895101813897,0.025908226608752465,0.025994713809230057,0.026003302667809147,0.02602230561757412,0.026042358719262887,0.026070101729824907,0.02609560299449845,0.026115859022405215,0.026116085308060928,0.02615551273406802,0.02618545436567895,0.026197282559986136,0.0262062363435921,0.026207071944063486,0.026212535861245084,0.026213442996654512,0.026256975754703326,0.02629073796566453,0.026307800209924802,0.02633043822271975,0.026344126035068933,0.02635551071109426,0.02640555998347404,0.02641271076731915,0.02643552833602037,0.02646044937562042,0.026494645321075337,0.026494693925007622,0.026495591312998796,0.026505721371200202,0.026528701925567378,0.02659925149280156,0.02664301561502949,0.026662646305816818,0.026676001060659753,0.026701625744460954,0.026740152194603035,0.026741649369326798,0.026759500317688978,0.0267615887520966,0.02681065099343773,0.02682142391116931,0.026828920176551675,0.026841157067841986,0.02685388899443724,0.026862005865319942,0.026868646974952753,0.026873901926326278,0.02691147064183938,0.026925134992117773,0.02697340219562718,0.02698611582644639,0.026993411565158727,0.026995121061080096,0.026998404041962375,0.027010441911826465,0.027097956514377484,0.02715658544473452,0.027183016239346762,0.027248177077309164,0.027276624491387395,0.027322063012081614,0.027348188414070922,0.02735383850909396,0.027356239338967497,0.027432331868995304,0.027434929109418898,0.027448101785778018,0.027502214225532767,0.02752054646372923,0.027533141307455527,0.02753830998210142,0.027544989147580516,0.027546528929414206,0.027570791357869766,0.027574357983641218,0.027584381150856197,0.027598189008588965,0.027648218982132673,0.02766422852619848,0.02770781254020955,0.027717342569524444,0.027723609911111825,0.02772819868838693,0.027738273912040468,0.027774867927656453,0.027777931597563504,0.027800100615893497,0.027802531083648968,0.02783878684675294,0.027846462267517323,0.02787771496184724,0.027952379938077226,0.027969741152313793,0.028043668683682393,0.02805343663189939,0.028097309605979413,0.028118770200437303,0.02816166857807175,0.02818471858424676,0.028213440191467126,0.028225868482211876,0.028251342844379718,0.028253231619420914,0.028272650168672482,0.028296925107168407,0.028308418365878517,0.028326796440095513,0.02834983732388114,0.0283652734846502,0.02838885778938157,0.02839841755318352,0.02840714900185668,0.028440112201860258,0.028450659280531938,0.028469705762985548,0.02847083773235644,0.02848682572113253,0.028491998572517507,0.028533997677002498,0.02854597623611438,0.028548223710715824,0.028561461350022428,0.028602290668815988,0.0286332479817634,0.028669073202051375,0.02870859252536494,0.028717627666421042,0.028768940224953733,0.02878213981678044,0.028825030263884777,0.028828993035592432,0.028833948913359015,0.028875945166922528,0.028878949039466417,0.02892266656386208,0.028938417991903588,0.02899460999933971,0.028999438880953937,0.029009960979455962,0.029029789863536083,0.029056583097247696,0.02907159334454971,0.02909435917007926,0.02911218889545299,0.029236862212163706,0.02924184612287519,0.02926310208176806,0.029277925061738853,0.029286475673934532,0.029286607791557073,0.029292631776001825,0.029298950641702654,0.02932243340014782,0.029322615308292506,0.029330844580572397,0.029333251496790412,0.029341511985704095,0.02936218204383086,0.02941841033799336,0.029425195978264273,0.02943025781523941,0.02944309302055224,0.02946264398705359,0.02946281200005601,0.029464931481917273,0.029508729135349798,0.02952777001311672,0.02953108839281867,0.029558898436675606,0.029567697296346913,0.029575588573217366,0.029587371297799963,0.029599090066355128,0.029618732240957606,0.02965055192558048,0.029663655098578885,0.029673710480531645,0.029691663366351047,0.029717516081631274,0.02974992802070637,0.02975328318074308,0.02976623213312064,0.02977795847825781,0.02978989481474077,0.029803585685808605,0.02980749175666894,0.02981408435706829,0.029816042237900027,0.029817284715059816,0.02989254518118799,0.02989597639825959,0.02992325956873989,0.029930378305864168,0.029931501304027754,0.029940221225734115,0.029941934161024624,0.029960794654340338,0.02997514649290045,0.02999638022861943,0.03000038339903415,0.030046691282112403,0.030069111421062715,0.030072341729315565,0.03011436030994527,0.030136352172614657,0.030156076188900365,0.0301809124174142,0.030200238709896625,0.030242947886040678,0.03028951589263696,0.030331009297592526,0.030367852225421967,0.030377653534806037,0.0303832478165811,0.030389890828044074,0.030397285183920153,0.030411517235568092,0.030433826401204418,0.03044471402024996,0.030509041863247627,0.03051114790986755,0.030535197871105584,0.03055934000421156,0.030564819834126355,0.030571917647147534,0.03060283629462035,0.0306038226695994,0.030614809954802702,0.030625768910836165,0.03063038223383923,0.03065420846440488,0.03068251971706969,0.030705008066665422,0.030710436154333758,0.03071867801421112,0.03075405145222742,0.03075415573780303,0.030767536391362253,0.030799079912422346,0.03081948496448645,0.03082119352207948,0.030824611221574955,0.030837865806439112,0.030847390870843837,0.03085078446688274,0.030866681270415714,0.030874702408478637,0.030909482840690127,0.03091469022066014,0.0309320530227786,0.030946523043896872,0.03094708209854573,0.030976286563776167,0.03100228303288068,0.031007729745762307,0.031022503650176537,0.03103652074573195,0.031039934338116866,0.03106488486689522,0.031072580591976644,0.03108019554906751,0.031148624116954775,0.03116680468875785,0.03118937874096243,0.03119729088040944,0.031210392102314445,0.031247285845367588,0.03128929922449177,0.03128996156535161,0.03130647037957558,0.03134343100854921,0.03134717661954983,0.03139905035686754,0.0314302120125536,0.03146448056790857,0.03152253636939438,0.03155185153177249,0.031555934708020726,0.031561466888534985,0.03156401160577427,0.031615845898895704,0.03162729316942553,0.03163191108231155,0.03163444044687278,0.03164306059819336,0.03165182565303516,0.03165660661304114,0.03166114923164448,0.03167658512336702,0.03168217894468452,0.03168218811015802,0.03168426252975619,0.031716571922820916,0.03172305569542158,0.03174359285911469,0.031767197861171576,0.03178551117468131,0.031802278360407374,0.03182789529556866,0.03183884345438608,0.03183929038048426,0.03183981418360533,0.03186335970693648,0.03186564873052987,0.03187772484258598,0.03190788548983952,0.03191545394743356,0.031927435291507225,0.031936488037799596,0.031936533359500356,0.03197771447559863,0.032021412533509264,0.03202211579965343,0.03206597786557386,0.03206634933687749,0.03207774406389484,0.032084829894041905,0.03209679411366327,0.03213637022773409,0.032154017757299924,0.0321725768689217,0.0321732893796347,0.032180288931221615,0.032184145791118966,0.03220607067340338,0.032208848960844735,0.0322108183662662,0.03222631253451651,0.0322328972312761,0.03223601400317712,0.03224354815883025,0.03225797977901866,0.03227938795721609,0.032330909008123736,0.03239688593029162,0.03240199767538407,0.032409209983848254,0.0324142096707455,0.032430325107534914,0.032440544399287514,0.032451457622997344,0.03245829728718237,0.032513543662318414,0.032518903022928966,0.03257532499163616,0.03258343012185593,0.03260691122281501,0.032609827103075834,0.03265408374533531,0.03268885495032029,0.03269927064835908,0.03270404013442274,0.032712608151635576,0.03274333341197363,0.03274417482639201,0.03276594020610877,0.032785470001315334,0.03281211219706363,0.032821279604737554,0.03282882669608908,0.03284762106090442,0.032852685852435594,0.032902662697108905,0.032910890829992816,0.032942212757597074,0.03294558895645904,0.03295498524507133,0.03295682004706874,0.0329680501352516,0.032996616252568965,0.03300287339950533,0.033020934935964134,0.03302497303051214,0.033062681653224235,0.03308597570812246,0.03309155150656388,0.033169973081944734,0.0331841787722564,0.03320743068489831,0.03323227395738422,0.03323440151403297,0.033239413535992156,0.033254784643751575,0.03326389154575531,0.03327943001689647,0.033297551157452795,0.03330508326520894,0.033343033781623385,0.033373481443551,0.033438794291537996,0.03344168072332896,0.0334880372765415,0.03348895263988207,0.033535909652532545,0.03353735279527921,0.03353828756401149,0.03354733303933391,0.03357495995436454,0.03360057957868045,0.03360286684945503,0.03360994489741788,0.033614783242271476,0.0336172327548308,0.03362373398296818,0.03363680320392726,0.03367011933398352,0.03367991192764508,0.033697922273220715,0.03373553199037828,0.03373882022331755,0.033772460522922244,0.033866465448154956,0.033882057536554225,0.03388343495363082,0.03391170948743829,0.03392722180489992,0.03398492432452094,0.03413308293037669,0.034168057079095225,0.0341682074381672,0.034241414691913274,0.03426075499170238,0.03429520504250054,0.03429777935794955,0.03430918231334419,0.03431318346284469,0.034336039979932155,0.0343547890165007,0.034364680170751706,0.03436488785399102,0.03438551855582546,0.034415994013018335,0.03444616839854285,0.03449323601035308,0.03455930458040899,0.03456232616182548,0.03457056334125842,0.03459849806243996,0.034644721641873634,0.03464768637385333,0.03464886206100738,0.0346490686812572,0.03465072455016112,0.03466217881151104,0.03466765137608922,0.0346749644637341,0.0346913084355902,0.03471560091870596,0.034722439406322164,0.03472561625276444,0.03474707708682181,0.034753259604543715,0.03475754987164428,0.03482965469851078,0.03483148234347744,0.03484289862158048,0.03484380883899791,0.03487005210184576,0.034883210488401685,0.03489058374485278,0.03491455011830089,0.034914891184679456,0.03494455596154179,0.03497425423796505,0.034988013320373525,0.035005977957449266,0.035008651566563846,0.035015857267527734,0.035035403839923096,0.035047307392582806,0.035089106675621005,0.03509054428285928,0.035096759981620144,0.03511088126647148,0.0351172976741465,0.03516644429800536,0.03518652342660082,0.03518931226461171,0.035216994865015054,0.03523332306340231,0.03530749899183089,0.035312081341195185,0.035326261386991034,0.035340016999025295,0.03534010151528306,0.03534114679013243,0.035365463989463976,0.03537620834540945,0.035384662708005915,0.035393152747679194,0.035408486425629854,0.035420970056787855,0.03543030795035266,0.0354323985675178,0.03545119406530945,0.03548099933058977,0.03548387703222363,0.03548838667146685,0.03551360950809206,0.03553088980964163,0.035542883244562035,0.035568501357800306,0.03556971735210328,0.035575426636997275,0.03559028288581516,0.035597545552800904,0.03561576525846317,0.03562145902494437,0.035659079285150455,0.03566847990394467,0.03567419440875011,0.03568602175416514,0.0357428264342586,0.03580020322093857,0.03583830660230577,0.035851817461940716,0.03585476984850876,0.03585648536238598,0.035922404157230955,0.035938731489809264,0.03594903122678452,0.03595412566049228,0.035995229936471994,0.03602535109553753,0.036071452877378375,0.03608061691053436,0.03614112586838594,0.03623396796169426,0.03624047183097979,0.03628377776789847,0.036303746017440526,0.03632709083303522,0.03634884781902261,0.036354336845208696,0.03636978029910713,0.036376703894946315,0.0363793842245599,0.03643519403113521,0.03648941608225461,0.03649397830989522,0.03649405497847929,0.03653129284719599,0.036543321469088036,0.03655272922226208,0.03655464966486453,0.03655892119070172,0.03656970282747462,0.03660524831040884,0.03662565145227658,0.03667457039085912,0.03667593645099438,0.03670137777005847,0.03675702758855925,0.036762118232447155,0.03677172758730968,0.0367833390051661,0.03683534537139381,0.03683587069636213,0.03687235721849203,0.03690467197548576,0.036907775335244045,0.036911470432513735,0.03691398193297939,0.03691607989674356,0.03693224058141674,0.03693946461582358,0.036949988746653145,0.036983887251896636,0.036990454843897194,0.03699718386085101,0.03705952295736505,0.03710037116483965,0.037100615868274345,0.037121714296150285,0.037126760684282625,0.037132506472246404,0.03713463986700986,0.03720693487438624,0.03722656798413535,0.0372658371597662,0.03728486248877319,0.03729057721127878,0.03730315945560604,0.037304907738626616,0.03737631388964597,0.03744637710913938,0.037455372722571795,0.037472842770252875,0.037482653600955165,0.037510947424256987,0.037524422859737894,0.037545862940488964,0.03763942965806891,0.03764784820895588,0.037679751690238725,0.037683311994923516,0.03769102871197165,0.037722225091977826,0.03776178754273754,0.037775375932342745,0.03778127428248902,0.03778590756604795,0.03784269005296987,0.03785713570506411,0.037885016760716386,0.03789706766791079,0.03796351661454261,0.03796577845153933,0.03796996533141563,0.03800535090797899,0.03801259948389816,0.03803518927673054,0.03803965294152902,0.0380643490254476,0.03808647354242194,0.03811454223743499,0.038121565244970186,0.03812660399164778,0.038146952347224754,0.03817990259815459,0.03824309759268067,0.03830545785893778,0.038325550137049226,0.03842011522902846,0.03844039266830438,0.03845793723739608,0.03847362277113362,0.03847715120298327,0.038580229129986814,0.03861117691268983,0.0386120019493799,0.03863401494955067,0.038640236483251426,0.03866177625251244,0.03868161326580578,0.03868369603896143,0.03870471179414558,0.038724848300217454,0.03874930968379449,0.038772616883852126,0.03880496733879134,0.03881689153737763,0.038928756070763154,0.038949648206291175,0.03898527734066321,0.039011485215942936,0.039033056901115415,0.039037980381903134,0.03905257819820636,0.039107266666499316,0.03912345379635921,0.03915098312328463,0.039173740625587414,0.03921262193795821,0.03926928402667608,0.03931924914436709,0.03933474265087113,0.03939880514870472,0.039400880366066746,0.039426386166145015,0.03942941266672952,0.039495932858858294,0.03954456356722664,0.03960454608403005,0.0396502473610046,0.03965244975212128,0.03965547826501966,0.03965584790134029,0.03965711175011496,0.039658654705934875,0.039693797172178656,0.03970077651204975,0.0397461947331718,0.039750254619768624,0.03983484692337853,0.03989687490483202,0.039902867257567184,0.039916596530241606,0.03992820936860843,0.03992982327439242,0.03999118690413854,0.040000172945738356,0.040155000565956014,0.04016154346983651,0.040178412895373126,0.04018779802692807,0.04025140273583205,0.04027882480060557,0.04028545410169087,0.04029761716386782,0.04030478294598466,0.040315436465088325,0.04036739545151353,0.04036791057522184,0.04043976129923169,0.04048310799784231,0.04050515612297266,0.04062582760921564,0.04067437961139307,0.04075141526848864,0.04078584583222451,0.040855181346728595,0.04086570975827022,0.040897333695838436,0.04090375951932068,0.04095844117328918,0.04100394767550335,0.04103568583987202,0.04106897352440283,0.04111978905167559,0.04114435869804421,0.04115892925202467,0.04116502214099768,0.041174761943002715,0.041187840834604014,0.041273754756924186,0.04129876523039998,0.0413328046042978,0.0413379568009757,0.04134336899444759,0.041370056564851494,0.041435818235197534,0.041443568910985405,0.041580005626852476,0.04160602937940364,0.04161412011858491,0.04164240251661066,0.04171590147921952,0.041814468720046245,0.04182452994993158,0.04185682367981698,0.04189886335177927,0.04191731137003261,0.04194059172720464,0.04202171410813165,0.04213549227662237,0.04228449514443118,0.042297572661333833,0.04231455210423488,0.04232101164698582,0.04233657148882324,0.04234411858473492,0.04241104088404031,0.04249052708896016,0.0425926060899609,0.04261287968262977,0.042691071193249205,0.042708650465750495,0.04285047931060743,0.04285156804253533,0.04288133324592253,0.042890567836110546,0.0430495980988417,0.043086812688879164,0.04315523378992269,0.04325096320854302,0.04336045358332809,0.04337541861201718,0.04346101960923101,0.043489985632300755,0.043752818404119764,0.04377333610661279,0.043853426909826465,0.043864314473290617,0.043869964414710125,0.04387519243258407,0.043916705485865556,0.04392126651933788,0.04405663356172723,0.04427589625937908,0.04435292206220448,0.04446576073602723,0.04476081742245169,0.04515862929883042,0.04516919535460233,0.04533741932942355,0.04534914680903153,0.045594128659548476,0.04572687588646408,0.045760532773377154,0.0459325264800743,0.0459619015700367,0.046013377147947294,0.04607280421163787,0.04631794472849062,0.046478735740081094,0.046581260566774654,0.046605594205971414,0.04665045143994119,0.04699879110995506,0.04705617083829479,0.047213235846926896,0.047345622258800436,0.047449608122309375,0.0482379813994263,0.04855154927001555,0.048833286934835324,0.04893833342325614,0.049091430852274374,0.04966523817420144,0.04985650387497623,0.05373237981704654,0.05452626167091366,0.05717500685494461,0.05777046548442601],"xaxis":"x2","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y2","type":"scattergl"},{"hovertemplate":"algo=mp\u003cbr\u003ek=20\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"mp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#636efa","symbol":"circle"},"mode":"lines","name":"mp","showlegend":false,"x":[0.0047123712031906725,0.004722788055853075,0.005565174618381584,0.005651131316238206,0.006249547792632406,0.006337795568216,0.006439967963787496,0.006873917699212723,0.006929760012224671,0.007075286294958326,0.007099027187338329,0.0071494586569915905,0.007200361788588276,0.0075809814114289695,0.007609094534468648,0.0076213150837514745,0.007749272123786162,0.007801773856818514,0.007888195778265774,0.007995423498205227,0.008001655877050543,0.008005588276408758,0.008046999919490335,0.008050017120321974,0.008068301342777076,0.008117630127024388,0.008211251244634218,0.008293859333673236,0.008333606748083129,0.008356267626352698,0.008386141615168267,0.008402266081026165,0.008423081595829938,0.008435533607282265,0.00846510204555434,0.00851804616780259,0.008525162366021395,0.008555370699934204,0.008568874892777672,0.008608037196724847,0.008617416058418996,0.008623240776431087,0.008667485188352057,0.008681350525355005,0.008735531847512667,0.008772697854648393,0.008828097344954778,0.008878658229732394,0.008892279341365893,0.008899748312456107,0.00891269464541594,0.008989352867071358,0.008992418434443546,0.00903365317484771,0.009068161559315831,0.009081538202641916,0.009097338955209311,0.009129620170957654,0.009158551744347406,0.00919209584745257,0.009199880524636255,0.009294906721056165,0.00931492219936771,0.009324219627154011,0.009352532729700321,0.009391925252762143,0.009441914327123558,0.00948767397017622,0.009506154684493475,0.009512541508378066,0.009531382247901536,0.009535040340921477,0.009593413842656166,0.009594622697368731,0.009602729231019862,0.00961180290635203,0.009619953758698674,0.00965434507354849,0.009654975978259026,0.009661282789893295,0.00966267299349818,0.009667935542665339,0.009683336051496249,0.009740702403970476,0.009741502225193389,0.009769757143595206,0.009778380175803791,0.009799973725820522,0.009821625978623787,0.009835290137856207,0.009858883559992902,0.009875482062723774,0.009880336632964792,0.00988766344050739,0.009887939806683448,0.009889300573591704,0.009904298280361096,0.009915480672313095,0.010006631404394457,0.010024586365362312,0.010095708072911444,0.010135751023468233,0.01015935310789928,0.010182862753565284,0.010196040106464811,0.010202451324104031,0.010216213991251027,0.010371093810893878,0.010376399347078584,0.010389807744857989,0.01039404890159765,0.010405578758996538,0.010408432222996252,0.010413746815557621,0.010419459347635177,0.010431577051716214,0.010432331396228326,0.01044977297321119,0.010518539265558794,0.01052143475998411,0.01052320037026483,0.010538195556229537,0.010562797906014224,0.010586701513790984,0.010600399109111121,0.010631710609053078,0.010639676606834915,0.010643413620719097,0.01065466042916537,0.01071205944056215,0.010718377693148252,0.01074706828789197,0.0107640135745649,0.010784488206806737,0.010802148853840784,0.010848907261709685,0.010870081784187299,0.01087723092507618,0.010891362340234627,0.010910677813902276,0.010913535480930655,0.010915243875187632,0.010938558419772008,0.010944714484874312,0.010961509341480137,0.01096709156844211,0.010988101995650567,0.010994972847039087,0.01100094776120678,0.011036217365478506,0.01103786996691294,0.011075098150576492,0.011103134782408715,0.011121123449702371,0.011122004173621657,0.011123611279495352,0.011129508721909757,0.011142003012805464,0.011165043778716462,0.011210730476779895,0.011216104949050218,0.011223943070336506,0.011236925411090611,0.011242574027698191,0.011249362930748987,0.011256093843772425,0.011264405420179658,0.011288553385443269,0.011290171879994043,0.011295884971737405,0.011310896204989849,0.011324557426693953,0.011340333313374952,0.011343426086468713,0.011365607820808431,0.011368676265396336,0.011385091101419012,0.011389122430097359,0.01139280469984925,0.011402755550212118,0.01141142378764808,0.011439301660888983,0.0114428211182294,0.01144913794305739,0.011453433983158392,0.011461942748341735,0.01148945155717082,0.011498634300316451,0.011512478429482772,0.011517755264318261,0.01152442796418025,0.011531560035620591,0.011534476398614433,0.011538327214637467,0.011543554182428938,0.011543565899886388,0.0115505856320333,0.0115790030462302,0.011601837141895378,0.01162170255885173,0.011624496698112953,0.011634392937852006,0.011642498035786932,0.011649798683820009,0.011652348859396531,0.011667165307279587,0.011686787114177342,0.011698941717226161,0.011739199325101642,0.01175159806985511,0.011757578436237497,0.011770168409880322,0.01177241996948658,0.01177406047523164,0.011776831995773798,0.011780520654108066,0.011791695380127693,0.011803234706876272,0.011803484960757943,0.011807798095815877,0.011844882571609866,0.011851309728940223,0.011852649482716674,0.011856277255780739,0.01186599004276315,0.011874553692002984,0.01188229327478517,0.011890186379187791,0.011890204434641768,0.011890284749640555,0.01190457966910197,0.011920892917535883,0.01192374082189942,0.011937965743268046,0.011939027247211156,0.011940860817158505,0.011962164693099164,0.011966665559124432,0.011970167612955213,0.011990695262316933,0.011996327437238505,0.012005319186319176,0.012015544347956408,0.012030521383463775,0.012038586578523696,0.012070514167170946,0.01207245856162848,0.01208504600836645,0.012091624891902663,0.012113475533436457,0.01211935918235776,0.012129273828861932,0.012132757564390709,0.012138029873922037,0.01214221375146603,0.012142672270467048,0.012143617833944251,0.012152728476399014,0.012163105378004576,0.012163446091572914,0.012186767471555848,0.012198796836478166,0.012222521709118172,0.012230724696685744,0.012232259698861488,0.01226886462499786,0.012276581190878046,0.012279849688298197,0.012282531142904523,0.012289008941003658,0.012300232217740284,0.012301660203755336,0.01232661978325082,0.01233435891136932,0.012371575743662872,0.012377269161235323,0.012380592417354598,0.012411606682394431,0.012415074813211472,0.012416079227976185,0.012429893873119071,0.012454193061870409,0.012455537386798454,0.012465234744502918,0.01247000980674812,0.012474278354435004,0.012474349539587704,0.012502677448877716,0.012517136704785704,0.012544134239578139,0.012545608312835197,0.012547217329731528,0.012553838688654114,0.012559786768944342,0.01257765372149253,0.012581539806688352,0.012593501139513342,0.012594869685998794,0.012602712708030876,0.012605062616597162,0.012606351125598334,0.012606572769183977,0.012615245455767105,0.012625315702861972,0.012636685049728466,0.012642234321013895,0.012648674287503398,0.012648849209722729,0.01264978134992924,0.012651990533276572,0.01265377654324507,0.012658581817639538,0.012662962542912926,0.01266877452575162,0.012684431238106674,0.012684581055431712,0.01269355615711252,0.012694552016728616,0.01269721567450412,0.01270959787692488,0.01271201263186412,0.012753048263507585,0.012774844790299259,0.012779866814447946,0.012783771472891845,0.01280017529941222,0.012815809391407192,0.012816173159743732,0.01281930260926395,0.012836928723936377,0.012843845108016907,0.012862447541060713,0.012862565503631188,0.012869092006926161,0.01290499807767545,0.0129054435566572,0.012916589690101222,0.012926446202635283,0.01292840972960164,0.01293067406846403,0.012944168919371685,0.012953945781454734,0.0129556944130891,0.01295842910577022,0.012972099619470146,0.012979720334701964,0.012999804856286861,0.013028754362778316,0.013031777107052953,0.013040022468047956,0.013042077472453738,0.013045323319491047,0.013048394976371175,0.01307135493210177,0.01308250715714436,0.013085194905904212,0.013093017371578142,0.01313344129708951,0.013137958360053461,0.013142060944486103,0.01317308868794664,0.013182225437902446,0.013187080718045308,0.013189990930095338,0.013197562661449697,0.013199039213155442,0.013221845767417284,0.013224527731208987,0.013236326134607188,0.013241261928108611,0.013253081813122037,0.013263340401758045,0.013267883104730932,0.01327334801295455,0.013287019586653556,0.01328998853036752,0.013291484046481441,0.013292145081820606,0.013297605671793809,0.013305017804755764,0.013316288537604467,0.013316297327205646,0.013345130563245416,0.013346562215334316,0.013360900575815241,0.013380922739235978,0.013381717465858622,0.013392654565207676,0.013413718237527562,0.013423590495102519,0.013439258520830666,0.013445345714743437,0.013447491214353138,0.013454783150473822,0.013460026915700488,0.01346897041108212,0.013481188594139404,0.013501581473757665,0.01350396187338147,0.013531529496665867,0.013548430111352613,0.013548612964606337,0.013549850537668509,0.013550334940315267,0.013561924263269362,0.013565497243001212,0.013583863684828246,0.013596243846510505,0.013598549409738179,0.013599920244848043,0.013613471085425535,0.013652329608129685,0.013665130619966475,0.01367982977816386,0.01369913111414108,0.01371480067908147,0.01375319709612818,0.0137551721905008,0.01375767238892033,0.013799853414326366,0.013818416642405684,0.013843788085591457,0.013851276791428552,0.013860727149157617,0.013871885009254395,0.0138751456329681,0.01387986435245254,0.013889681412164895,0.013890125880495674,0.01389753305947335,0.01390974851824717,0.013910893627157717,0.013913328688879848,0.013918000627878405,0.013921254925117591,0.013944919091202287,0.01395022439041697,0.013962507518430572,0.01396399367440899,0.013965516833771585,0.013978732403861835,0.013991140935215205,0.013993838755343093,0.013994841803521132,0.013999088532537642,0.014014964665002358,0.014026925064787315,0.014032842339406201,0.014032890127978573,0.01403926105012196,0.014039574946255054,0.014062310991340305,0.014065752990498942,0.014077644308746468,0.014097549449414621,0.01410262147317389,0.014106323825667422,0.01411904588554902,0.014119514991840974,0.01412527107356533,0.014126886793594542,0.014152112950491336,0.014159917156699595,0.014160126014573452,0.014189216899103827,0.014191173944637554,0.014192643339067884,0.014193590702966788,0.014198265202115617,0.01419909255539186,0.014209065881668,0.014211289991231247,0.014211576676127376,0.014214973053770433,0.014219730512868251,0.014233424699726521,0.01424003999601326,0.014244418062386608,0.01425399129853289,0.014254360177117667,0.014272916647342987,0.014285089380667564,0.014290666079465364,0.014296445769210315,0.014298927381926568,0.014312599288803499,0.014330846269894892,0.014345432513194303,0.014352185762528844,0.014353701642748556,0.014368204776678346,0.014390643189918628,0.014390709317710342,0.01442393497889235,0.014426194311013643,0.014432870975239365,0.014433233640114498,0.014439607996147288,0.014466981168357897,0.014473838325981689,0.01448024879294929,0.014485169551692056,0.014499623844571735,0.014523724664431081,0.014533660822044556,0.01454282280624509,0.014549117558921371,0.014549549908731266,0.014549943676449306,0.014559361803877199,0.014568017212469734,0.014602670538781906,0.014604933715676824,0.01462172492754096,0.014629916288461022,0.01463796334478643,0.014641335711152456,0.0146423949722304,0.014644508420247901,0.014647916004058922,0.01466157384642513,0.014672173825851176,0.014675201412811303,0.01468575179832397,0.014686174818142448,0.014690065627499676,0.014695234273504327,0.01469562755832333,0.014708716053465812,0.01472342266949038,0.014724773990268194,0.014736256423516784,0.014744087740831921,0.014746391271129724,0.014759188830619791,0.014759242714010804,0.014763710653255126,0.014769065794784163,0.01478687571118446,0.014788571425727906,0.014800505172990867,0.014808548759552528,0.014819285231157785,0.014844507133634822,0.014851649582532444,0.014856243517137718,0.014857043656294875,0.01485776223622968,0.014874508314829666,0.014879597039431567,0.014880106652171729,0.014886700210012909,0.014889453330541499,0.01489168247429376,0.014892474104192265,0.014900384393842375,0.014912501598086925,0.014920116954474061,0.014920815109732723,0.014935131460690306,0.01495848743595474,0.014958909448511387,0.014960809200740301,0.014970615509541822,0.0149773104428638,0.015055972103563697,0.015064002952608808,0.015066523142662958,0.015068856468848485,0.015069217560923553,0.015072104989607024,0.015074570051482767,0.015079058509470247,0.015113963321868035,0.015120523090393573,0.015124705386911635,0.015133617342417518,0.015134582569011424,0.01513604035769089,0.015168602697187377,0.015191292230185765,0.015198886909702844,0.015227041827259069,0.015228488272094701,0.01523595166462563,0.015242071329841103,0.015263492954334693,0.015270475117348503,0.015305106689924228,0.015325270560987888,0.015337763597331574,0.015342574320137487,0.015349058450526708,0.01535436769618714,0.015354880315997086,0.015360737062978752,0.015387890756869619,0.015393664741403338,0.015393875436672986,0.015394878126377923,0.015395641564769923,0.015419162464594386,0.015421951474800227,0.015430484176203657,0.015437853491428049,0.01544847728414445,0.015453967012442648,0.015470224540601924,0.015517339953261756,0.015518750226131476,0.015518814363014336,0.015526306921992163,0.015546610763874768,0.015565365392853265,0.015599288955809138,0.015616444422958477,0.015619002765631971,0.015625402875476778,0.015634108266693,0.015652052271990645,0.015654968882943654,0.015656413960933445,0.0156633194172384,0.01567551630635997,0.015680227031439575,0.01568949837230459,0.01569019625995039,0.015695002635457974,0.0157083714496437,0.01572535328570713,0.015735191702210244,0.015751363278604692,0.015767864643404333,0.015791448166139206,0.01580437470534346,0.015809483936516464,0.01582995742197269,0.015834200565807456,0.01583515219655916,0.01583886107812616,0.01584038226911806,0.015843316324091354,0.01585571128138303,0.015855721950631403,0.015857826950824352,0.015861731499555925,0.015869585553894127,0.01588283167645935,0.01589433970456538,0.015905359342621432,0.01592293710474574,0.015934837935384534,0.015941270953767066,0.01594323242472003,0.015957756389120885,0.015965686249472085,0.01596581783884589,0.015974363151836937,0.016047710925546438,0.0160478702365491,0.01605175248875145,0.01605296526365681,0.016055356902460443,0.016079430101541968,0.016088371033808287,0.01614958874576517,0.016149848245848208,0.016188935556976612,0.016189094961996955,0.01619673326559742,0.016218411864781512,0.016224431716530964,0.016231545366506613,0.016251292731917354,0.016266102252897645,0.016272787153440264,0.016284730760924723,0.016307914868213376,0.01635202282360167,0.01635279922447862,0.01636055610107663,0.01639516547188981,0.016405983293313456,0.016423039492924056,0.01644357657977511,0.01645582098506391,0.016457707419133438,0.016472183593136038,0.016483506707066803,0.016486654045777142,0.016514798891504152,0.016523693670281222,0.016532000867606055,0.01653572170366819,0.01653662457725981,0.016549545951011258,0.01658155527801302,0.016583703483691144,0.01661019870595778,0.01661835154707048,0.016621894609273692,0.016643143650425407,0.016654932141398328,0.016671658195523447,0.016685655559899128,0.01671460881062197,0.016718764258295438,0.016724447057723333,0.016732447763014538,0.016738632093071977,0.01675586032203486,0.016777257886347527,0.01678082081537435,0.016798965908218534,0.016799438174775198,0.016818415343260555,0.01681878933669767,0.01683372876510043,0.01684468497244845,0.016848401923608595,0.016848796918715117,0.016858781457631578,0.016906216116395492,0.01691915293733548,0.01693327908462428,0.01694101973678138,0.016982751243132872,0.017000861212683353,0.01700274919469307,0.017016657747098256,0.017016815740072033,0.017078928051793927,0.017099036966802657,0.01713137906114619,0.017135896292485537,0.01715440530621484,0.017204591640710053,0.017219338093570496,0.017225016409179627,0.017237948346526408,0.017239254620882535,0.01724663241158137,0.01726260867554463,0.017266363024774946,0.017283089181596344,0.01728700548059725,0.01729534968935939,0.01730293359940911,0.017335581521407694,0.017338231646420805,0.017345945564689803,0.01738221138022004,0.01738347700098587,0.017408536922379435,0.017424742259962104,0.017431896242431442,0.017447621551239433,0.017448876269736093,0.017479429066815617,0.017502353924225875,0.017509191346342774,0.017512838307790612,0.01752240670215761,0.01752456435482061,0.017546375522748306,0.017547976320999706,0.01757649708384372,0.017577425841150768,0.017610253688662165,0.01763212150173601,0.017649129746851285,0.01765087136910835,0.017651007011738926,0.017659154869031014,0.017675665458947,0.017713403748284135,0.017735007511564427,0.01773513684982633,0.01777122218229537,0.017800333526149757,0.01781625387531307,0.017818691513390886,0.01782848588662861,0.01783249819718606,0.017840698077081947,0.01785165890231569,0.017874539098087346,0.01787895666798902,0.01791069010769528,0.01793677588271112,0.017943329189495396,0.017946795479966204,0.017955777282897094,0.017982525870488507,0.01802979772414722,0.018046384955672352,0.01804697746173869,0.018056158212160216,0.018087232685768832,0.018098806673968355,0.01810816027047347,0.018114789726846395,0.01813899115375648,0.018146341538031442,0.018151154564556536,0.018165860986787703,0.018171273830559585,0.01817929027841648,0.018195240215394117,0.018223819322627594,0.018228068434252887,0.018237469106676004,0.01825190249257693,0.018252229438672562,0.018264664177326383,0.018279223288820375,0.01829160732301837,0.018312258310677166,0.01833133267904053,0.018342382775910653,0.018351595352641633,0.018362392130474605,0.01837743844196047,0.018380221773938388,0.018388510217404213,0.018389345740601075,0.018403771172437227,0.018411560145999303,0.01843158507927923,0.0184379765921487,0.018470531003036195,0.018476361507506735,0.018481970360148946,0.018487031137642752,0.018492712093914943,0.018495014381651888,0.01851021596610496,0.01851606400191983,0.018516866555559085,0.018546873636426462,0.018565276817545308,0.018572420559562063,0.018589049151182074,0.018603817392096995,0.018618953281055894,0.018631222165604623,0.018638853865500654,0.018642090385767703,0.018644779014145924,0.01865577950217432,0.018666535471058364,0.018697733656020878,0.01869786564671307,0.018701401747033917,0.018702383257897758,0.0187300244050028,0.018736601690306127,0.018745411504676866,0.018813936290461943,0.01884005465956802,0.01885691700823641,0.018861785444092262,0.018869785655694693,0.0188932234006158,0.018902607303892472,0.01892491689671445,0.018928736507997892,0.01893691156989217,0.018944465112994364,0.018996926604280078,0.018997893241672267,0.019013935370594474,0.01906238776911855,0.01911153389187483,0.01913598412296893,0.019138647683980856,0.019142493314174207,0.01922195112792108,0.019224772536023813,0.019263331834996872,0.019270978144697793,0.01930744718255986,0.01932065652449779,0.019381421192106146,0.01938253501291537,0.019389464825119627,0.019445560221900103,0.01947440947501868,0.019542314488166145,0.01960254119653332,0.019610747556314687,0.019625430234894915,0.01964064556880922,0.019652970391219285,0.0196733580437669,0.019681198106614468,0.01968395624162495,0.019720513319495023,0.019727398573913858,0.019738833850497788,0.019755377206294417,0.019764763009411157,0.01977443809980505,0.01980083697805875,0.019802971215174762,0.01982531817092365,0.019827867409682905,0.019837806104610563,0.01984984275546326,0.020031375163891925,0.02005072072943221,0.02005522954495416,0.02016125451431061,0.02017775227921217,0.020213152287576367,0.020263737735245545,0.020334352623493555,0.02034647835181711,0.020352988111033487,0.02038851025029627,0.020417901007195254,0.020424140712497978,0.02048358907783703,0.020550831545103405,0.02056787521538435,0.020629942933121773,0.02065429487649049,0.02070669015137213,0.020756539089130777,0.02091450558480667,0.020922975803522852,0.020944598009310703,0.020951924826713247,0.020954675039405887,0.020998697999002086,0.021098527270276526,0.021212418821348476,0.02125999039233981,0.02129123902156271,0.021295294134655897,0.021339650176764043,0.02137042315977919,0.021376727894204008,0.0215553185443167,0.02161819181425543,0.021628624967678953,0.021717009243166656,0.021751737546947404,0.021802544917424723,0.021809838153296505,0.021819571873381616,0.021855783149687762,0.021858103890164248,0.021894727223938863,0.021901954521452837,0.02195893005330652,0.021988060705757862,0.022003901710121218,0.022187666728852043,0.022305047742132725,0.02231802304182627,0.022344857042398542,0.02239601462351823,0.022415599326809935,0.022431723350640944,0.02248889073095179,0.022571644593667117,0.02263791001601433,0.02265082969091466,0.022660716618875403,0.022889277643560788,0.022988799084171344,0.023308191339068994,0.023443229814236664,0.02347357019600057,0.023682984180684215,0.023790504978141537,0.023849697930499313,0.023879927378994213,0.02394839885027757,0.024059668937506527,0.02424079837468965,0.024316661546312783,0.024518243571602177,0.024570472780956937,0.024606925981724044,0.024896455969326953,0.024908401281613014,0.025671772601607475,0.026038012314734144,0.026069787387512366,0.026121670544256098,0.02625516002891103,0.026324336080286982,0.026397820502917297,0.026518771661697277,0.026876039653898138,0.027518261600472625,0.029903976245416485,0.030947550857087884],"xaxis":"x","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y","type":"scattergl"},{"hovertemplate":"algo=omp\u003cbr\u003ek=10\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"omp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#EF553B","symbol":"circle"},"mode":"lines","name":"omp","showlegend":true,"x":[0.029471280407691557,0.03244535433361009,0.03420345568793854,0.03518553881514206,0.03541190217985453,0.03549486049098907,0.03562316338400896,0.03581684001751216,0.03683604438834216,0.03684885553516027,0.03686490336239541,0.0371494281997881,0.03746339895466654,0.03764988896925279,0.03774773812668084,0.03840663406258753,0.03897715848305851,0.03960718189964323,0.03970251549797479,0.03997206405576778,0.040016290399215906,0.04008592450388148,0.0401150474882143,0.04028325591087314,0.040318394214934036,0.040347421360377785,0.040386859045748236,0.04047992054976769,0.04055776451509935,0.040768277711588544,0.040902007431816226,0.04100867381469671,0.04104632950925031,0.04104666627618487,0.041162442772673065,0.04117116355514005,0.04122790618089073,0.0412717923498245,0.04128598945285221,0.041449753672189574,0.04159113054892156,0.04171362354124049,0.0417280730463041,0.04179132374964317,0.04192047018766525,0.04193800038197483,0.04226837405840036,0.04233472858568994,0.04236599364073324,0.042375989785728256,0.04238874890743009,0.04245193885918037,0.04251469591613777,0.04269746555286665,0.04270708519839797,0.04275630133910357,0.04277255321872424,0.042774019651083635,0.04283458318546028,0.0429500101048511,0.042971254954980956,0.043018081108067006,0.043046204859107325,0.04310649820015876,0.043127970191016954,0.043208969852611176,0.04325861534498616,0.04329289437954801,0.04342390547097276,0.04348818085167029,0.04349892715588914,0.04350399061488768,0.04350496512473112,0.04353667698047918,0.04360170643277277,0.04370556528398551,0.04371082652663993,0.04374935711051865,0.043750447872117654,0.0437628995842582,0.043763283962445707,0.043766838275480294,0.043804432346965155,0.04391540683064029,0.0439269105191024,0.0439636611192599,0.044061746394238696,0.044068638156177016,0.04407469381464905,0.04413097044028261,0.04415523602794333,0.044194597109737624,0.0441947070748109,0.044364848126962196,0.044452243234236924,0.04446242988325419,0.04448817784512005,0.044557190599358906,0.044560636594289466,0.04460952185481099,0.04463084030338446,0.0446525780297639,0.04467367611478474,0.04481144092496803,0.04482337460726732,0.0448286192304787,0.044962915346035956,0.04496777044957398,0.04502399401000105,0.045028991011730954,0.04503594208124133,0.04506294544050407,0.0451374276907697,0.04526573336189542,0.04527700519125534,0.04535501627140644,0.04539719777049775,0.04541047623741861,0.045432803758887,0.04545133818551658,0.04549411905072375,0.045517525225872844,0.04555933964926538,0.045638409315519275,0.04566746070384501,0.04567842142011228,0.04571127718683293,0.04572533461259399,0.04579288224647337,0.045848577002998464,0.04585846899492691,0.0459201035266519,0.045935545317994805,0.04594918520790771,0.04610830572845171,0.04625089086685087,0.04626502132096365,0.046290648891768045,0.04631056410221278,0.046346424163105936,0.04645519606953471,0.04648366283194403,0.046514965150497564,0.04664019614512872,0.046720927736000506,0.04677462584613231,0.04677716594785616,0.046782696308401414,0.046809378194513,0.04682964949156712,0.04683060989334963,0.04683445285883178,0.04686015489360956,0.046892984103622935,0.046895201607979345,0.04693199762491385,0.047023104851629624,0.047036351117934756,0.04707466570069002,0.04707623420268865,0.04708408949783942,0.04710726700808885,0.04713752344387993,0.04720273618925471,0.04720748267973658,0.04721037153903153,0.04721444852427035,0.047217304485112174,0.04725947821988648,0.04728117267620992,0.047302368003104917,0.047340511692539264,0.047369576187284754,0.04739754792344485,0.0474095538713578,0.04744624841142875,0.04744989762650799,0.047460051100588735,0.04748503909564945,0.047485364302247944,0.047498306839639834,0.04750640744603782,0.04751822605237351,0.04752056954617126,0.04752694143286866,0.04755385205234076,0.04757287831099903,0.04764570856647517,0.04768202671758155,0.047728792907678855,0.04774358537389052,0.047749497437709704,0.04777132389744046,0.0477922677367675,0.047828461693649245,0.047874463878197736,0.04789237153969269,0.04791584961459276,0.047937307597861974,0.0479373216630672,0.047973310228039814,0.04802308575685834,0.048150636591647665,0.04817801850860268,0.04823808782093964,0.04825106410542275,0.048267112146438904,0.04829055334411327,0.048294924750148585,0.04829501109641152,0.04829896049321439,0.04830523141571943,0.04832050101653032,0.04834791611885801,0.0483615967649715,0.04836397755590445,0.04841122109998435,0.048504433571317764,0.048541806149017784,0.048542817940372955,0.04855927329822452,0.04857392179732945,0.04861609480344711,0.04865685689594405,0.0486817220856281,0.04868633991016785,0.04871441279502546,0.04873276207537588,0.048742652209876995,0.048745150254395006,0.04877280328586958,0.048772807772242965,0.048775010303996134,0.04882935834949516,0.04887005389903845,0.04889540745378429,0.048957148463434255,0.04897816551881709,0.048985885386277545,0.048999454300542554,0.049007525258204315,0.04902511184766768,0.049031176018735335,0.04905458695235927,0.04908737915364521,0.04910065098301749,0.04910187118855017,0.04911414795936175,0.049127819089764976,0.049188129013814516,0.049209792546444364,0.049229338233666604,0.049234073461812564,0.04925948797274646,0.04933422087200824,0.04933948855482371,0.04938468266067802,0.04941391981870933,0.049437405307484375,0.0494484505676799,0.04945241281797938,0.04945912521697031,0.04948262306570033,0.04949894031924056,0.04954552288158991,0.049602583276491585,0.04969697876451232,0.04971412464486423,0.04975591839691691,0.04979777381182578,0.049894418770121504,0.04996512190792769,0.04996544023105065,0.049976023387735484,0.04999893927168551,0.05000833213073912,0.050017053401003916,0.05006530810165697,0.050096721209896646,0.0500980758230147,0.05010225481624235,0.050108180313246806,0.050108220908868864,0.05014618186989327,0.05015613922982246,0.05017275284218299,0.050180784423978164,0.050237228647746934,0.05024495183640278,0.05030103436850895,0.050301577514074505,0.050322046718631705,0.05032253388291126,0.05038921089918173,0.050390523808106734,0.05039431165823177,0.050414737915062645,0.05042283785541448,0.050477301245261084,0.05048494503612074,0.05048637563446592,0.05050528961800427,0.05057953778320847,0.05058877917597525,0.05064200989393547,0.05066376353872709,0.05074680293054087,0.05081821080809995,0.050826494742473874,0.05086759107220159,0.05087860449983077,0.050908081671153196,0.0509432238503554,0.050949775477348705,0.05101818880800527,0.05102478089113297,0.051049998997625426,0.05105250622456804,0.0510548278928486,0.051057539726343304,0.05106256254535733,0.051105590389467916,0.05111295339825539,0.051127942956629646,0.05112929887626104,0.0511477710983145,0.05116813563545101,0.05118142493405231,0.051190518144862605,0.05119259755254313,0.05119631664891228,0.05119824382770184,0.0512441600859058,0.05125311734925838,0.051272988107742036,0.05129289059807142,0.05133851712677438,0.05139325763467234,0.05142921781315055,0.05144763470406436,0.05152087147935146,0.05154615537595569,0.05162775743107371,0.05166118877579586,0.0516729424771466,0.0516764405588816,0.05172260390181195,0.051772183329550844,0.05177287160077408,0.05184086986554309,0.05187352086770549,0.05189214545574468,0.051905394747125064,0.051936544696835694,0.05194103105667257,0.05194171816400385,0.05194823714147199,0.0519581743181504,0.05197857962248674,0.05198721729899329,0.05200821194368471,0.052036968297119235,0.05204054089235601,0.052044942923618887,0.052102632035426755,0.05212829004699594,0.05215565447682384,0.052160801628210164,0.05217660552429274,0.05221216112319947,0.05221844623295892,0.0522205104072833,0.05222291611295852,0.05223485362999157,0.05225344515842391,0.05225386648891028,0.05227792117074166,0.05229103794674208,0.05232532269004971,0.05235611471722327,0.052374677330700725,0.05246156072424217,0.05246457583062283,0.05247009903056995,0.052502191240304655,0.052525239119101744,0.05263022922765599,0.05263531489177912,0.05263716987600635,0.052650037075097335,0.05266348779796039,0.05269416293049465,0.05292356939213818,0.05294792865778318,0.05297786546142993,0.053044727249829796,0.053065529092786834,0.05308358341783294,0.0531173170101123,0.05318277874540909,0.053217576100928254,0.05328547530021747,0.053295925514842205,0.053334526575093986,0.053402302334291794,0.05348072616094803,0.05348983456688643,0.05350308611976121,0.05352520273670728,0.05356659015281136,0.0535752070552384,0.053599867753571506,0.053738075502926454,0.05373940731497852,0.05377799737645824,0.05378682936100043,0.05379253109344184,0.05386639046781103,0.053885294433081134,0.05389437685042668,0.05390368197257993,0.05390526335740886,0.05390987355969111,0.053927277894384895,0.05393302307078115,0.05393953183692427,0.053954327464965944,0.05396052570023376,0.0539725952095005,0.054012706760479555,0.05402466366805325,0.05402957407930201,0.05403490352942238,0.05404061464155474,0.05408725862411066,0.054104196600725886,0.05414101815776601,0.05416130732259035,0.054179340772726695,0.05418439826435648,0.0541916339850856,0.054201603938071906,0.0542145501390018,0.05423216111086071,0.05427187311891003,0.0542878351250346,0.0542883541538194,0.05428917797811511,0.05430556382045446,0.0543199580387866,0.05437164180381956,0.05442610534840523,0.05446801654721325,0.05451189931919644,0.0545292031813943,0.05453048169621457,0.05455300001820952,0.05456254304792983,0.05456636794554931,0.0545999834113013,0.05460741679519334,0.05463762073999734,0.054639283632006694,0.054646108553831695,0.054704880202079934,0.05479041836036349,0.05479125860806663,0.054900802780441224,0.05495630546093453,0.05496059617177236,0.055001306787699336,0.05500340316158624,0.05500711897921436,0.055039297525709206,0.0550424942270739,0.05504777398432422,0.05505522546244405,0.05515905654209556,0.05523996160237889,0.0552598261904349,0.05528540327601424,0.05528893887819394,0.055293645501345946,0.055333161294188894,0.05534466785091603,0.05535804679960018,0.05536912022989528,0.05537274864168738,0.05537928928174564,0.055409740160458666,0.05541280934609383,0.055431746404269525,0.05543230890487666,0.05545001405154558,0.05545097884439297,0.05546148390723643,0.055468700578142946,0.05546993863717503,0.055497570762196205,0.05551221169011642,0.05556755710575543,0.05557519768204172,0.05560218288229076,0.05561246788692229,0.05561738371674938,0.05563707111028574,0.05567885876961004,0.05572296254421109,0.05575025071204488,0.05575045106264924,0.05575314730926996,0.05581007772142771,0.055845918035852884,0.055872293446047924,0.055885353513089445,0.055895226831407796,0.055901194262111056,0.05590985755519153,0.0559171713167124,0.05597103491693663,0.05598447649128656,0.05599096041954219,0.05599279423290954,0.05599507732029978,0.05602317399482154,0.056025955921969774,0.05602659813577967,0.056054072092722416,0.05606002416961349,0.05608450249085355,0.05610368137650303,0.05612471101991874,0.05616977814378682,0.0561822533532582,0.05622703555844307,0.05625879272578984,0.05630118393685372,0.05632519612985465,0.05633696205845011,0.05635665751521527,0.05636492987761756,0.05641200038319762,0.05641958141792042,0.05642875457749556,0.05644340964557326,0.05652970970645045,0.05654714580653928,0.05655426468068401,0.05659008372811053,0.05659800505942444,0.05659826229160281,0.05662490825263463,0.05670577063739461,0.05671251930360714,0.056714292819666036,0.05675232305048692,0.05683898785749673,0.05685283977742874,0.056862004024244836,0.05690745914017135,0.056907525479168664,0.05690926012733414,0.05691117388349385,0.05695310872588229,0.0569591949366278,0.05696912728899045,0.056978692328325586,0.056984340427340144,0.05701250577328811,0.05705932239448219,0.05708107294212346,0.057138633105020806,0.057176091455050336,0.05718435077746681,0.057185151814770054,0.057201879063249986,0.057212905976467145,0.05725761700060065,0.057260312216036544,0.057264245488730145,0.05727663581993246,0.05728698595726075,0.05731120331839577,0.05732187720715435,0.05732362063838427,0.057326098699712155,0.05732907996777675,0.05736619177082193,0.057388825463447854,0.05740469853138743,0.05748595589769977,0.057496827380485206,0.05749751419608048,0.05749752547755638,0.057549197743923065,0.05755123208880616,0.057562155661607085,0.057580395858874034,0.05758830965646448,0.05759541995775208,0.05761703729344915,0.05762299183777107,0.05766332811449153,0.05766570319151628,0.057678153351770844,0.05768376907310585,0.05769420994710857,0.057726887769445547,0.05774349869677721,0.05775381081211126,0.05780837249024466,0.05787686184872972,0.05787694633229601,0.05789747068117057,0.05791773144839559,0.05794637779447499,0.05794781494038702,0.05797949508826673,0.05798009683032661,0.05798474970570903,0.057987864848586944,0.05800037784470015,0.05802275190632959,0.058108924263873006,0.05811022173244421,0.05811521675006759,0.05811846127310677,0.058125023531360176,0.058183083699202685,0.0581997840766589,0.05823254789314206,0.05828110816062092,0.058281480981184255,0.05829553877672015,0.05830136333080989,0.058312201264238625,0.058340246564776,0.058369402583344485,0.05837853142859709,0.05842920985527703,0.05845382407492664,0.058456243342333014,0.05850858886032014,0.058537641344283534,0.05853833317649983,0.05857180204638503,0.058578284374474945,0.0586278295081807,0.058685474744985346,0.058697916667423945,0.05872867131045206,0.05873028781087945,0.058797901358424734,0.05879801394282689,0.05879838710649593,0.05882520407829239,0.05884176357022147,0.05895455559996683,0.05897220136227991,0.05898005139873649,0.05899614168119174,0.059007204564782534,0.05901830262650115,0.0590475370132106,0.05904900676623803,0.05909450501299089,0.05913659987367896,0.05915469345455129,0.05918842208601931,0.059266053978245414,0.05926976264537394,0.05930710430885493,0.0593253989200407,0.05939373493755101,0.05939431498520735,0.059440571441847466,0.05944841205577669,0.05945030033188449,0.059470777706056924,0.05953182563641115,0.059537862555677966,0.059579107783440666,0.0595899924914474,0.059594658517973076,0.05970748203365199,0.059719625658339937,0.059770636432726745,0.05978792798234132,0.05981664168631,0.05986488244542839,0.05987238073528717,0.05988103519711239,0.059895464378265616,0.05992351467751188,0.05999743374855537,0.06003678404481618,0.06009972870857978,0.06009979684312724,0.0601066467214069,0.06012819557388612,0.060147156934190406,0.06017090813523518,0.06018671647421344,0.060280460834211125,0.06029418750425317,0.06030415174683916,0.06033631804619104,0.060364278928826176,0.06036532229203634,0.06042214421565682,0.060423143637778884,0.06042543609233557,0.0604381212739115,0.06046627802786144,0.06048265367716832,0.060500837466604264,0.0605393757336304,0.06054265543203656,0.0605528228810267,0.060641411942502076,0.060662583242638846,0.06066649827102044,0.060679582104469575,0.06074900730930118,0.060786958023065996,0.06080160152001353,0.06081318680709761,0.06081362716195658,0.060824174327462434,0.060834472622897894,0.06090087191871156,0.06090099869429375,0.060917918783871206,0.060923220099768916,0.061006965061994115,0.06101216016357963,0.06102010346251062,0.06109167089828206,0.06111555346862595,0.061119784603375894,0.06124408662481858,0.061261907633835544,0.061262474485020445,0.06127598526575122,0.06128689112808029,0.06128827741100682,0.0612997118146207,0.06136076931550816,0.06138080016957244,0.06139510817541173,0.06142784367530343,0.06144268324317834,0.061448663400692015,0.061449533394658716,0.06146640102032662,0.06146674362941068,0.06147635378200476,0.06148438099411777,0.06149243781209368,0.06153258313411515,0.06154237839612255,0.06158720314487163,0.06158888797332174,0.06160680947756829,0.0616185709462236,0.061622530538668376,0.061623162529716655,0.06166092106971879,0.06179066899457071,0.061794173874473265,0.06180925457022364,0.06181328362824542,0.0618407318430329,0.06185313649615369,0.061853502671179325,0.06186114981081868,0.061870556635836405,0.0619230004231466,0.06195700097726635,0.06199263851163504,0.062016927570605895,0.06210940979881062,0.06212698325240781,0.06214715588343404,0.062187023487990475,0.06220577882250909,0.062286477488098756,0.0623899059423124,0.06239438051590016,0.06240098340095377,0.06241554002017073,0.06250730046744975,0.06251559037089789,0.06252877450138003,0.06253975132826901,0.0625491793034573,0.06254932057840282,0.06261215481586394,0.06263624504750129,0.06269717357584156,0.06273399540322684,0.06276853211393811,0.06286313209416299,0.06291505037291474,0.06292225208407752,0.06292713656046062,0.06293822108440628,0.0629748279488542,0.06298388676813031,0.06302233337696947,0.06302538704904224,0.06306754604821756,0.06317965199068938,0.06320517587951321,0.06323892727197249,0.06325937348376919,0.06334036354248607,0.0633420108529136,0.06338534704551997,0.06342783550025027,0.06343217501092066,0.06345294455722762,0.06345892799343024,0.0635362259088416,0.0635458909686464,0.06355157616933169,0.06355389763142129,0.0636041782495078,0.06362219789963669,0.06365538158187836,0.06367738360974888,0.06368030232968568,0.06373076259413475,0.06378620453365746,0.06379007949303099,0.06381649560322597,0.06381924535026197,0.06387880031396097,0.06391609554588708,0.06392150807809464,0.06393587406573789,0.06393823926781037,0.06394068483180436,0.06395751640570249,0.06401107491742516,0.0640522149918255,0.06405963952949235,0.0640994939355771,0.06410978718491063,0.06413305688981977,0.06430675441204761,0.06433449443025344,0.06438106913819697,0.0644070172067011,0.06445647683290609,0.0644670576654385,0.06451304692117873,0.06451475500784673,0.06455592535290294,0.06457066957333654,0.06462035168123748,0.0646383946614185,0.06465147311845514,0.06470892123180741,0.06471645589020412,0.06472705199940182,0.06475558688503505,0.06478428887251933,0.06478812640601767,0.06487639307827944,0.06487703162287389,0.06492912252276027,0.06496552152615101,0.06496818044821857,0.06497298405244976,0.06507723617117925,0.06523321125886597,0.06525681955021467,0.06528461454907732,0.06534039981451552,0.06535471689251413,0.06542041888136643,0.06550488710243513,0.06557064613584215,0.06567994243665605,0.06573317523284111,0.06574113689705005,0.06579404205035273,0.06584378740891136,0.06608356711771642,0.06609538504859705,0.06609998402546688,0.0661109842488118,0.06611330202016992,0.06625056935563636,0.0662558553203803,0.06626722940505396,0.06628213855099704,0.06633148385097316,0.06637386708462527,0.06642085201706255,0.06649300655800992,0.06650018728178,0.06669312152213532,0.06674037347659893,0.06678536908130474,0.06692799307585602,0.06698825436489643,0.06698962075192814,0.06701311657814543,0.06706264019775288,0.06712626475194748,0.0671605207645562,0.06717760320362577,0.0671890356061545,0.06723074523130725,0.06723970680666631,0.06731058231914244,0.06731459999142367,0.06743951521430473,0.06757507880646803,0.06758226816579828,0.06767922994260285,0.06772515812535086,0.06774009855941004,0.06783914032837782,0.06793051480358592,0.06838800622084378,0.06844281404301389,0.06855711080557593,0.06867142139864515,0.06885265720766978,0.06890922024172799,0.06891006772196966,0.06911842227217277,0.06920757177289652,0.06922033160537036,0.06928695315822841,0.06934597332063351,0.06941553606756462,0.06965469731661843,0.06973197235948382,0.06977499502926857,0.0698397778623828,0.06992462734282437,0.07010533667163169,0.07018492042483707,0.07018659784225614,0.07018868324455628,0.0703284568491352,0.07033592080512067,0.07038539047212619,0.07040824718888555,0.07044060995364784,0.07048020357409504,0.07083772680471259,0.07132374979983334,0.0714847257798216,0.0716140219655431,0.07167603267531727,0.07182611565772505,0.07183880678896071,0.07231058007084584,0.07234527148537516,0.07258220007573543,0.07281131639231833,0.07290172825154757,0.07291177685162085,0.07292465720921107,0.07293165966511436,0.07311837289368742,0.07331402750043119,0.07339449424557583,0.07343594302306711,0.0736136558471054,0.07392043267530991,0.07402259292830711,0.07463301854242757,0.07463932406976839,0.07488380520087548,0.07545614561853801,0.07549490823314858,0.07554997413190365,0.07558971504357605,0.07568454635905067,0.07584059828508471,0.07598219032748668,0.07711340664694201,0.07746355471585874,0.07832223888536002,0.07841662100842896,0.07979382994414938,0.08139001933719212,0.08382406414357714,0.08494519137736971,0.08514001765077486,0.08570615886667188,0.08699072836686549,0.08711148125298865,0.09255889053972549],"xaxis":"x3","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y3","type":"scattergl"},{"hovertemplate":"algo=omp\u003cbr\u003ek=15\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"omp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#EF553B","symbol":"circle"},"mode":"lines","name":"omp","showlegend":false,"x":[0.01520424841420633,0.01582867046805616,0.015905372834889884,0.015939138761063433,0.016696704562205907,0.016705064038783655,0.016726791716120647,0.01693523310356788,0.016990172297813688,0.01727056338950178,0.017472944972425384,0.017678330044325223,0.017888684989493725,0.018076138549557884,0.01821630519036854,0.01829438738460819,0.018312593953564195,0.018349128398754896,0.018388348653335312,0.018497339425635723,0.018595680794421355,0.01877919469663398,0.01898970139787006,0.01902236779832856,0.01907622665202358,0.019076766153963618,0.019150109891483674,0.01923527586639916,0.019288284163378546,0.019308098857575624,0.01940722257293843,0.019432698993733215,0.01956703022041386,0.019760021911175867,0.019839154485704925,0.01985247763408013,0.019900123347790264,0.019954652765487364,0.019999897366718405,0.020016760643046236,0.020094967883533287,0.020166667434637693,0.02017140383896458,0.02021917091476002,0.02026232395706316,0.020275658092849477,0.020308185113369886,0.020315841083575127,0.02033515041670465,0.020364740452888982,0.02046535110708408,0.020470145998826177,0.020484386538020732,0.020567965817164822,0.020624089431658728,0.020650774800760916,0.02076747329911157,0.020805288934392905,0.020823082625262522,0.020823498597270913,0.020868476720175257,0.021026657439234587,0.021030354455886684,0.021037988680633533,0.021077999876774004,0.021113870563584124,0.021133202493709598,0.021158702470538228,0.02124470816712342,0.021270785317393386,0.021404114674579665,0.021408817700688698,0.021443662684199668,0.021506935782661336,0.021509654569744855,0.02155461647483494,0.021568515560310874,0.021575960498862342,0.021688351611318723,0.021694246591629725,0.021709210268775765,0.02171419092389416,0.021759478441389583,0.021815423126202008,0.021860635482249494,0.021900659347944773,0.02197140476986422,0.021980119658203165,0.022008688064138915,0.0221543141722411,0.022200571454938464,0.022229436032934362,0.022252184165954026,0.0224455901477803,0.022471102759561573,0.022473340253916153,0.02249085139423815,0.022580617101248646,0.022613264653177943,0.02262419799994389,0.022656058045133008,0.022657569536927775,0.0226833406100127,0.02268529707435889,0.02270146889955614,0.02276743960981019,0.022850235946647732,0.022877374173553896,0.022927146569612762,0.02293947012706473,0.022940013864543838,0.022980169488212594,0.022998322333277987,0.023001912206414992,0.023035366617556664,0.023068923418554693,0.023076559276775203,0.023082716844461237,0.023110326566690244,0.023111470635164673,0.02311791514897372,0.023126673073093504,0.023143099133758446,0.023151661694600166,0.02321327213362309,0.02324160236880379,0.023268157827692707,0.023270844188902386,0.023280742116641502,0.02328597313449201,0.023291982137391018,0.023299625427839508,0.023374600351720735,0.023385140420863773,0.023390251201754916,0.023607481949742513,0.023642063235395867,0.02364953534638365,0.023657750081256063,0.023751779400083002,0.023764990726565984,0.023845719010781183,0.02388806098353775,0.023898683419099134,0.023916960557799547,0.02397059244662399,0.023978090823230443,0.023987467824825536,0.023996008911110665,0.024017295156009064,0.024063258661233704,0.024070152718160202,0.02407699606526738,0.024110272392710335,0.024124720712991155,0.024133429442117167,0.0241460488506904,0.024154930334118054,0.024173671680008838,0.024229165763382708,0.024230514594779557,0.024231667932056146,0.02425841086700726,0.024270884122715303,0.02427112011803847,0.024274231882002263,0.024370340495362384,0.024371560499848077,0.024411831226708716,0.02445014227078306,0.024483047536093525,0.02451744245883172,0.02452109703179494,0.024535019678142037,0.0245474065394057,0.02458447777718734,0.02461061046412351,0.024640538567383064,0.02465801992899109,0.024664260191605326,0.024718906279896292,0.02475332649946674,0.024792842645366554,0.024797659243879507,0.024811997024905975,0.024834108312024303,0.02484090739646937,0.024859598535315903,0.02489289003862811,0.02490222441773541,0.02491823090175472,0.02492166322219588,0.024979930381612597,0.0249964392184054,0.025104110336122188,0.02510928167929033,0.02513005258505322,0.025134112583277024,0.025150044998048637,0.025153765071658785,0.025175926928676994,0.025194390555986507,0.025198383271843768,0.02529926958443989,0.025307871004882444,0.025315462065219563,0.025330825811804514,0.025336243459261783,0.02534375302126086,0.0253661369016893,0.025400882120119983,0.025432509431245774,0.02545397758759859,0.025489086176702473,0.025499388882312412,0.02550870198013397,0.02551906440533141,0.025538544759089194,0.025542076364523925,0.025570233173052997,0.025574604887310216,0.025592063817273126,0.02560972629956317,0.02562003241824896,0.025625940369636287,0.02562673329422897,0.025643240445520733,0.025657343735417358,0.025672453438424486,0.025727669769713725,0.025731823268599165,0.025739318640179698,0.025809511427916144,0.025830023203166685,0.025845179850498688,0.02587425638134263,0.025879599663471902,0.025921984771262695,0.025934324581588113,0.025941388933637028,0.02594500341376213,0.025950448308524488,0.02596635321041995,0.02598993254085063,0.026015764818561153,0.02602102778778987,0.026035660991166443,0.02610725689211248,0.02618312849984915,0.026194385081407017,0.02619709110165519,0.026197431005476824,0.02621649766844747,0.02623009358226332,0.02624941868583918,0.026250785430160968,0.026258584543597058,0.026261233016490533,0.026281452949258204,0.02628683277393739,0.02630569161751341,0.026341317620581596,0.026342926772361157,0.026403884909512586,0.02643922685395953,0.026453064731742533,0.026464080664085666,0.026475786502475673,0.02649419953506535,0.026505031757868412,0.026510914775988187,0.026528199468535508,0.02653245624204133,0.02657188194195258,0.026577751299554687,0.026609398883687685,0.026618985571620273,0.026643869979949823,0.026702732567765987,0.02674845798681998,0.02675716074582112,0.02677536240695766,0.026796212253141993,0.026798406696094353,0.026809806824679504,0.02682142391116931,0.026825864427439734,0.026836516068832344,0.026838647601628306,0.026903212373207468,0.026903831247338895,0.026908625857089013,0.02695610877403256,0.026977180816999274,0.026989549945212892,0.027005008478593735,0.02701149452322702,0.027016625832784323,0.027032936113267847,0.027038370835624083,0.027049423160419094,0.027074473437603292,0.027091685055901416,0.027093744906261975,0.027129700924266732,0.027142883500740213,0.027188781773008586,0.027215033948487573,0.027218113561796105,0.02721820339132392,0.02725464159562128,0.027269999840712045,0.027279787454584963,0.027291168649977195,0.027294453740322373,0.027357302364597554,0.02736703698453073,0.027468955752437206,0.027494194370106546,0.027503140647008328,0.027524337390250536,0.027636154075885357,0.027650589909958456,0.027668195510181154,0.027668408096347335,0.02767956761596016,0.027729056344417267,0.027730885843147313,0.027731371740134226,0.027742310684098652,0.02776894203026254,0.027779668302920175,0.027805046491182168,0.027816092977628,0.027833839675266075,0.02785237254696204,0.027857626667437872,0.02785863439517818,0.0278606802741293,0.02787418371456194,0.027877428041430875,0.027903100040919154,0.02792851332902337,0.027935414619231026,0.027937702027481866,0.027972421895186846,0.028012051054146763,0.028013817069453634,0.028036735868317175,0.02806486370579702,0.028067197267533893,0.02806768515372448,0.028077048835948165,0.028091025650364847,0.02811519455271493,0.028128362568227972,0.028163191999296306,0.028167201243790557,0.02820812965886934,0.028210790842111543,0.028223483544242832,0.028241551574778957,0.02824329752087057,0.028261883061798432,0.028264995082489884,0.028316358013681577,0.028320948977850972,0.02832239787858657,0.028346743089787862,0.02834912824218655,0.028358712889095965,0.028371411206245884,0.02839463722126622,0.028403691473078384,0.028453943947271482,0.028470259459011486,0.028482790407766034,0.028500094496453188,0.028513522483680952,0.02852481748580605,0.02853874581831594,0.028539805829079213,0.028549344203400318,0.02856632047178518,0.028607423523052844,0.028620275988488993,0.02864377172356285,0.028644758098152912,0.028657553820394827,0.02869644828334485,0.028722052266187494,0.028742667808341928,0.02874381735053947,0.028760190042339255,0.028775216314473218,0.0287818283131547,0.02878481526509848,0.028797998080216748,0.028801906483222412,0.02884185053339471,0.028866895441518027,0.028878949039466417,0.028924593162568756,0.028934007566412394,0.028952693114481363,0.02895467477194813,0.0289628426623393,0.028988848554184675,0.0289908969146776,0.029008735284646536,0.02901530233655213,0.02902436392581668,0.029026572552085145,0.0290318464509767,0.029053508892445824,0.02906771135302036,0.02907549441827535,0.029086876520187564,0.029111038724622552,0.029115673114168017,0.02912403791361128,0.029158694733706032,0.029165678521639447,0.02918142293004657,0.02919553534228944,0.029212333987881967,0.02921894413170763,0.029226626813925884,0.029232755163993908,0.029238557772228634,0.029244297081178323,0.029259860905395774,0.029337950928244122,0.029344520256952184,0.02935000497515965,0.02936380433140929,0.02941713885753603,0.02941750064006835,0.029461611709543062,0.029479294354073527,0.029479530819143528,0.029492018162981203,0.029496819990962386,0.029530656087997976,0.02954177889330255,0.029547136560446795,0.029576847930007782,0.02964399807150613,0.029645855203273397,0.029648204266867796,0.029691739827771867,0.02971905432016182,0.02974190049459529,0.0297506881401422,0.029783465018744487,0.02979525830010212,0.029812270982800225,0.02981937974789963,0.02982010292398906,0.029823116698827236,0.029869324003502912,0.029877774638553713,0.029881469093729723,0.029930239377829553,0.029931183494990365,0.029946443199474843,0.029965251325093645,0.02997815940170375,0.029992410763836824,0.03000001572107089,0.03000004433979835,0.030038402970583616,0.030039801893600726,0.030040582903314732,0.030060378138311077,0.0301250617711989,0.030137151976652832,0.03015738263275338,0.030161037719987033,0.0301809124174142,0.03019300595709089,0.030224681476996834,0.030256727764313894,0.030265191179926616,0.03027168253444578,0.03027235421772386,0.03027372313904837,0.0302804820000596,0.03031823351577275,0.030323014886991746,0.030334479213433775,0.030345034977189093,0.030378851343577075,0.030384548580342917,0.030385488939158017,0.030408703572199595,0.030417234547151735,0.0304200401787871,0.030433826401204418,0.03043720554753655,0.030468846901935166,0.030471080715471582,0.0305082150926614,0.03058074326406644,0.030605747983560905,0.030610337220358903,0.03063525996444269,0.030638374269594443,0.030645242901497532,0.030657968452321473,0.030671097033141463,0.030712927161573016,0.03074607407864603,0.03075323919366153,0.030769864331859083,0.030773292020790537,0.030783066699301496,0.03079058714024734,0.030800873070765115,0.030811330765189392,0.03083906794637482,0.0308433474683465,0.030908717782000057,0.030928640490149928,0.03097760053251803,0.031007764226335388,0.031007769339209047,0.031036391202884994,0.03105560910959145,0.03107454900922112,0.031077883430314274,0.03107960883548344,0.031126146319133054,0.031132113599709105,0.031163761510620485,0.031209528513647618,0.03122440027732249,0.031291352416540456,0.03129439770989175,0.03129954096630369,0.031301610287371356,0.031320795676263696,0.031321034301314374,0.03135558292015945,0.031376867636934036,0.031384074774910765,0.03141038960645783,0.0314252775245485,0.03143403518024881,0.03144959298943975,0.03145944257564677,0.031470283138436375,0.03154689574665695,0.0315514760133311,0.031566502999016324,0.031636301556362025,0.03164982970423775,0.03165969693279846,0.03168263541015906,0.03169139807306977,0.03170464438606587,0.031731156588957164,0.03173686674048827,0.03178864262433989,0.031792373105623016,0.03180002378217102,0.03181056839723222,0.0318206253883821,0.031870932967920484,0.031877078669673785,0.03190130436192968,0.03193677879596789,0.03193776227987362,0.03195945226309536,0.03201776457618612,0.032029031420663774,0.03203677051387521,0.032050071244130826,0.03205609843766124,0.03206001783754353,0.032105422451726255,0.03211492961913217,0.032131652543219,0.032143300321079996,0.0321445867664167,0.0321655520718568,0.03219192418479678,0.032195446346498235,0.03222364730628167,0.03224355730256831,0.0322891051409199,0.03233840523258541,0.03234745841731827,0.03235621325950303,0.03235675534142004,0.03236948072679037,0.032382332630558966,0.032394117338177195,0.0323944649940578,0.03240419217570916,0.03241648464712747,0.032421073763949856,0.032444014393376325,0.032468787320157,0.03248062821142087,0.0325004248980667,0.03252654054913872,0.032528953227381675,0.03254928617074125,0.03255517457147886,0.0325590960126296,0.032577412826501,0.03258599236429073,0.032586628294237044,0.032588143380407095,0.032594059523902943,0.03261151532110916,0.032633982695263075,0.032655693055685116,0.032659774582192484,0.03267680957355985,0.03268992592316622,0.03271014943796614,0.032742203541909334,0.03274280579443088,0.03279185128069906,0.03279358790902783,0.03279696431767786,0.03280400005540099,0.03282753347388044,0.0328585861023785,0.03288147114137119,0.03289958114307577,0.03290549138921496,0.03291803732905668,0.032922594573131604,0.03296970967379785,0.032972605495613476,0.03299638112198001,0.033014354263592297,0.03303022654044202,0.033039536799890676,0.03304946643177254,0.033050860080201604,0.03306758361281674,0.0330815166385921,0.03309359610689415,0.033097519657278585,0.03315451476584266,0.03315468186924949,0.03315540466653201,0.033172683897179187,0.033174074343323506,0.033208252815521054,0.033228356862917134,0.033263528749631295,0.03326387020751225,0.03327025702970575,0.033322487584285146,0.03334648934821534,0.03336908991348734,0.03339145761222091,0.03340037112424798,0.03341335601639604,0.03349797295321515,0.03351107545919459,0.03355789756827489,0.03355814791879892,0.033566180797273984,0.03357653779631607,0.033581758548002605,0.03358366789992939,0.03359982927255425,0.033679672623918015,0.033702229965004146,0.03370877769127788,0.03371933705331309,0.03373210054043842,0.033750775284092975,0.033756939008023706,0.033761635604176686,0.03380666647752883,0.03390893803111755,0.0339378284445714,0.033994610716533515,0.03399961769354494,0.0340284182520139,0.034061157798318184,0.03409247509600719,0.034105782433720484,0.034122573686880105,0.03412571695446497,0.03413332057911431,0.03418828582673053,0.0342415642934973,0.03426367689188777,0.03428856378398336,0.03429838904887765,0.03432205174925694,0.03437769772152448,0.034441896956939826,0.034455084597465706,0.03446985984316693,0.03449998508647879,0.034509321600054134,0.034525433115803215,0.03454587169090211,0.03455285593153557,0.03456756849994543,0.03457226826045849,0.034576264686013256,0.034664853390921006,0.034687562537961715,0.03481811505144193,0.034831977605744587,0.034834792832149346,0.034861069716705315,0.03486918228874662,0.03491121291129261,0.03492984809981065,0.03498604829635907,0.035011535492169336,0.03502579421705425,0.03502601660565044,0.03504829178267983,0.03505400357497115,0.035056054236972584,0.03508250490152499,0.03515429454179773,0.035158335243309934,0.03518806942207253,0.03519826724559403,0.03519876384638521,0.03520040439151266,0.03522828830036128,0.03524930262104256,0.03525998286984229,0.035260126504516605,0.035265201155846006,0.03527535202799454,0.035303674156741974,0.035312172091256096,0.035329293071309706,0.035356121982300676,0.03538511711931195,0.035393311946833546,0.035426176527506445,0.03543581086594214,0.03544735879129968,0.03547179385843316,0.03549321251077946,0.035534270090044215,0.035540882863009805,0.03554636583612337,0.03555670234519383,0.03562296133190554,0.035645608652868914,0.03565019107999329,0.03569436436620595,0.035700464930040185,0.035761975654121814,0.03576571885348897,0.035778122504206575,0.035796818458702924,0.03584549016729628,0.0358482735260725,0.035895843896078715,0.035910153866074955,0.03591389068644933,0.035926846238891044,0.03593268260003269,0.035939935520742365,0.03595177011242545,0.03597886978418115,0.03600280278652384,0.03606010850044848,0.0360829979229784,0.03609267020840776,0.03609788047216502,0.03609908771798394,0.03610203768835831,0.03611138243222464,0.036137746516279275,0.03614306172542441,0.036145806202960276,0.03619114582292761,0.0362154434955624,0.0362770243550594,0.03633935345720697,0.03634509108562973,0.036358890618280466,0.03640176520798667,0.036409215056105934,0.03641724486628241,0.03645742745872119,0.03646035732982224,0.03646620092932205,0.036492980954401545,0.0365509958225442,0.036585453061440225,0.03662219366084904,0.03665648856326556,0.036669356984060694,0.036672791394419396,0.03668447927942088,0.03669058493918007,0.03677486380709181,0.03678596186463634,0.03684077076055234,0.036879487682211595,0.03688603269629216,0.03690234548034813,0.036920700423011474,0.036968097340913886,0.03700604024642219,0.037026359209045526,0.03704170516583293,0.03705755038352544,0.037100615868274345,0.03713188343603664,0.037155147014996934,0.03719311540970039,0.037221470487117145,0.037248112710706764,0.03726133256359052,0.0373049840607738,0.03733091163752032,0.0373505483103246,0.03738722226461365,0.037410654695712135,0.03741901323187498,0.03745220510435829,0.03749222094625494,0.03753525991905067,0.03759753764947791,0.03760130566883553,0.0376130145378747,0.037623263262994866,0.037733746742033294,0.03778324658071218,0.03780141031505784,0.03783335129058594,0.03784420153399295,0.03788017987924907,0.03790209750990138,0.0379044247045063,0.037927200272425736,0.03794086906155046,0.037995750725886804,0.03800328106128552,0.038070915860391574,0.038111613427216744,0.03813276984030831,0.03815738420803907,0.03820222785805807,0.03820817103877281,0.03833450282148292,0.038348973864549114,0.038370279792154925,0.03837781629800159,0.03839187635680121,0.038408630539554775,0.03841430742538517,0.038477167371774965,0.03849448007493089,0.03851028150881346,0.038594158037175204,0.038597187477902126,0.03864317035479805,0.03865349385399632,0.038772961457441586,0.03884039200752812,0.03888864892643587,0.038937092774619333,0.03915086738651729,0.03917452779897355,0.03921578365605478,0.03924810031205861,0.03925812826678688,0.0393323489542592,0.03934165718593581,0.039353449867116204,0.03937416515863404,0.0393755592754465,0.039467416307358796,0.0394976605898748,0.03950143404201678,0.039514331723243444,0.03952385877525765,0.039599481789266325,0.03960258475261324,0.03971889983147668,0.039811637610243075,0.03984007373297264,0.03986677442127189,0.03986765566580608,0.03992079189022795,0.039961884254380374,0.040020422657154164,0.040057374169301344,0.040118657245707304,0.04012053994046537,0.040161212382940466,0.04019992297755233,0.040305584704376125,0.04033854472411819,0.040438810074009575,0.040456822734348426,0.040488700751278126,0.040535207963871396,0.04059598822468941,0.04060938848151126,0.040639950226500465,0.04066485105690028,0.04069379883443571,0.04073558406784149,0.04074588851567077,0.040807525910686177,0.04084874673868651,0.040855181346728595,0.04089185534641722,0.04089426436598622,0.04094291897893485,0.040976663815616245,0.04104389326571824,0.04109369040857955,0.04110615921729574,0.04115087292443669,0.041174440244056976,0.04121406069687399,0.0412381494722742,0.04124468410413752,0.04126297490487792,0.04128801934391745,0.04129214919328735,0.041324722628638885,0.04140021764185768,0.04147919206034543,0.041517200916077975,0.04160249485267704,0.04166313429231082,0.04170892660207604,0.0417706482588826,0.04180314964502608,0.04184497891410905,0.04199340560166737,0.04221471074474419,0.04226052268340531,0.04233567971317718,0.04237404975746928,0.042378126042416794,0.04246133255934405,0.042494206029102186,0.0425201524243601,0.04258199083817367,0.04267964074176134,0.04272681834953078,0.04288491851271127,0.04299002031572419,0.04332383454517504,0.04340535652806459,0.04343021962733234,0.04367793023277598,0.04377225304057726,0.043861973818475904,0.043881620466318404,0.04394347398247611,0.04400967875370303,0.04414400944402076,0.04420816505830761,0.04429932498312245,0.04430431285498691,0.04450855718555577,0.04487479077646363,0.04493834630091177,0.04494558142756116,0.04495013450312611,0.045077536316340226,0.04514789631604445,0.04530031256683788,0.0453366332547215,0.04537021064914843,0.045505432226121516,0.0456594366640241,0.04570045840633263,0.04602029022791558,0.046085754995615034,0.04646803926417054,0.046506819907978204,0.046609631447844226,0.046781695822479305,0.04678891404762862,0.04727619808540325,0.047973982530967595,0.04821219217083604,0.04898798665652766,0.049241118267749344,0.050768620825906954,0.05410899413520256,0.05416078427518524],"xaxis":"x2","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y2","type":"scattergl"},{"hovertemplate":"algo=omp\u003cbr\u003ek=20\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"omp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#EF553B","symbol":"circle"},"mode":"lines","name":"omp","showlegend":false,"x":[0.0036675585253509043,0.003930356094894521,0.003959853702641417,0.004287208610309719,0.004365410657961021,0.004645742417146344,0.004787317629747108,0.004976411058978541,0.005006784317267783,0.005026980537120783,0.005157165876137044,0.005208394411407063,0.005290558531976552,0.005488154838676663,0.005519372823857642,0.005523172149351257,0.005543819435860623,0.005547585248975752,0.005556885479642221,0.005622491502209769,0.005710476525492862,0.005722717682265786,0.005731499794270136,0.005783968026934904,0.005835128245445164,0.005838333224952275,0.005901681754831433,0.005916126220552464,0.005935978075597666,0.005962416248901643,0.006015313489301232,0.006024322230271957,0.0060568201665443825,0.006154290181926059,0.006171687054612405,0.006182582520882915,0.006320719412722017,0.006332865943769908,0.006335897343834743,0.00636447339035712,0.006381442492858965,0.006468936589613142,0.006519774875551107,0.006522094343564766,0.006525470111590742,0.006535611952205911,0.006538450600060948,0.006545423718687447,0.00656115707959735,0.00664534776245926,0.006651399138228618,0.006658954075240965,0.006662893082215998,0.0066970479081392975,0.006842688993347475,0.006890264380170345,0.006919264673597056,0.0069494895112452715,0.006958368222928925,0.006971384845789555,0.006983542927199786,0.007049685123636071,0.007076461660507821,0.007084850968908643,0.007114388977147607,0.007132626613457309,0.007142045607986892,0.007150467546063291,0.00716509793307713,0.007168461592442608,0.0071930279886581764,0.007207710503259296,0.007263012657561664,0.007291712711171343,0.0073117393784936525,0.007321633856468275,0.007333873357066288,0.007364112883385119,0.007384932157250111,0.007388319831894319,0.0074010993281562465,0.007428592887742116,0.007447460169666969,0.007457173437921173,0.007459120267567306,0.007464721470162601,0.007473421546979241,0.0074752378062246955,0.00748646095390562,0.007489227915178325,0.007490225747356379,0.0074951974241134894,0.007528543839180957,0.007565018255686569,0.007589677317121636,0.0075959554993875415,0.0076245462350596396,0.007656285296251933,0.007677723305957683,0.007692441152372857,0.00769574624530395,0.007700434586787976,0.007704260350867634,0.007751918947864325,0.007758626138478703,0.0078113257501141635,0.007827514671904097,0.007901485007263464,0.007907228034935421,0.007947500409983366,0.007956381538743406,0.007961871340003967,0.007969267927420556,0.008003595392930012,0.008006880785862812,0.008011755578504533,0.008012830420908822,0.008040261458229043,0.008051468723960026,0.00805357832729405,0.008054400168725266,0.008072123946986297,0.008074939964355566,0.008081371690636557,0.00809196401927909,0.008099452233176572,0.008110027920963157,0.008126881450086328,0.008140908609444764,0.008155329308358359,0.008158018353113023,0.008166038360024068,0.008214358382770303,0.008278658367357606,0.008292588949750993,0.008294043410402593,0.008300575540764929,0.008303599211972702,0.00831228623368466,0.008332630708810558,0.008335558412233372,0.008355670947304564,0.008358036464990134,0.008363352453835383,0.008366118732118035,0.008373061949177876,0.00839988407408926,0.008406565831782491,0.008422151535267696,0.008426970856774208,0.00842902684054898,0.008480720521011655,0.008484690737239661,0.008509818596306801,0.008510687963718533,0.008512325777290278,0.008520990524094873,0.008537298201193954,0.00853755607896248,0.008552055161328965,0.008557215556840967,0.008570277457095729,0.008586402309052362,0.008600652885351907,0.008616158378209369,0.00863342636499266,0.00863451045590975,0.008640400039941591,0.008643129878706529,0.00866502136717655,0.008677408805979555,0.00867845057827282,0.008682583707317473,0.008685281028987527,0.00870009265812585,0.008702898451163562,0.008744550404347354,0.008748997573612768,0.008751391935264686,0.008754659062187576,0.008759837463528802,0.00876705535091658,0.008770021469643147,0.0087715964703154,0.008783392489775709,0.008798889237359069,0.008799430093327272,0.008812853986857602,0.008826516688154375,0.008843155567822057,0.008855939061787155,0.008862580345792206,0.008864636786363757,0.008864987856181566,0.008865249088164962,0.008875747742863087,0.008893519596809308,0.008907205904519497,0.008915046899442803,0.008944797144787586,0.008945109217940008,0.008950131213000997,0.008950777557862695,0.008954465059885766,0.008979630981392673,0.00898100565508025,0.008983680098899643,0.00898495029482709,0.008986063121355841,0.00900713090475828,0.009020556305292254,0.009023984643081196,0.009049345113614169,0.00905190310879855,0.009052701851985569,0.009057413849924352,0.009083178857248752,0.00908950260107957,0.009091341519595965,0.009093404604603951,0.009104611663775554,0.009109324895404664,0.009110736989112234,0.009131868140142031,0.009144977812463703,0.009152975237624171,0.009157450564818618,0.009162910273002192,0.009183995076729193,0.009193559243744622,0.009208899649653572,0.009213619672843404,0.009216372346382593,0.009226081898694628,0.009228563253341385,0.009229614182492744,0.009247282429686038,0.00925050637776541,0.009253313421461356,0.009261202395389707,0.009272429004270341,0.009283435803960433,0.009285917805184883,0.009301339494172695,0.009353852817485433,0.009356265950808086,0.009377367158962946,0.00938129136541777,0.009382627734257043,0.009395661361607601,0.009427093037116783,0.009428502406078149,0.009441805867308837,0.00946171432184798,0.009480780737074163,0.009493212536738535,0.009494959668482572,0.009513196341603285,0.009513817269198418,0.009517114329724682,0.009523128183715205,0.009523684974763873,0.009529963541980721,0.00953332132729043,0.009564028826055226,0.009565165340653744,0.009568239576700078,0.009588284133435217,0.009636172714368259,0.009663125246773353,0.00966725108488873,0.009725163674266952,0.009729600292919204,0.00973450680893789,0.009737986749927338,0.009739318756503485,0.00974468411111017,0.009762449649355614,0.0097662946823129,0.009772457804942689,0.009778221149388403,0.009791120439535521,0.009792979814402898,0.009797750443487609,0.009798663237600666,0.009801854809745799,0.009804017075194307,0.009806533461289745,0.009807726435569498,0.009808118274303147,0.00981171073810615,0.00981386166377508,0.009817343708547949,0.00981858091017381,0.009819839711650248,0.009824227284744304,0.009830299380874864,0.009843153671921947,0.009843494369490955,0.009846280136940768,0.009861750487438417,0.009864676277491536,0.009870488445106322,0.009872142098300318,0.009881432883420287,0.009882622875912326,0.00988567571834832,0.009904802438858655,0.00990494618874844,0.009909253101046896,0.00991695335387746,0.009929056185206965,0.00993653039019566,0.009938357627171778,0.00994654043865058,0.009947933957454002,0.009949474997414068,0.009949896424424936,0.009951657882572374,0.009959638568068124,0.009959855924115812,0.00997679697840191,0.00997733648230534,0.009978109842462383,0.009982905816572251,0.009984916699550862,0.009985726521085477,0.010004707018766,0.010005542872904049,0.010006497727752158,0.010014174862533077,0.01002524220355793,0.010026618155653862,0.010058955881022935,0.010067801304332233,0.010070158259339289,0.010076061549721689,0.010081339444388205,0.010084637948300763,0.010098331555598326,0.010117374944909503,0.010119130297825909,0.010121928100777862,0.010145240185765228,0.01015245161645523,0.010154497264235734,0.010171434936311001,0.010175304251837557,0.010181669522092665,0.010183156730986147,0.01022008203612865,0.010236374223101193,0.010253419085083367,0.010276763567040156,0.010279827583846522,0.010306908965002073,0.010307774977480394,0.010330934017586217,0.010336025466391275,0.01034722168006531,0.010354008431611588,0.010368108002364413,0.010371756417402938,0.01037722643790852,0.010385866891045904,0.010389393300750508,0.010393497259368478,0.010394816485757043,0.01040233072262094,0.010417469345020764,0.010435193311871104,0.010436241561193193,0.010438977762992655,0.01045163438308787,0.010455189757352056,0.010474182712797642,0.010474560387138308,0.010509631617524027,0.010511155979194012,0.010519003615373107,0.01052390534968539,0.010525901045018401,0.010549705815458287,0.010555286566719094,0.010558599117522535,0.010564593817944843,0.010565079720737988,0.010569488116008089,0.010582245040855226,0.010585794601436816,0.010588468122962567,0.010593023796399018,0.010594017914973374,0.010597295339408471,0.010600057768388018,0.01060618729152325,0.01060982297889657,0.010610420221291187,0.010642877570764585,0.010646647099407328,0.010650427228240663,0.010659642418825725,0.010659830231014846,0.010663456467130647,0.010670279145487243,0.010678359773272979,0.010708782297795743,0.010717233770539366,0.01072179162820475,0.010723340893944989,0.01072594654403022,0.010747539825226879,0.010761005615511725,0.010763803324591927,0.010763960503169266,0.0107720107754097,0.010779019938518269,0.010798558480414068,0.010813737420160922,0.010824841088983728,0.010827803213944982,0.01083698599502014,0.010847282402535713,0.010858005206604989,0.01086476375630665,0.010867258519432602,0.01087195813977427,0.010904348535745978,0.010912441156714766,0.010912581205540152,0.010922885952711896,0.010944181273055997,0.010948830981745448,0.010968451597420655,0.010969465563019282,0.010979537122997995,0.010987813934068596,0.010990937662564444,0.010995904525875724,0.01099845349957204,0.011015388629937768,0.011037078196575375,0.01104159628688103,0.011046809715550018,0.011046843585424893,0.011058182427136128,0.011060519850608259,0.011071119077197765,0.011073651821779667,0.011079666347867282,0.011081866973391329,0.011086107936244746,0.011086131397864027,0.01109710872026071,0.011105106371577878,0.011109564319496378,0.011115351511909729,0.011117470651013662,0.011119867653459502,0.011145768088477009,0.011146696866248388,0.01116590527588086,0.011180379604832893,0.011191549332832205,0.011215514830677037,0.011233006715532474,0.011235028356293763,0.011253016635997811,0.011270295128593724,0.011279424731852264,0.011281372900697754,0.01128198937111331,0.011289428890238906,0.011301775050588376,0.011305999002007046,0.011337010235132972,0.011346799047592313,0.01134958526037148,0.01135756282300748,0.01136069426123951,0.011380477824652629,0.011384063990266879,0.011387958451632277,0.011401017309587836,0.01140719213434286,0.011415730779793468,0.011426859979932867,0.011444237259889136,0.011446032260819656,0.011452917454799419,0.011456820997030288,0.011457671186272684,0.011460690517500758,0.011463674168523078,0.011470450036648573,0.011497197193525205,0.011510485177589323,0.011542578386188423,0.011544791284433017,0.011555131596935062,0.011576360671074485,0.011579271315714342,0.011588999041411358,0.01159906609358453,0.011606834952489615,0.011614420292068373,0.011615678359816277,0.011618661177937667,0.011623059533295146,0.011640310311679301,0.011668872760858089,0.011673753520445395,0.011685727957172511,0.01169130651339582,0.01171210790715287,0.011738118008504519,0.011740457563110356,0.011751724745443715,0.011762289672646921,0.011762736063385845,0.011795421432201891,0.011796207076218866,0.011808867566709128,0.011810620841134517,0.011810849713639745,0.011813593561407328,0.01181783931918772,0.011822335252009993,0.011825013528926374,0.011840122043996803,0.011843535155545646,0.011852748110005567,0.011859803734680123,0.011876474818458205,0.01189658588241997,0.011900098228764028,0.011900262807869997,0.011901740212034037,0.011902605357347985,0.011926251567261226,0.01196425090551767,0.011970543576898373,0.011981969006204855,0.011983960048733436,0.01198938256989935,0.011998871299830394,0.012004771563472055,0.012006544197712878,0.012009479396527079,0.012018615606739353,0.012061005105069873,0.012068434934932723,0.012082621275572884,0.012091500796323665,0.01209649128870029,0.012114850581744541,0.01211518614415609,0.012117276573702812,0.012124602710977987,0.012128028024980508,0.012143206220113568,0.012159991327596266,0.012160971671506779,0.012161718438771073,0.012179368524539862,0.012183374502728623,0.012183548389097338,0.012189584764935627,0.012195628944335766,0.012199912519479404,0.012200352826916745,0.01221249221835644,0.012215967147595486,0.012237451459693835,0.01224142736627248,0.0122490541909495,0.012261010413756119,0.01226361565100083,0.012272745264585001,0.012273779551026327,0.012299470690150619,0.012309493591294376,0.012316169806156785,0.012318737455744506,0.012321122703014617,0.012336226482167199,0.012364433201839959,0.012372101465058877,0.012378602299776557,0.012393401571907253,0.012400057467541786,0.012426632052762544,0.012463170952578626,0.012482836707850609,0.012506839399648941,0.012507278049606666,0.012515231949464975,0.012521310030553987,0.012545372902908594,0.012552279035210046,0.012565400443100876,0.012574129428559839,0.012574630491667219,0.012576922899566054,0.012586165460900824,0.012589300861138278,0.01259895346045122,0.012605945153257045,0.012611836453731484,0.012625528737639757,0.012658514304086085,0.012663019089481989,0.012663120309710197,0.012675037340194836,0.012678766018379842,0.012688349883300388,0.012688794216788849,0.012689651137260815,0.012697512997747304,0.012706155182897295,0.01271803688185672,0.012718446831590147,0.012722034415046556,0.01272882716672905,0.012758420249133661,0.012763425292460006,0.012764708638787653,0.012791165137076562,0.01279263842241136,0.012806927618501388,0.012818316945614383,0.012820295345831461,0.012849615701319594,0.012858737014312286,0.01288748346436997,0.012887513671039802,0.012902631593299446,0.012918064651757846,0.012943627630646423,0.012948149684352725,0.01295841992594643,0.012979739585582892,0.012987208852066478,0.01300959596789,0.013014013020817419,0.013023787316783886,0.013027171279817642,0.013047173680181944,0.013053558007389898,0.013083893791310813,0.013102639533233398,0.013122946595569238,0.013131736211310719,0.013139030258755856,0.013141498172260282,0.013181333341204882,0.013190287485472556,0.01319411975883559,0.01320594151522123,0.01321340764186297,0.013221883145422584,0.013223034021742597,0.013232949024371268,0.013247465417456781,0.013247662277974092,0.013270537561058351,0.01328017508622861,0.013290846115554108,0.013292037539183916,0.013297451720714986,0.013306981499692746,0.013354389664611516,0.013370315374347455,0.013373600059393632,0.013377431113787858,0.013385502574901722,0.013389329643141193,0.013390019792798283,0.01340437425081858,0.013413156598086553,0.013413834669040248,0.01343138911807168,0.013438383070671672,0.013443929797149756,0.01346921573062464,0.013475745279676715,0.013493339591869569,0.013499043724698198,0.013502204859154673,0.013515607165710963,0.013521799543543059,0.013522700743640698,0.013527423642185298,0.01354713221926438,0.013551644359902987,0.013564467435594413,0.013592006101444443,0.013592289489712573,0.013607848438885697,0.013617023493381236,0.013648205972734542,0.013672948701013547,0.013688493906393859,0.01369508675767695,0.013697897042190238,0.01370982052730126,0.013712197637141186,0.01373847572940494,0.01373855343361222,0.013783221117291331,0.013791983210755693,0.013808811209896419,0.013824884066458371,0.013842824928722564,0.01389625055971174,0.013909668420989438,0.013927361383751963,0.01393748757720332,0.013941374699651968,0.013943377431199439,0.013944904349333523,0.013950829469480555,0.013953289835830038,0.013964911231411279,0.013993567760700936,0.014029754767323007,0.014030303768228286,0.014031471456925038,0.01403914993997513,0.014046129493058172,0.014050420712616547,0.01405688456083674,0.014061544049725589,0.014070603542904842,0.014088350672975623,0.014100978253518512,0.01411415334063738,0.0141310059275444,0.014133663647938676,0.01413393161050162,0.01413927796677735,0.01415202650708445,0.014175902340351185,0.014180192748229476,0.014195796421974605,0.014207399238179708,0.01423014972327415,0.014233778421614876,0.014256962351927114,0.014278336574396217,0.0143087715822873,0.014315996808366604,0.014320483467141903,0.014321619680240677,0.014324939763345737,0.014338018623374513,0.014352618229465448,0.014370062448481154,0.01437769642541335,0.01440257095501267,0.014431942497644495,0.014435835184987741,0.014455286701678685,0.014457919901459695,0.014474028152236269,0.014487822389085536,0.014520821115835919,0.014524450380531847,0.014528408550452218,0.014531954539631001,0.014551201767471943,0.014566349262670416,0.014577119789326246,0.014585893196330931,0.014587122473661429,0.014598177053955178,0.014623956843317767,0.014647146827321735,0.014651099325623007,0.014654216746898183,0.014692867947696461,0.014708710078329995,0.014716374054354516,0.014717630287925864,0.014725507565335732,0.014727150771912195,0.014734590414793363,0.014737375443628997,0.014739949317559365,0.014744883925892922,0.014761787348928673,0.014770725815745938,0.014782126207707918,0.014791569543557296,0.01480069464817783,0.014805811054947946,0.014805969689963717,0.014823784691162583,0.014845777832567974,0.014877441025159196,0.01488481417375545,0.014891826720558134,0.014900698007592453,0.01492801255981815,0.01493797363105418,0.014949995507241978,0.014976647879305197,0.014984527361410307,0.01498798668028021,0.015000940807844287,0.015006628773388125,0.015012496901191566,0.015037500576885401,0.01512518940195963,0.01513818808027097,0.015141028396569908,0.015155979017393106,0.015160578368180567,0.015190526941504473,0.015197487727827025,0.015207448400119335,0.015231130346666424,0.015231436655351044,0.015233316841807096,0.015275830396146939,0.015292891567402708,0.015307851436206072,0.015312505563954058,0.015318612011319335,0.015340525786520707,0.015358040841400892,0.015363971054164292,0.015367586535405492,0.015379890682234502,0.015386786647883335,0.015412847728655157,0.01544030987090698,0.015462545784411313,0.015472325229248468,0.015475271652699083,0.015544066231417167,0.015546173182881162,0.015548626186368503,0.015578273496957529,0.015588560385312164,0.015617279645429753,0.01563449460468985,0.015653135613322475,0.0157144327755347,0.015735548868132766,0.015759414088711007,0.01583931469752725,0.015858454003485376,0.01588200338229472,0.015882538153361027,0.015904627370671342,0.015914881921237536,0.01592353007163619,0.01593576271958543,0.015942719776271957,0.015947529448007543,0.01603400561750625,0.016065481846374702,0.016085206770035932,0.016085596952769725,0.01609385621761902,0.016112254040286673,0.016138870727344826,0.016152253318524067,0.016202164412250586,0.01628421031919391,0.016328297106150202,0.01633076557197902,0.016352552682505565,0.016356676209363857,0.016362716044182733,0.01636509297171195,0.01637122277445246,0.01637650460359156,0.01638443895995343,0.016393697025873698,0.016397924744741752,0.016406779218145102,0.016413918116600805,0.016414055235779774,0.01641633163378425,0.016458135218771885,0.016468144276038644,0.016489331061428657,0.016501281561670838,0.016524813524613307,0.016538154495742204,0.016553549621609993,0.01657178099510974,0.016572295248422124,0.016573369325913197,0.016586638035045488,0.01669338745824838,0.01672888399068085,0.016747643593875237,0.016842613252001367,0.016850827769128225,0.016899076058332282,0.016941870201480004,0.016954223331095163,0.016969365244592537,0.016969962958042577,0.016972090090213787,0.01703872452495781,0.01704450025937234,0.017065090445895128,0.01706813747466537,0.01707096729300379,0.01709884772677996,0.01710323884217119,0.017145975260333287,0.017151203889077395,0.017201943724760516,0.017222880125190753,0.017226180153176965,0.017231566283678116,0.017276417307026795,0.017303286406136122,0.01736745651682114,0.017388121281353926,0.017427447312728335,0.017430445043650784,0.017436510461745904,0.017464606314978166,0.017604854305200153,0.017701519060091854,0.01780835015805325,0.017826761559360232,0.017903081745231038,0.01802025687243238,0.018022153247109228,0.01807259686351949,0.018086577377173195,0.018234542657974417,0.018340326612750625,0.01834162004878931,0.018411161105335354,0.018489544356150533,0.018490868571436815,0.018499648845831756,0.018566890169167453,0.018658976677850145,0.018671391749938482,0.01868109204655429,0.01872158309399337,0.018763245395200884,0.018780580159105324,0.018806515397242814,0.018816674350702292,0.018881996040670593,0.018932407995063835,0.018939018370917864,0.018990877498013717,0.01906995595786693,0.019089322279200918,0.019106464692867364,0.019133411308448152,0.01919679355482934,0.019197966880772896,0.01924104773557933,0.01931416707097174,0.019557804400341598,0.019591091458159092,0.019780532801342284,0.01980172918689208,0.01984591322963567,0.02003580893374536,0.020061871642997654,0.020155461840858,0.020200113953535177,0.020386607489974125,0.020387138750885967,0.02039400142780658,0.020542607164639168,0.0205645102311017,0.020591089230984398,0.02086131725815807,0.02091023507200915,0.0209977032139012,0.021044381180903363,0.021057702145725558,0.02155316061230502,0.02181173068888483,0.02186098921695926,0.021876254635163038,0.022124828864199732,0.022374407500928815,0.022793730494492844,0.022941920259874543,0.02299387157746879,0.023801733194763905,0.024228603064769552,0.02472553197559278,0.02528451722204058,0.025665739621031414],"xaxis":"x","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y","type":"scattergl"},{"hovertemplate":"algo=momp\u003cbr\u003ek=10\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"momp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#00cc96","symbol":"circle"},"mode":"lines","name":"momp","showlegend":true,"x":[0.029001337378538966,0.03151441523920317,0.03403631834697995,0.034575508836611965,0.035396430870465026,0.035436117450543626,0.03547272380083096,0.035866072529526984,0.035923415428292464,0.0363647209926648,0.03661367438478282,0.03665629052715034,0.0366899699898125,0.036936258067869286,0.03699829713749727,0.03724639343252713,0.03783541263282317,0.0378509987932974,0.03841453236225494,0.03852859226786537,0.03864424781842043,0.039360890071331804,0.03950850052939285,0.03962098449798986,0.03971518049756826,0.03973182664775821,0.039738741933464286,0.039826863930659734,0.0398631878253507,0.03995763686409547,0.04007350964429409,0.040191353543898314,0.04019900419598318,0.04020839670573427,0.04026682788677734,0.04035992304440057,0.04036998437192362,0.040529968689306764,0.04055776451509935,0.04070937616642572,0.04076760370826561,0.04077992604405727,0.040844355191202046,0.04086935166585945,0.04087675206506739,0.04107266143392114,0.04114583004509108,0.04119397098276141,0.04128598945285221,0.04141665587791009,0.04144733484928572,0.04151953117000018,0.04164732342232916,0.04170820698938511,0.0417280730463041,0.0418464480181469,0.041857622584439556,0.04187513631543315,0.04196535400619009,0.04197789319515641,0.04203455800151905,0.042178458145423404,0.042225423883385585,0.042375989785728256,0.04238874890743009,0.04245193885918037,0.04256032942663515,0.04261581755566726,0.04273268583578353,0.0429500101048511,0.042954336942792264,0.042971254954980956,0.04303067210562298,0.043046204859107325,0.04310086145421114,0.043127970191016954,0.04313570091292743,0.043179531560378966,0.043212963740829655,0.04321484467930748,0.04322647804107109,0.043242373456264915,0.04325836282275562,0.04325861534498616,0.04340870666582348,0.04342715593113425,0.04342853748688837,0.043428974906556894,0.04348818085167029,0.04349892715588914,0.04353123030764818,0.04357336140284423,0.043588406045053596,0.043606158267729705,0.04366703595129403,0.0436940785264415,0.04374935711051865,0.043752593768606074,0.043804432346965155,0.04388784412686035,0.04390078232253759,0.04394055836786472,0.04394252339470784,0.044061746394238696,0.04412877440579639,0.044261590340463314,0.04427540001479255,0.04438554171144625,0.04440055460493429,0.04448069675906063,0.044482564202910684,0.04451389216248323,0.0445164588279352,0.0445368240696926,0.044546844749417405,0.04456814939974399,0.044613518134623914,0.04462552575668753,0.0447107497105692,0.04477369055364001,0.04482337460726732,0.0448286192304787,0.0448593651154164,0.04487828650186215,0.04489562487381037,0.04496544119624383,0.04497020781606571,0.04499265052754372,0.044997590474586506,0.045043029760675014,0.04507239550337274,0.045112533461884916,0.04511878761576331,0.0451523517264435,0.04518796535834603,0.04520245692518235,0.04526637569406254,0.04535501627140644,0.045450156711031406,0.04546386059195286,0.04547137218360724,0.04558958137037562,0.045598606177415665,0.045697080858870295,0.04569961271645689,0.04572274551578817,0.04575162565963655,0.045760438250466315,0.04579288224647337,0.04582198708911178,0.04584629692604334,0.04594918520790771,0.0461456753965743,0.04615967502894569,0.04619829030023591,0.04621478912267428,0.04623910586759651,0.046262120877510776,0.04628842158880555,0.04629280659260728,0.04629821875441535,0.046300448815662046,0.046310307530326465,0.04637982811665524,0.046418863564094734,0.046441075409820516,0.04645519606953471,0.04646028662904001,0.04649301513531967,0.04651738904951659,0.04658892311683152,0.04659311688872482,0.04664112415927957,0.046731250439007846,0.046741015347515076,0.04677462584613231,0.04678725631140602,0.046795836332683714,0.04689191076158621,0.04692110476724438,0.04695321224440967,0.04696672713803921,0.047002067650552766,0.047011875999403,0.047048377585941284,0.047073721263432976,0.047077124064605666,0.04710726700808885,0.04716119375750334,0.047169689920890344,0.04718030092624794,0.04720654442943257,0.04721037153903153,0.047217304485112174,0.04721981072741173,0.047246944049472286,0.04728117267620992,0.04732075986083289,0.04732893450879065,0.04733329680904703,0.04735349211127495,0.04739754792344485,0.0474095538713578,0.04742875922534758,0.047438867657100786,0.0474403891336938,0.04748503909564945,0.04749807281062817,0.047498306839639834,0.047499736925653124,0.04751822605237351,0.04753715074758173,0.047573556732802,0.04758464198211456,0.047604749963299706,0.04763117963300838,0.04764570856647517,0.04768202671758155,0.047703482432516195,0.04772514043449379,0.047728792907678855,0.04773188498206267,0.04776623229645785,0.047772154180698696,0.04777409176468434,0.0477922677367675,0.04781178699152696,0.047874463878197736,0.04787450152097746,0.047878228908985535,0.04788049145711026,0.047881515660795716,0.04788408997846831,0.04791584961459276,0.04792610473911613,0.04793728862615646,0.04800133486782356,0.04803423640097007,0.04808083678129717,0.04809790892552586,0.04813041017062118,0.04818440674143125,0.04823172618903908,0.04824518684247929,0.048246464030749545,0.04828979554039295,0.04829055334411327,0.048294924750148585,0.048308375262823604,0.048323019665024045,0.04835340327834603,0.0483615967649715,0.04838122979395189,0.04840335809695473,0.04845547912259641,0.048481640267511186,0.048528104339743826,0.048603131203125795,0.04861609480344711,0.048625720132562264,0.048629888724412165,0.048643414554600625,0.048664350212474894,0.04868633991016785,0.04871672084906211,0.048748248868322394,0.04877661733889879,0.048844093024231205,0.04884541174688749,0.048869361427126654,0.04890098161904699,0.048912407714434426,0.04893248684854092,0.04895819937962383,0.048978077980470144,0.04899398800623733,0.048998644986541155,0.04900175864858963,0.04902799713066368,0.049031176018735335,0.04906434960657922,0.04912007311657107,0.049127819089764976,0.049151362269009666,0.049169253468451045,0.04920880700524137,0.049209729344064444,0.049209792546444364,0.04922440910110433,0.04924016680135966,0.04930528930476908,0.049333642053990955,0.04933422087200824,0.04935285396491655,0.04937623620036487,0.04938580060679097,0.049396266337466244,0.04939675138976065,0.04941291619928552,0.04941714209817124,0.04945153555184152,0.04945241281797938,0.04949477051125749,0.04950777880517015,0.0495304685952877,0.04956727454059974,0.04957800219972976,0.0495930663079028,0.04959960805262284,0.04966216496602155,0.04978028731209163,0.049781039903070014,0.049787859193487195,0.04979777381182578,0.04986171710664056,0.04986576019794197,0.04993250471312109,0.0499446479152504,0.04996512190792769,0.04996544023105065,0.04996924533622733,0.049975908528109916,0.05002311331016416,0.050062118418566434,0.050096721209896646,0.05010225481624235,0.050108220908868864,0.050113454719518426,0.050118358237073946,0.05012680350795323,0.050158631380802045,0.05027389079023379,0.05030103436850895,0.05031917120874656,0.05033097565746551,0.05035954292982005,0.05039431165823177,0.05044012985233998,0.05048637563446592,0.050542938264185835,0.05055636981378988,0.05056890167012749,0.05061108768133944,0.050691132102364714,0.05070472951183382,0.05078289120225975,0.05079514269031234,0.050817644940670814,0.05081821080809995,0.05087236999167951,0.05100393999329267,0.051105590389467916,0.051129205686271675,0.05115340712003104,0.05119259755254313,0.05122054264124282,0.0512441600859058,0.05134597683313417,0.05136381732631221,0.051379987804496,0.051392258738291896,0.05143686461380418,0.05144763470406436,0.051478532279237474,0.05150158151456635,0.05151505457602652,0.05152190163281439,0.05153288055925285,0.0515371699652584,0.05154196563948661,0.051542986738984733,0.05154615537595569,0.05154867627218597,0.051552513165222566,0.051605581666059386,0.05162181613839789,0.05167372774004998,0.05172662559368526,0.051739220324176674,0.051780152322806736,0.05180388172273755,0.05182585456378276,0.05183398454580716,0.05186617172356017,0.05187352086770549,0.05190223183990879,0.051905394747125064,0.051924230282782494,0.05194103105667257,0.05194171816400385,0.05197857962248674,0.0519876544628068,0.051997406709188176,0.05200745023750256,0.052014912444110405,0.05202099815827125,0.05207212489183301,0.05208655739396747,0.0520992180132417,0.05212789095554402,0.05218474554020989,0.05220004201164694,0.052221837735687816,0.0522666857774711,0.05227735261056502,0.052277530082441515,0.05231157822521233,0.05232962617640986,0.05233845528171319,0.05243001246981986,0.052448680033513426,0.05247040062041034,0.05247316560043752,0.05247534100743698,0.052508938669777926,0.05255948447064266,0.05256432260435858,0.052572033603235216,0.05261107711065036,0.05262997694159573,0.052668003713619814,0.052716722186971154,0.052724536447159534,0.052733234535693105,0.05275293775724266,0.05275847113216312,0.052770200192421665,0.052815028782806764,0.05284013074339613,0.052864679760720526,0.052888747817873026,0.052889604124909506,0.05292802579770602,0.05295004614407025,0.052955331246082746,0.05296292473282688,0.052992510265090574,0.052996365512123667,0.05300369543751352,0.05301222595275486,0.05306212174864193,0.053083337928560737,0.053130079957921225,0.05322482108208389,0.05323976049985535,0.053270827155790124,0.053346979388099715,0.053483031089565834,0.05348395422980484,0.05350010190633662,0.05353239535132527,0.0535432884547082,0.0535503200848204,0.05355150888140999,0.05363359110012389,0.05366732498481391,0.05367558542417189,0.05370916206292886,0.05370933939444496,0.05372204018959985,0.05373367336281896,0.05373523139042593,0.05373779042691105,0.05375739098521436,0.053772401520310084,0.053774737130408465,0.05378098831270393,0.05380134479903601,0.05384465955575596,0.0538657468399142,0.05387919146393337,0.053927277894384895,0.053934701792076545,0.053944945104538886,0.05396052570023376,0.05401240140588579,0.054039887462427924,0.05404889500126046,0.05405914957866929,0.05406567508819306,0.05408725862411066,0.054139118254777574,0.0541916339850856,0.05419828990690907,0.05423216111086071,0.05423283771625078,0.05423422685016473,0.054264105814899474,0.05427446069915689,0.05428373130961963,0.05434456821732601,0.054348732820147815,0.054350593756673934,0.05436090872620901,0.05437949282978057,0.054403249657038036,0.05440613458703797,0.054406359729015086,0.05440899995157929,0.05445482746632048,0.05445580244700837,0.054465406635936756,0.05446801654721325,0.054477444800836904,0.05454247059682884,0.05455350760207291,0.05456295341468451,0.0545999834113013,0.05460069568941052,0.05460741679519334,0.0546176635729585,0.054635276833792865,0.05469363508945346,0.054747243648956334,0.054767029072186245,0.05478209948433196,0.05479125860806663,0.054800429931837966,0.054817161324366204,0.05483021229791356,0.054845469919386304,0.054871185726646655,0.054888076130671745,0.05493883297516243,0.05495630546093453,0.054993817872563286,0.054997913344152737,0.05503241119083564,0.055034119111651444,0.05503452736559793,0.05506865515817333,0.055097822163353516,0.055113308969777876,0.055142421828742315,0.05517664849581053,0.05518365996632594,0.05518898410621156,0.05519483122739911,0.05519999334108779,0.05523825843157511,0.05523901349589936,0.05526501950316455,0.05531515419159718,0.05534601766316144,0.05537274864168738,0.055381715383461635,0.05538497630030333,0.05538605981059047,0.055396960549951545,0.05539861724443275,0.055445567033571674,0.05547381832937759,0.05549793734465114,0.05554982939200711,0.05555103691320682,0.05555802448290703,0.05563707111028574,0.05564991579177957,0.05565241162815574,0.05566307871614479,0.05567885876961004,0.05574006064975214,0.05575025071204488,0.05575546599030986,0.05575943742527216,0.05581727177516989,0.05586468393608187,0.05588238047073182,0.0558837993851785,0.055887238933778095,0.05589402723106686,0.05590068664837476,0.055942226084966594,0.055980973775571964,0.056023126205347606,0.056044023626572535,0.05605015557062445,0.0560610821332882,0.05612349336604119,0.056142924361855695,0.056150651066230704,0.05616085701236482,0.056161844109010886,0.05616542437246623,0.056205883229402856,0.056229061694308004,0.05624804971401069,0.05625030356472144,0.05628690336853324,0.05632519612985465,0.0563268570125087,0.05635311722461249,0.05638486797209531,0.05641200038319762,0.05642875457749556,0.05644340964557326,0.05646333297107936,0.056463460062850836,0.05648906926787508,0.056514793588953394,0.05652218475470139,0.056534140141366575,0.05654714580653928,0.05654827083402002,0.05661371526263674,0.056627414940960144,0.05670577063739461,0.056714292819666036,0.056776972262790014,0.05679147525103617,0.056835948318509896,0.05683898785749673,0.05685778212403243,0.056862004024244836,0.056882729410151364,0.05695310872588229,0.056978692328325586,0.056984340427340144,0.057002388542205705,0.05701123417081222,0.057021662224175484,0.05703087218637463,0.05704880226508137,0.057064632831205477,0.05706736746949137,0.05709070641130905,0.057113912978479967,0.057219513714351265,0.057255214561766764,0.057268895634664384,0.0573117178073019,0.05731303502621352,0.057326098699712155,0.05733489804116452,0.057340352775572326,0.05734416001592009,0.05735481324616517,0.0573777322103009,0.057382556775537885,0.057391843263341256,0.05745607063382178,0.0574567451616909,0.05749751419608048,0.05753158173086059,0.057559749875565745,0.057562155661607085,0.057580395858874034,0.05766175716667009,0.05766570319151628,0.05768770207971972,0.05769266857522555,0.057693005337963434,0.05777741434750405,0.0578259668064092,0.05784577636734093,0.05787614861167288,0.05790696298173568,0.05791437984768903,0.057945947418387954,0.05794928199775549,0.057978171257875896,0.05797876011224656,0.057987864848586944,0.05799369317774912,0.05799628207207119,0.05806899019041399,0.058075287157983616,0.058077427701473894,0.05811022173244421,0.05813955546830607,0.05817522988842957,0.058194530176185866,0.05820059153810969,0.05823254789314206,0.05825953319809133,0.058266288486376085,0.058271760184554065,0.058285231453981644,0.05829553877672015,0.05830988914145228,0.05834229365543594,0.05839463433954695,0.05840840954125557,0.05841619643874359,0.05851166798298779,0.05853833317649983,0.0585611112932212,0.058570976005944145,0.05857180204638503,0.058578284374474945,0.0585837844832824,0.0586125194405567,0.058635304019186166,0.05863916528794507,0.05865598994522114,0.05866350646239381,0.05867966541968252,0.058683656369953915,0.058685474744985346,0.058697916667423945,0.05873231715109416,0.05875040866947401,0.05875923043607349,0.05876468811085461,0.058797901358424734,0.05884176357022147,0.05887035570895173,0.05889432152010328,0.058896915028729,0.058954717869006544,0.05898348979346291,0.059024020523451834,0.059135447323562684,0.059161758272806994,0.059231281835004514,0.05923199126453765,0.05925130190583798,0.05928723002578273,0.05941227861358076,0.05946186474157693,0.05952386702159831,0.059550404572705405,0.059577654533529464,0.059589409220802095,0.05959310179790115,0.059685766100157155,0.059726035704298776,0.05979484995428155,0.05982120579591799,0.05983809608827284,0.059843636608394056,0.059873484047497906,0.05988103519711239,0.05990705801146823,0.05993431869888827,0.05996025056481391,0.059982109547231266,0.060013266403140496,0.06002280921495969,0.06003678404481618,0.060180192135825025,0.06021193907349254,0.06027606977333191,0.06027652373727479,0.060277908714903336,0.06028780137128825,0.06029824297149592,0.0603633563510283,0.06039777559828831,0.060404414715874696,0.060423143637778884,0.0604617045148952,0.06046378309811371,0.06048511575975655,0.060493911132400914,0.060500837466604264,0.0605905906837094,0.06059233315304573,0.06059693118337391,0.06060285433080391,0.060638467126940696,0.060643295782424354,0.06066649827102044,0.06083516632082101,0.060847094935014744,0.06086179111718036,0.060941781235663975,0.06094970575509788,0.060960310706579215,0.06100767998490908,0.061012754925153054,0.06106807690912434,0.06109167089828206,0.06111555346862595,0.06111802475959686,0.061119212738302585,0.06113296913345777,0.06115784624929281,0.06126597345917732,0.06127598526575122,0.06128827741100682,0.0612997118146207,0.06132568285082211,0.06134141102961401,0.06134248465132056,0.06136076931550816,0.061366958217562294,0.061402586748542855,0.061455415338512134,0.06148438099411777,0.06151253652813684,0.06154763436613716,0.061561391528409636,0.061563197743012776,0.0615845538337472,0.061640658708936015,0.061648379090858366,0.06165543897224283,0.061671760082137636,0.06170584826485916,0.061731696312536374,0.06176240641720517,0.061764994787010294,0.06178407673059374,0.06178693128604923,0.061794173874473265,0.061809796820302666,0.06182524343571971,0.06183380064840155,0.06195700097726635,0.06196639834921297,0.06201128311604022,0.06201221249415741,0.06204787042295736,0.062049592384599006,0.062073754310924656,0.06220108521309026,0.06222221707993599,0.06227110250654673,0.062330406506490035,0.06236855661185078,0.062389318253239644,0.062421586326078846,0.06248628381524164,0.06255801249108647,0.06257300373557192,0.06257812634595361,0.06259368565578806,0.06264205268757478,0.06273607149966268,0.06275361316541953,0.06278532336859093,0.06279168429166239,0.06284460583877956,0.0628759876029203,0.06287623794425257,0.06288240119752009,0.06290199495107446,0.06291505037291474,0.0629578911190976,0.06303651856044265,0.06314603831855448,0.06316495550748812,0.06320517587951321,0.06323892727197249,0.06324400068848907,0.06325862673664381,0.06328084147262622,0.06328339020156065,0.0633420108529136,0.06338832998375553,0.06343068183626045,0.06345061801838356,0.06349006538882243,0.06350994939890695,0.06355637957304876,0.06362847131549552,0.06367738360974888,0.06375200519528683,0.06392150807809464,0.06394018695215582,0.06395481364326283,0.06395751640570249,0.06399421041235635,0.06403081393872692,0.06408808487027276,0.06409606758923879,0.0640994939355771,0.06412197407238537,0.06413486506064352,0.06419115813357187,0.06430726694160668,0.0643354054623392,0.0644070172067011,0.06444704880804154,0.06446036954866798,0.06458196422392537,0.06462182431238932,0.06462606562567255,0.0646336562355933,0.06465147311845514,0.06470476124018071,0.06472705199940182,0.06481062871531447,0.06487639307827944,0.06492336937754194,0.06496818044821857,0.06513628167750851,0.06518202150649734,0.06522795963020567,0.06523171838062228,0.06536322422640085,0.06555684956635804,0.06556387821555487,0.06565472358266805,0.06567218084751095,0.06569982581802934,0.0657290254933143,0.0658336626935761,0.06584378740891136,0.06592514717089593,0.06597872426411777,0.06604394062768738,0.06604589835134485,0.06606864814733791,0.06612911255171929,0.06613832187335907,0.0661796502870195,0.06625056935563636,0.06625855042517509,0.06637386708462527,0.06646868246863834,0.06667224987680123,0.06667985863035078,0.06674037347659893,0.06678536908130474,0.0668183925818967,0.06689927751798404,0.06698962075192814,0.06706512627255344,0.06709533608729529,0.06714981776603025,0.06729394872363104,0.06767909917705324,0.06767922994260285,0.06773253440203204,0.06781308723897646,0.06783914032837782,0.06795452955064826,0.06796885563035446,0.06801657017650507,0.06809766894783818,0.06815295947918479,0.06838800622084378,0.06842539912108656,0.06843236897534122,0.06846738822477316,0.06849930516655314,0.06853550727177259,0.06855711080557593,0.06862238914333553,0.06892846696876022,0.0689561501857811,0.069114338605126,0.06916866714489782,0.06919477451981174,0.06932651041659321,0.06944533589452642,0.0695960734589963,0.0697169702902597,0.0697450750628846,0.06974869887225105,0.0698397778623828,0.0698539905292855,0.06996722981202841,0.0700988809306394,0.07031618978214614,0.07040144775530495,0.07062468488835548,0.07070320747626319,0.07082190129190732,0.07083366844030359,0.07083772680471259,0.0712036241089988,0.07123462772681985,0.0713016947764851,0.07161676538944045,0.07161760294123054,0.07162262328670896,0.07163228252782677,0.07167603267531727,0.07208618662917957,0.07216454188607721,0.07247936647995154,0.07254016537468407,0.07255806193128203,0.07272698471926418,0.07287479918655723,0.07297684184766731,0.07339449424557583,0.07352194020971334,0.07362773022893378,0.07410905821289848,0.07436293443894337,0.07443322426560237,0.07467553041045208,0.07524314649933125,0.07542685056794458,0.07598219032748668,0.07702245328710366,0.07711340664694201,0.08127273050162992,0.08133637666366818,0.08370974534896311,0.08451388148045469,0.08514001765077486,0.0856682692933181,0.08693321421392664,0.08699072836686549,0.09304824001129151],"xaxis":"x3","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y3","type":"scattergl"},{"hovertemplate":"algo=momp\u003cbr\u003ek=15\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"momp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#00cc96","symbol":"circle"},"mode":"lines","name":"momp","showlegend":false,"x":[0.012365717250504815,0.01245554635136418,0.01348863755907059,0.014800575541808438,0.014998697453453383,0.01514461862954741,0.015743696316431226,0.016265445614213697,0.01641847973118527,0.01645798395733014,0.016501997215859262,0.01653145687027439,0.016671658196617103,0.016740636826426035,0.016813927714902397,0.01685554509529779,0.01685568664461937,0.017039994965312437,0.01706162964468607,0.017352558891871723,0.01736567653541653,0.017423542394532136,0.01744077551879004,0.01744362022316241,0.017710767774979087,0.01779949416815705,0.01792893549529187,0.01806553129530913,0.018137990292492443,0.018193440359880762,0.018231008679928214,0.018466114934830517,0.018484142506555196,0.018804935664076643,0.01881885170008578,0.0189435734736404,0.018944631881623757,0.018963235094353523,0.019096157864557123,0.019096458762841446,0.019232766672380674,0.019242034979889303,0.019264606801214328,0.019273567220843064,0.019282481518754137,0.0193072426350212,0.019350170772368842,0.01947126514582414,0.019584260789780022,0.019611511970687643,0.019718383829215144,0.019719045953762546,0.019751237397676796,0.019760509566222544,0.019772045785992266,0.019775260149962386,0.019782463759919853,0.019920641882864194,0.019941536850956544,0.019960951164381062,0.019968650799169756,0.01996945478450361,0.019984224264208768,0.019989634877912606,0.01999562076748369,0.02003223168178688,0.020059702997992854,0.02006723522435241,0.020103448870806034,0.020151389764798213,0.02016232420387559,0.020197222539856244,0.020207898951777693,0.020220083666012526,0.020226554384504353,0.0202689674278494,0.020272531948558298,0.020286256751310558,0.02046535110708408,0.020483605132034757,0.020504334146583463,0.02053286179068759,0.020547627068284606,0.020561232918715467,0.020671351574044403,0.0206901741126257,0.0207119720936664,0.020782255274026606,0.020799360861197852,0.02082318099622434,0.020943044272922602,0.020966662342424676,0.02102040894086595,0.021023964043809917,0.02110744857115095,0.021113870563584124,0.021116515810987148,0.021122165463849985,0.02115322319089202,0.02116688387413356,0.021175480658915842,0.021193336226466537,0.02119748586972427,0.02128774150087736,0.021288992127276286,0.02130605108201033,0.021310770101856443,0.02133434793757735,0.021335997831690334,0.021349771243175913,0.02138849343651039,0.021404114674579665,0.021431993646654917,0.02148176504100972,0.0214911456903518,0.021516028610491632,0.02157250942152911,0.021579827931324836,0.02160516891529487,0.021641106390705787,0.021663198749902746,0.021712357941616016,0.021719850432495504,0.02173132512408553,0.021757559451020543,0.02179182569019345,0.021800822076108707,0.021828602583619752,0.02188528060210045,0.021916693338262894,0.02200401912142327,0.02203778973150804,0.022061484584489425,0.022070986384770327,0.02207431695295701,0.022081254701293098,0.022111064928559444,0.0221230814433639,0.02213612922349139,0.022160155794757504,0.022230197511937166,0.02227689607115987,0.022311112337607842,0.022341121201686916,0.02236198168925704,0.02236363734552587,0.022373191139347208,0.02242945799734462,0.022456611978196653,0.022474888220427446,0.022489771490721077,0.02248996737898251,0.02254056616779762,0.02254783117685672,0.02260227421897848,0.022615279499680726,0.022615644642285784,0.02262885484022139,0.02269572793611131,0.022698656546638003,0.022705090136031066,0.022715274866828263,0.02272899251473818,0.02277393310134796,0.022795652940595077,0.022796914675054437,0.022800834494640838,0.022808650153942216,0.022816559780688906,0.022903787467554927,0.022903802937163945,0.022911821377738527,0.022997338695416557,0.023040508799580164,0.023058484921090024,0.02309682658044174,0.023129170096502054,0.023168237279488017,0.02318990317364222,0.023197819628708143,0.023232425294677072,0.023250834257050337,0.023272717703817527,0.02330006334228968,0.02330482036990248,0.023334955324907602,0.023341073928460694,0.023363391922008203,0.023381578381783483,0.023416895595592048,0.023426920630376512,0.023483856057535474,0.02353528246817555,0.023537749404097807,0.023546360450201663,0.023569794293384565,0.023578369215931712,0.023579693178472187,0.023601387736388944,0.023607348942061807,0.02360928308976227,0.0236252557870828,0.02362652986858605,0.02363209572399971,0.023632361918889615,0.023662706188919174,0.023672466830715146,0.02375444946325518,0.02376948258971397,0.023772751073646237,0.023776426830360343,0.02381286204692907,0.02385814440891059,0.02388323044115524,0.023887221651954012,0.023972198031896784,0.023993292974173713,0.024010867313869887,0.02403900322646297,0.024056841538920355,0.024113604783565508,0.02412470062223191,0.024128833785623364,0.02413401215759773,0.02420833845147321,0.024215212039027177,0.024240891730512103,0.02424796734253828,0.024248243869454095,0.02427518043794052,0.024285290793163366,0.024286412203255506,0.024311252587054126,0.024341783351135578,0.024386311692854556,0.024403682307762874,0.024421399701336024,0.0244346413619611,0.02446929439501715,0.02449897648933557,0.024503271483578507,0.02450772863276047,0.024514053880380086,0.02456729766650978,0.024591874680412493,0.024600777361738886,0.024601994994998246,0.024622754143307828,0.02465801992899109,0.024674805355498916,0.02469494106731745,0.0247004140149287,0.02474389741423932,0.024756428047989203,0.024756712753548064,0.02476799902766927,0.024773053268154842,0.024795326856173347,0.02480999693283424,0.024843596098920884,0.02484538157698941,0.02485608832535096,0.024868674408627403,0.024897445537543174,0.02490222441773541,0.024927546487452663,0.024939339396202125,0.024968119374166993,0.024978050101659943,0.025032695031798176,0.025050493310733953,0.02506913346953829,0.025083610957897552,0.025095397859532464,0.025128484135791128,0.025172479324142405,0.025206235949970752,0.025261539251074987,0.025284147856278,0.025293988961140868,0.025331400712168755,0.025395930893961843,0.025407326087083126,0.025450493963363314,0.025491778415988726,0.025499388882312412,0.025524792485593206,0.025536048425044023,0.02553829189205074,0.02560445012322546,0.02566810886454759,0.02567104832091289,0.02568245535668674,0.025694785539564176,0.02569738667876554,0.025706441535843588,0.025731304750177635,0.02574492384242417,0.025752108630967274,0.025766450400141673,0.02577516210940123,0.02581482745092334,0.02581950880055154,0.02582132548300097,0.025846462790084478,0.0258517884166792,0.025860964661583823,0.02588917327666864,0.025890566730792014,0.02589490891361665,0.025896244003012762,0.02591469333600969,0.025921142795477508,0.025941388933637028,0.025943307493356056,0.025944993302296125,0.025947175424378178,0.02595236880777239,0.02598117885574679,0.025984677270620926,0.025998814805556703,0.02599938673534376,0.025999563355100647,0.026013202492531223,0.026016990524779203,0.026032662150476265,0.02604453189304128,0.026049836365440542,0.026064574851077972,0.026080235924550076,0.026080563068035308,0.02608501319966705,0.02609091334150505,0.026106489090015284,0.026111550798743853,0.026127597610344035,0.026139492655283203,0.026154385823878835,0.026192559077265858,0.026212685223796446,0.026249726382387883,0.026252896138300442,0.02626133368676729,0.026273008256636664,0.026282326274574698,0.02629691118321297,0.026297746553756073,0.02629940414980625,0.026307941618322384,0.026334943185945987,0.026348899820441935,0.026352731279793783,0.026357637539122125,0.026359752634771205,0.02638518902271763,0.026402466600285973,0.026418591381241686,0.02646392834219488,0.026466765483455678,0.026475872203513034,0.026485149727358125,0.026499668968441602,0.02650060512044049,0.026523506247706163,0.026527624733744405,0.02653288715153816,0.02659265896243813,0.0266536100710399,0.026671900971613965,0.0266736971310711,0.026675217386675738,0.026683862426619596,0.026687278893487244,0.026694400847778486,0.02673501048194061,0.026780605923064305,0.026793246222788364,0.02681107203822969,0.026829219408593508,0.02685116699676508,0.026877994221970486,0.026890373065647954,0.02689271934213608,0.02691637086133671,0.026918479755717466,0.026925513016484826,0.026932708221951013,0.026945051842761553,0.026953901983203972,0.027012744352208556,0.0271237267239993,0.027127858212082256,0.027129870133471606,0.027167208410146852,0.027180872962216295,0.02719436414198283,0.027230962254780406,0.027241231169080515,0.02726089215464076,0.027301111100504837,0.027311082367812305,0.027334045192566426,0.027349822022375597,0.027352395938682484,0.027362198052139592,0.027453062983863942,0.027469277238679514,0.02750048570949761,0.027538743061919495,0.027575447413478494,0.027577589774053512,0.027599779891726635,0.027634531782448724,0.02765235367404198,0.027656843979856967,0.02766504090860892,0.027667745768619856,0.02768720334142692,0.02770111106940758,0.027705608242258976,0.027714550018051648,0.02780143033225645,0.02782572149978747,0.027847399845920412,0.0278496411124974,0.027862102959410634,0.02787099211277084,0.027875899764144296,0.02790674958242335,0.027939965137270033,0.027942085853259597,0.02797523080958473,0.027988241757064342,0.027988849249275267,0.02800091299229015,0.02801408958246395,0.028015611277269044,0.028024158963587176,0.02803168101612777,0.0280363460782032,0.028044020585656658,0.02806449106027887,0.028076377348727522,0.028095484122814245,0.02809701704867623,0.02809959034420121,0.028106597054379463,0.02811706473213082,0.028128362568227972,0.028135576732374407,0.02815320825731401,0.028158413534624848,0.028164009641167033,0.028184068073926944,0.028185205166050425,0.028206139324154618,0.028272171183671518,0.028287574436045255,0.028292442775225024,0.028295298367579897,0.028313080057098355,0.028331553826545876,0.0283343494765596,0.02837999848260007,0.02838389871693503,0.028443248785609478,0.028466958556809162,0.02849949497075758,0.02850921954658662,0.0285176614870571,0.028525230389622645,0.028546516820268274,0.028557906355338974,0.028563716783273456,0.028584377266246585,0.028590543126134384,0.028612778305549237,0.028614434135141566,0.02861675559727902,0.02864983784317158,0.028651331900518245,0.028653847174668578,0.028676610435518262,0.02870826477849369,0.028714917743286515,0.028718437651407185,0.028724616047952234,0.02872477369851207,0.028747903040250912,0.028749877933677496,0.02875278488246305,0.02877128512802951,0.02877358286457123,0.028794363387249916,0.02880091742498591,0.028804473103010986,0.028842447595215343,0.02886398158319418,0.02886661962368393,0.028873531095376666,0.028874955931538097,0.0289135313583792,0.028986748506990057,0.02902038654643625,0.02902545619230652,0.029028163678798263,0.029034024690834555,0.029040721175530117,0.029053508892445824,0.029058886884557066,0.029059688271667578,0.029066946119886368,0.029093704995621035,0.029142276747199474,0.029151043579146767,0.029152449447517947,0.02915796502477941,0.029164833476452,0.02916853461650601,0.029169177596697116,0.02918347521196412,0.029186646228695427,0.029197104119618088,0.029201712571244775,0.029205245476224367,0.029222066100158352,0.0292447800187972,0.02927547849799537,0.029325026597260405,0.029333131489351315,0.029335519484031864,0.029344441695559432,0.029398790314053097,0.029408620595513423,0.02941494975875738,0.029432218371793543,0.029446939184482872,0.029462134826206205,0.029465475620532127,0.029484964274063986,0.029487498508200972,0.02948991048357704,0.02949260921880245,0.029509471450756542,0.029521229323762978,0.02953130022928671,0.02954727484406052,0.029551911868511655,0.029556981209523013,0.0295652209548652,0.02959343988179383,0.029672174716234533,0.02968237031760205,0.029701435012949216,0.02970312000158038,0.029717502917946486,0.029720299783984584,0.0297203731817822,0.029721833751427718,0.02975770476272911,0.029762471350598108,0.02976266871573115,0.029768495618112335,0.029771691662323983,0.029809067379181615,0.029814688948600777,0.029817012846570245,0.029834587652884693,0.029881573647024595,0.029903136777600932,0.029910727193565752,0.029912590681935157,0.029935681520807677,0.029976510220314688,0.029985154675857034,0.030045198597414002,0.030101118282129403,0.03012019545652179,0.030121163558662577,0.03012508615523084,0.030138240871961097,0.030146512269879244,0.03015509001169231,0.030155702482751612,0.030156165670918583,0.03018495433021371,0.03019312374650948,0.03019347245538846,0.03019577454118168,0.03020325304793982,0.030209358580833075,0.0302233368266919,0.030224267283381095,0.03022549043325847,0.03025624108842407,0.03027734154786214,0.030278150203909098,0.0302955188855962,0.030300454067547417,0.030328916460160327,0.030356722612664516,0.030358970447487615,0.03036647888402691,0.03039593900907688,0.030412650699596637,0.030417857196540554,0.030452183754553042,0.030470466724024446,0.030500102567219228,0.0305323296444955,0.030549384719078525,0.030608270410293664,0.030609127839230765,0.030613221862516353,0.030622905947140705,0.030681059558862155,0.030691188831715767,0.030733682909991306,0.030742554106990082,0.03075619446900269,0.030775023248662776,0.03079823832612182,0.03081654271439683,0.03082268953003094,0.030831574093686438,0.030840882659976903,0.030882206137029543,0.03088981523435803,0.030915738389820303,0.030979594771714476,0.030981773526785477,0.031012857463932352,0.0310412572550116,0.03107282490997983,0.0311221681194593,0.031133766435091137,0.031187320732176803,0.03120898828788705,0.031218666701011987,0.031219312999346355,0.031245438065413043,0.031261568790042896,0.03127154967458941,0.03128381933336303,0.031297945491359905,0.03132666171666069,0.03134321408618091,0.0313663961826991,0.031405293146268005,0.03142385554681808,0.03149862372891188,0.031525413244136595,0.03154329288294715,0.03154572806209702,0.031557059073273774,0.03156548096842307,0.031569032419783115,0.0315804016509549,0.03158680311048167,0.03159122761188891,0.031601487148628066,0.031602569210676605,0.03161164696918467,0.03163659997109828,0.031646639961319216,0.03168970017270975,0.03170111184383497,0.03173079671653893,0.03174899385294123,0.031765893703286384,0.03177321832004493,0.03192455582404325,0.03193416350765667,0.031935188393279655,0.031962064238349144,0.0319626147439046,0.031972491241979636,0.0320701273156483,0.032121871319778006,0.032124528265172946,0.03216505880221979,0.03217218215203281,0.03218724301019691,0.032263150111755026,0.03227040847856648,0.032288818713272674,0.03230523745596582,0.03234129134153664,0.03236046516455236,0.03236365379691241,0.03238774250420191,0.032389950713143184,0.032395405147379736,0.032417542633509655,0.03242275814349459,0.03244448150547876,0.03244840633764619,0.032480009962680564,0.0325254789713376,0.0325532557647679,0.03255329836793173,0.032557348251683486,0.03258599236429073,0.03258834142604993,0.03260910709286573,0.03262665857324863,0.032643487959957584,0.03264611225935794,0.03268605888818064,0.03270838591841805,0.032759690099720976,0.03277793287274518,0.03278962372812529,0.032808379045286044,0.03281079255951015,0.032818521807748004,0.03284725912257521,0.0328739708545709,0.032882122085052444,0.03289064495261654,0.03289341616649387,0.03292559265245432,0.03293937811027499,0.03294302964624317,0.033000805776926115,0.03300420380924302,0.033012747532046,0.033018517792524724,0.03304863884815394,0.03305167300711222,0.03310891097773798,0.03311289613305826,0.03311508626882604,0.033141409980175104,0.03314193272283906,0.03314909143083099,0.0331792043606691,0.03318306725858137,0.03319327148882555,0.03319486776281862,0.033196742871752324,0.03322078830670376,0.03325932154081773,0.03327503291357662,0.033280330319625934,0.033318092980529165,0.03334278244971946,0.03334981299465244,0.03335813621972679,0.03341743393593452,0.03344009033674787,0.033449009970274134,0.033457717419695235,0.03347496034222769,0.03349289112900095,0.03356228027009678,0.033604852350489815,0.03365832145812987,0.03365923737355393,0.03368735545888192,0.033719229195574345,0.03372493828827404,0.03378149288107182,0.033788453575534054,0.033853034265897516,0.03386507602644092,0.03389770836523606,0.03391720779213302,0.0339489638088667,0.03396926738243171,0.033988503371948385,0.03403458025972089,0.03405988569701345,0.034089905261622505,0.03409866937824806,0.034107550244579524,0.034114900635860396,0.03423995450061348,0.03424779925493748,0.03426207764849195,0.03427386660270691,0.03437025915957633,0.03438091129579724,0.03442645983360777,0.034434134047939144,0.03443559282445534,0.03450171446472218,0.03450327499345029,0.03450883497173615,0.03453505299503448,0.03464719208506879,0.03472337046257447,0.03472454536858323,0.034739477552102355,0.03477930600965005,0.03480200389949085,0.03482671710684791,0.034827678468700274,0.03485541395843107,0.034891389050751356,0.03489618327207753,0.03494447028150429,0.034954282585268136,0.03496726512528807,0.03498288088068639,0.0349936185463044,0.03499497868577069,0.03501709737834592,0.035018473703028484,0.035032698939080016,0.035066599466223765,0.03509007605547447,0.03509378048782285,0.035117786292911436,0.03514968953091916,0.03515057150064712,0.03517042721377514,0.03535010983893916,0.03535349403056095,0.03535527601220043,0.03536587099639134,0.03540701573853289,0.03541309601187092,0.03552168515415954,0.035531500589492754,0.03556127082180533,0.03559854586029697,0.03561248244323857,0.0356593785493495,0.035734844269172,0.035742798959098286,0.03583543258703822,0.03585671366514859,0.03586859530216659,0.035974178022910516,0.03598596453520564,0.03599552226823094,0.03599981302732204,0.036042710760625846,0.036118370720603955,0.03614643043590021,0.03616077959688686,0.036165189935618326,0.03617806214682991,0.036178770468821134,0.03618406096058471,0.036190818685509246,0.036234344912757635,0.036262296139675174,0.03628592946993488,0.03641174792146692,0.03651315084788528,0.036609149273618594,0.036625655576887156,0.036632454499626564,0.03663830381535543,0.036721028987601656,0.03676472601733867,0.03678609802332289,0.03678910792925386,0.03679282064526972,0.03679449979256142,0.036890743988232104,0.036897116813033204,0.036918287449010906,0.036938099105822915,0.03694229906457204,0.03696289684670666,0.0369954335381214,0.03700770323459256,0.037009522778719814,0.03701784904362487,0.03702251652047575,0.03702714544542933,0.037091458926380876,0.03709659093302981,0.037124355371202636,0.0372355641073398,0.03729574456633336,0.037303946559510234,0.03730793629262087,0.03735464502742491,0.0373807699837307,0.037428883743385474,0.037473612069805956,0.037479902658977354,0.037495820405501076,0.03750103095701676,0.037544246782195596,0.037573725013696,0.03758715575225753,0.03759943040866751,0.03760929539668275,0.037613360644217095,0.03764087637640906,0.03764825575414445,0.03765051005541789,0.037682800204820406,0.037692799909884306,0.03774333325396202,0.03774481225891548,0.03775749693880229,0.03776492717401261,0.03776607548979144,0.037848525202668126,0.03793061637988464,0.03797098154270408,0.03805482920807413,0.03808210740154246,0.03812988933746271,0.03815834833559624,0.03816726004760797,0.038176189884596826,0.03820530931422715,0.03821541808196926,0.03822417628832901,0.03831688009897697,0.03840287804219999,0.038419386742233635,0.03844449860033863,0.03855480154167595,0.038618765935806994,0.038727413977183524,0.038742414484930994,0.038755836732416844,0.038796920924766214,0.03885500333782459,0.03890982888139646,0.03892622281999507,0.0389937332481649,0.039025123031350285,0.039031439876273465,0.03907504495584236,0.039124929330361276,0.03923386190453915,0.03928197277292041,0.03929887687408387,0.03939906528469764,0.03945269212846409,0.03946968656060508,0.03952154082751648,0.03960084747601031,0.03961559271485155,0.03972180766448978,0.03975653280823764,0.03985100908930249,0.03985743272987558,0.03993570320995029,0.04004153206170386,0.04011535281393319,0.040126798206892404,0.04020534522125097,0.04021474830019944,0.040225962554452,0.04026984915847696,0.040361639102629014,0.040419676024461296,0.04063121026600015,0.040719251492605156,0.04072627228897922,0.040731383539914,0.040799967786683766,0.04091052069752653,0.040932633498028725,0.040939699676327566,0.04099267393099686,0.04109322438913433,0.04114481107526121,0.04130094078153414,0.041375117507965166,0.04148712558908013,0.04170694711531439,0.041791232919795654,0.04193518484855768,0.04216565444775936,0.04270648931287815,0.04275568597191927,0.04279965294168496,0.042897413079293505,0.04316143837849052,0.04331195078698353,0.043473896507717553,0.043855556490921835,0.04390553395934134,0.04399949232592475,0.044868174606462925,0.04495013450312611,0.04545473620094101,0.045489495018766556,0.04552205492333014,0.04559368196880463,0.04615379419970236,0.04637491009178632,0.04649739096057384,0.04678685806618374,0.0479171320362177,0.04819598064892972,0.04858375103239439,0.04897051385506744,0.0497645838068107],"xaxis":"x2","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y2","type":"scattergl"},{"hovertemplate":"algo=momp\u003cbr\u003ek=20\u003cbr\u003ermse=%{x}\u003cbr\u003eprobability=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"momp","line":{"dash":"solid","shape":"hv"},"marker":{"color":"#00cc96","symbol":"circle"},"mode":"lines","name":"momp","showlegend":false,"x":[0.002375700800461464,0.003105917293978041,0.003183758726880307,0.0033439759408725256,0.0033641026991720934,0.00336648264772891,0.0038168346166613638,0.003858122579795084,0.0038720512431237564,0.0039011285323218314,0.003929990931849065,0.003943090498794323,0.003961159506648934,0.003992344468864627,0.004045519876567988,0.004050502183046201,0.004061293770295637,0.004091132971142513,0.004139588632168415,0.0041665801467009825,0.004177305778961402,0.00420626160228202,0.004252834885704514,0.004301078197871424,0.004341021837849188,0.0043494407320397365,0.004356722470020173,0.004363359021595429,0.004382675410062297,0.004400788081593632,0.00446929054194433,0.00448702701311258,0.004494262178829497,0.004497016434563147,0.004528317989069277,0.0045413045732569405,0.004604028677380743,0.004606175977688474,0.004615446885221839,0.004629075534616295,0.004686211587290138,0.004706295526463846,0.004719158290648442,0.004726671510288712,0.004730848148832467,0.004799953410937604,0.004804531650257174,0.00481629267820438,0.004866920186178064,0.0048900618949906174,0.004891527954781247,0.004913760837126238,0.004922341739967905,0.004982487321237967,0.004992281231675235,0.004995993082106524,0.0049991461456143065,0.005007777927420889,0.005026849665786816,0.005034594556477966,0.005048749079056155,0.005074349320011646,0.005102809446158764,0.005173704093827634,0.005185038720646003,0.0052144279834995194,0.005216974174777219,0.005267720286366964,0.005269164099023551,0.00528084434493105,0.005282091215432917,0.005298565086270672,0.005308997590436591,0.005332625335078137,0.0053329355062227595,0.005336565704082415,0.005344904497786608,0.0053697865000560605,0.005370391834062073,0.005397282446286587,0.005416756204245137,0.005422311075677247,0.00542317831788477,0.00542899752586474,0.005434741005184841,0.005459627588300789,0.005491909676654321,0.005492806854863201,0.0055036903889679216,0.005523667932633799,0.005529696167662522,0.005550646914438186,0.005565596448511409,0.005586084073806359,0.005596551348060968,0.0055997206860526096,0.005634726267646461,0.005636409594661008,0.005654156665923046,0.00568054363181906,0.005697885718141796,0.005705195636757062,0.005711936126651458,0.005727627980348876,0.005733901911922406,0.005760045571945443,0.005762156514196872,0.005786185964152155,0.00579695212415117,0.005806508026365535,0.005807430355979322,0.005812482832157766,0.005814977353935167,0.0058211460557746255,0.005823531001354204,0.0058242541669458695,0.0058339732236177885,0.005846701156163326,0.005856559399790571,0.005874718317389182,0.005886883784428157,0.005891300096300031,0.005938942030782859,0.005956195695472983,0.005966420338447092,0.005968153010528887,0.00597157129801005,0.005982288948079298,0.006002053004331573,0.006012254957564206,0.006042470886590245,0.006045358781125103,0.006052525880859231,0.006053972016660191,0.006064328371991007,0.0060669547485255925,0.00606917956508686,0.006088164929290853,0.0060991628351901315,0.006134080973741419,0.00614853081292372,0.006161546769933207,0.006189501070625817,0.0061941455631487835,0.006197626897432766,0.006213105903576089,0.0062334222906094364,0.006236232257253517,0.006241545448495063,0.006249845632367119,0.006273471890213396,0.006281825934332697,0.006282529599290992,0.006299918553734511,0.006301551579940673,0.006321542176382593,0.006325410050779192,0.006328822685674802,0.006343126725226427,0.0063457434305293796,0.006355500582212655,0.006356545621450949,0.006395393180579183,0.006402964481178298,0.00641241627032278,0.00641286180122502,0.006416301938813413,0.006425250294467789,0.0064751116987115526,0.006482612751031723,0.0064871937788170825,0.006499435478458505,0.006512117413662796,0.006521261970877381,0.006529180642112231,0.006534484288470176,0.006559063918008319,0.006572083429715677,0.006577882596012101,0.006583039951231202,0.006595151979171779,0.006601592022034546,0.006609598599817482,0.0066138050930035375,0.00661657187066083,0.006648217815421777,0.006649870410298646,0.006667780648703754,0.006668016240860883,0.006677637795645468,0.00668765435452927,0.006701806242738993,0.006709459948193155,0.006710889727675339,0.006745998865531772,0.006746426251918437,0.0067608528711737585,0.006762521652667881,0.006781338706361099,0.00678975425873111,0.0067990387881824875,0.0068029601917191694,0.006823110021956293,0.00682675784561039,0.00683054214488171,0.0068433151340381694,0.006846524232388693,0.006861387878691001,0.0068764059564153076,0.006882116381627149,0.006903067620828786,0.0069576517839471125,0.006967226386180317,0.006977458141432401,0.0069855610182557575,0.006994086884660579,0.006997309362259095,0.007003294957422493,0.0070048394850420834,0.007007040930228403,0.0070095242106658,0.007014587523162727,0.007015011315931997,0.0070189312961789225,0.007021013169985646,0.007023414459999919,0.007033604084736829,0.00706169754171246,0.007091067418932463,0.00709740124371115,0.007097573170594074,0.007098393319622379,0.0071079512887599824,0.007112496638945374,0.007119224333833815,0.007130040324633151,0.007135108693239092,0.007139738066820248,0.007147206737521773,0.007158837765362204,0.007173713412368976,0.007175422027919785,0.0071870499475753455,0.0071918564092837145,0.007201630560284085,0.007202655515161605,0.007210045769286128,0.007221748895571351,0.007236022548616812,0.007239677393578198,0.00725210221372668,0.007262590731298357,0.007287976463423373,0.007302265762517368,0.007314947302603052,0.007315629410546809,0.0073170786953973554,0.007317211624371199,0.0073294728698701074,0.007340880849274282,0.0073428027845295245,0.007350791351042463,0.007360058570233712,0.007360545618664642,0.007382196757706358,0.007387759545056745,0.007400834549452269,0.007404877736608544,0.007408065895842625,0.0074121190878124995,0.007413713903616897,0.007416289508123281,0.007435142123390248,0.007437113331105386,0.007444799241865863,0.007446922620938286,0.007463912048607771,0.007469051031446813,0.007469082351800192,0.007471588892560385,0.00747192565238942,0.007489388886269955,0.0074939075040987135,0.007503051286896413,0.007504228816445512,0.007506311202622204,0.007513271664582416,0.007513960605689823,0.0075154556789764,0.007524282520475144,0.007525364439494515,0.007530086149311109,0.007545986089817923,0.007547189700369647,0.0075490520598335015,0.007549511746185072,0.007555476453173282,0.0075751696760075985,0.007580605318513385,0.007605443593214717,0.007607144447802023,0.007644363403259402,0.0076504961996708655,0.007672151811771974,0.0076732387910450174,0.007675787484948927,0.007679184780934005,0.007693962953741439,0.007701575599351461,0.007701957716402358,0.007707790203672805,0.0077084976773646174,0.007714666597612325,0.007740883499946864,0.007742565781284997,0.007749205042639711,0.007754128878313715,0.00776137937502628,0.007766396834431595,0.007799659454044171,0.00780383146727332,0.007805359147974871,0.00781275531203981,0.00783376945726175,0.007843532361999821,0.007853066172006092,0.007854961782098636,0.007869015257965798,0.007871447418262598,0.007876251004044448,0.007877255391650382,0.007877471209172607,0.007883359850126209,0.007888025575779088,0.007897747526771415,0.007900023491049108,0.007902476966971548,0.00790777108761877,0.007909233633827931,0.007915193781039427,0.007921399826408238,0.007921893311537222,0.007927356410568114,0.007934108889666445,0.007941341208735863,0.007962020316186674,0.007965368595037008,0.007973991631874174,0.00798622014352539,0.007988275312616233,0.008023896216477302,0.00802809434838558,0.008029447387754664,0.008046848698650296,0.008055393805571482,0.008056788841135422,0.008085493457274636,0.008086664224077118,0.008091916525418786,0.008095497511502393,0.008111364725428106,0.008118722232851617,0.008119496223230781,0.008120576682949597,0.008153783762945942,0.008169285277860648,0.008179013545319132,0.00819970155944964,0.008199988523577815,0.008201048585727377,0.008217487342827423,0.0082398602441215,0.00825003475578033,0.00826099813828256,0.008275390114883438,0.008283113143742301,0.008291683757557752,0.008295043762617233,0.008298149031401239,0.008309597770061215,0.008311296897519777,0.008319648784407635,0.008321285683862803,0.00832163544082887,0.008323002760074187,0.008325656764409854,0.00834721995395179,0.008347224615205406,0.00835612054622798,0.008359043814989763,0.008366545691316715,0.008390702232055064,0.008402725536382893,0.008403899891302271,0.008405348442073138,0.008418074048408926,0.008425292427678959,0.008426009543690806,0.008431112882604752,0.008443833049067578,0.008462210515219526,0.008472177897822326,0.008475094576010115,0.008475386629506618,0.008476147575435992,0.008482042143824638,0.008484740409868621,0.008489016627577606,0.00849100956858099,0.008492315424996367,0.008498039337847308,0.00849851216531098,0.008502238463599376,0.008508712840872505,0.008526725199019468,0.008528867683408489,0.008533686035190603,0.00854018540428164,0.008545953755066662,0.008566150206440052,0.00857785979233926,0.00857959236371909,0.00859102052810058,0.00859199603323851,0.008598864798603091,0.00860285177323363,0.008617224841116227,0.008618474878557282,0.008618874902461906,0.008622741249059611,0.008632259200292283,0.008643315414705127,0.008644949240659549,0.008645290812661948,0.008646780768301634,0.008650750215955688,0.008659356551199095,0.0086608669970444,0.008664437471337586,0.008684752948430303,0.008687027694558506,0.008698715600411442,0.008700264331578398,0.008711203620374957,0.008718428310532527,0.008724627628285641,0.008732519081741181,0.008735814556363087,0.00875493218651894,0.008756251111445287,0.008767386138278325,0.008770992389961086,0.0087982880328202,0.008799103901055719,0.00880537351171043,0.008817402465740849,0.00881764387019226,0.008826130172263899,0.008830518826021365,0.0088449225486758,0.008848794586569732,0.008862463366200968,0.008865857446793503,0.008875282481024446,0.008875957805399978,0.00888160531180109,0.008898898818799354,0.008909909456375681,0.008911920563104074,0.008912302054827514,0.008916139255401346,0.008917189643209828,0.008922422071420334,0.008928264436000396,0.008933085114296156,0.008941749654303584,0.0089482753347983,0.008960193370880703,0.008963140198538291,0.008972278500033168,0.008975061622813082,0.008976704896904839,0.008977719137857654,0.008980796537516374,0.008985384402742328,0.008986025475045479,0.00898770738314445,0.008988067264657836,0.00899795377883589,0.009005845197537247,0.009016254037643986,0.009018270789172955,0.009020077966390103,0.009021304384495294,0.009023189914745485,0.009034118641409582,0.009035170181714649,0.009045598181049046,0.009055642388307614,0.009067535164961948,0.009083407638365699,0.009101260274279682,0.009101397396174507,0.009111417624932217,0.009117959708713562,0.009124750238732913,0.009131032455609204,0.009136400951521382,0.009139189873388995,0.009139450951947434,0.009140598186662659,0.009152048926421245,0.00915587760542411,0.009177538923384996,0.009188797062941978,0.00919529645074352,0.009197917782777397,0.009206866383483326,0.009220352252025795,0.009225860205441075,0.009226149951202028,0.009244700791493362,0.00924897477558959,0.00925415214201179,0.0092647831909984,0.009272737617002777,0.009276736385101236,0.00928628261196172,0.009286489066031592,0.009287130515494976,0.009287592187215293,0.009292650245024903,0.009300520717267386,0.009302328679286941,0.009320553537952834,0.009324426418432234,0.009324909313032535,0.009328168562071172,0.009335258319999947,0.009340506984822004,0.009349269366084299,0.009366373288641145,0.009377237002622022,0.009404877454587849,0.009408895391155107,0.009422342132403762,0.009431347769450413,0.009441193071403635,0.00944573023787729,0.009455314959048246,0.009456753723932891,0.009463457516306404,0.00946869508086428,0.009470334097095025,0.009470441375361075,0.009486732116756949,0.009491075131049163,0.009514161006811708,0.009524128061811238,0.00952562809132482,0.009543443470514409,0.009550838678054391,0.0095527577565323,0.009561517682186335,0.009563885632334977,0.009571195733800834,0.009572028063834028,0.009575939290973296,0.00957802204783716,0.009579177496823223,0.009594157746213593,0.009598619199637011,0.009607475797032734,0.009612392817878344,0.00961649429669769,0.009622175912845006,0.009626354906166727,0.009628089914149679,0.009633159286870302,0.00963458191230269,0.009638249733871423,0.009644871148325369,0.00964677006914271,0.009647621480871412,0.00966194570092881,0.009666303404090713,0.009667791376022946,0.009684666094685953,0.009715650532965966,0.00972320670259982,0.009726220049718732,0.009726845045105573,0.009731586193770054,0.009741225730049832,0.00974131746146327,0.009742379383124975,0.009759504486709694,0.009761957622698974,0.009769406663026843,0.009775805001185503,0.009799133410101779,0.00979966651518792,0.009804513548210176,0.00981724937261971,0.009823987408026655,0.0098285721887933,0.009843223599806234,0.009853647520924286,0.009878765807311847,0.009893215104940543,0.009899432526225625,0.009914330971558609,0.009915368832211975,0.009917467972474064,0.009921374236615002,0.009947172806787063,0.00995313558683118,0.009955900805778316,0.009963716008270182,0.00996406007398474,0.009972217233911318,0.009973126958756321,0.009980676163758636,0.0099918732941463,0.010017659287051811,0.010019592650008471,0.010020553936585568,0.010023870832409723,0.010037405967631984,0.010048232945524694,0.010084250513113272,0.010108736458736747,0.01010890629342066,0.010114863084872337,0.010118694415149903,0.010122040731183465,0.010135036819476872,0.010146039978401581,0.010155043151691318,0.010156492761473087,0.010169704393929394,0.010171572305566295,0.010174314109938161,0.010176075773989907,0.010183545557406378,0.010184102052074585,0.010192644548436105,0.010198710157262925,0.010200331664690924,0.01020366898513693,0.010228203814022932,0.010228384212809371,0.010231189657088297,0.010232211385913199,0.010243641337256143,0.010252286243642049,0.010256932120623206,0.01027948610642848,0.010280759190939914,0.010289583676302706,0.010309773651179178,0.010313255392067523,0.010315848720462926,0.010330393883715412,0.010340832133044136,0.010368409784217495,0.010369339407661972,0.010379570630520323,0.010381258435018107,0.010410923538622444,0.010413567109843622,0.010413582501021075,0.010441467497052196,0.010465396784568506,0.010469911233007854,0.010470224542858215,0.010481080672096892,0.010488670031105238,0.01049018339634922,0.010493970821788472,0.010525824894453794,0.010534522852573398,0.010534682051757876,0.010544386929318928,0.010558392178965242,0.010582419004948579,0.010590372238371238,0.010593090254842497,0.010601306222725707,0.010609191284106867,0.010635112325095173,0.010636224740089009,0.01063764881673181,0.010661307632261707,0.010664888935641364,0.010669311006306893,0.010672336528638018,0.010679940534482448,0.01068310462239243,0.010693732394693914,0.01072568304576605,0.010737433273998082,0.010739246682175171,0.0107535756790905,0.010756342473255427,0.010769092043994814,0.010792100655087915,0.010803619015591326,0.010810428646811237,0.010827236175179195,0.01083589776932566,0.010839128496312366,0.010841654739251505,0.010867925080351094,0.010875225754236023,0.010894631914783012,0.010898582262867433,0.010904514611501413,0.010921033770666744,0.010925577495339296,0.010927369094596098,0.010935219954515683,0.010942439068656943,0.010945581563564455,0.01094786253558715,0.01095435683576812,0.01096797414408677,0.010968235757075558,0.010969949183112879,0.010982419842121704,0.010982944039029626,0.010984183119849103,0.010985982534311085,0.010987843022787309,0.010989208862310713,0.010993742877015437,0.011000308241900297,0.011001650545031303,0.011014055128966783,0.011071959486034524,0.01107256438959313,0.011084371101767854,0.011104489293975962,0.011105614079582982,0.011106456013295934,0.01112932071865546,0.011132228391628269,0.011135638746588832,0.011138925235410671,0.011147261852133158,0.011152622316488758,0.011162833703161705,0.011181085065323974,0.011191089578580523,0.011191670624504201,0.011196141075800485,0.011210148693010706,0.011211550800288341,0.011213714863288175,0.01121796481691542,0.011227099069513097,0.011234340112176174,0.011235986334019591,0.011242092224457958,0.011250484480477744,0.011261974376687212,0.011271775936342775,0.011291224880344113,0.011298952913727834,0.011300040727600685,0.01130426356194007,0.011316333425939715,0.011320260674173823,0.011327944148298075,0.011329296756456855,0.011346336291953148,0.011346703519661628,0.011355572465838785,0.011372449365827783,0.011373192403919442,0.011386522156772907,0.011396507328748594,0.011397130164848149,0.011401513734209726,0.011404598398669164,0.011431736825473894,0.011441566210788914,0.011449191509805898,0.011462064277613714,0.011477834726183305,0.011483217828557383,0.011487962865623962,0.011495150935856237,0.011510444101667771,0.01152458653814047,0.011542872903911634,0.011545228557756288,0.011563482975974114,0.01159683184252641,0.011605056073068954,0.01162523275842164,0.011638420544795252,0.011651377202806274,0.011659296640522659,0.011666051814544325,0.011667807045923347,0.011709847341467693,0.011712565093794012,0.011737669974882311,0.01174473521759438,0.01176872608869618,0.011774985882050561,0.011777305571599858,0.011780986558972879,0.011783097160115663,0.011791026496051496,0.0118180019688447,0.01182687239573867,0.011881405221140362,0.011891525672103085,0.011908883814801988,0.011925863957689645,0.011931181021529806,0.011951520199774527,0.011960046774764815,0.011963213114423825,0.011965104110504254,0.011969261021647682,0.011979391364709402,0.011981930117699158,0.01198773680674804,0.012003139759541448,0.012004070805157434,0.012009593915444459,0.012021128624813988,0.012024954243771683,0.012031247989060087,0.01203850799890751,0.01205124605569185,0.012064542954925232,0.01207822595099405,0.012079549918249357,0.012089677047533935,0.01209370581699121,0.012104114093377147,0.012107018976932268,0.012134676576013668,0.012145007214474673,0.012158801152088446,0.01218663059468107,0.012191383858468892,0.012191822679647881,0.012223008813672213,0.01222907085960411,0.012236128291224252,0.012262466888577052,0.012287268328992892,0.012302015357245713,0.012331128783020208,0.01235011215393916,0.012375497975983208,0.012382658301159843,0.012405114818506199,0.012408195665737991,0.012427368551095512,0.01242981489200131,0.012438335351267446,0.012465868010153267,0.012475841080190496,0.012495287699219124,0.012495781657146678,0.012527735405055038,0.01254583271870829,0.012562318138376194,0.012590223417528763,0.01262772035504189,0.012630356943725187,0.012641006137416177,0.012668411067931337,0.012693509210638373,0.012700830333690821,0.01273294730502009,0.012737701294223224,0.012757890109288364,0.012831547209564703,0.012892794798681897,0.012901490888511091,0.01294106341495752,0.01296308601299786,0.01298343812302324,0.013127165524627532,0.013148522036279007,0.013150702827078442,0.013190943788188542,0.013264300859357278,0.013272767611607417,0.01329408589384154,0.013300759893954312,0.013335294609506337,0.013348362467236233,0.013366138213756127,0.01336622804582333,0.01338036835882217,0.013382878325229006,0.013411910123372348,0.013490256872951033,0.013500864260779807,0.01350701324625174,0.01351990906393755,0.013519923372342257,0.013580234468318585,0.0136024955471264,0.013636618041344917,0.013661433744594479,0.013665132723005707,0.013680863811812572,0.013681799311868104,0.013683239781034676,0.013686893132588172,0.013713740329734667,0.013736526652417824,0.013740575901344362,0.0137855671551592,0.01387084698752184,0.01387652900793892,0.013891116377815222,0.01390754382470651,0.013908117204587322,0.013991411961496534,0.0140016122691276,0.014005225713224077,0.014007873868908182,0.01401416239165083,0.014098263225586402,0.014099251492997202,0.014135143256513988,0.014248304061260864,0.014264657962793909,0.014329648954999902,0.014337642365685608,0.014365850674071664,0.0144698796472474,0.014478987161099004,0.014499258521290914,0.014502327985007743,0.014503495700836627,0.014506384128941788,0.014585139036267774,0.014645877310569433,0.014684683859025934,0.01474879849422905,0.014845433540870437,0.014849495349130932,0.014874794265239744,0.014894158303934266,0.015008452135423164,0.015048084428456445,0.015102146494977852,0.015117309178623545,0.015122855980833499,0.015143052475991339,0.015220783774154556,0.015280578034816519,0.015321507560301484,0.015347181931755566,0.015354715728587714,0.015456329339565084,0.015459533535046407,0.015473713706287338,0.015490470802291495,0.01549519355597879,0.015601180662822617,0.015733804127866156,0.0159316006423559,0.01594316979077685,0.016052662965719312,0.016057301607829443,0.016172605280207992,0.01618864753962787,0.01619525457854584,0.016316038852494042,0.01635627122452253,0.016574690273524303,0.016582281458605373,0.016647162532941677,0.016664722260853176,0.016780818231878422,0.017075865403203556,0.017202688197446463,0.0172429468744666,0.017557514877769277,0.017628049702783328,0.017847372476977007,0.017957836652719113,0.018250535492128014,0.018817622785031032,0.02022686778085309,0.020830003465207192,0.020899406357811685,0.020921916409494184],"xaxis":"x","y":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2,0.201,0.202,0.203,0.204,0.205,0.206,0.207,0.208,0.209,0.21,0.211,0.212,0.213,0.214,0.215,0.216,0.217,0.218,0.219,0.22,0.221,0.222,0.223,0.224,0.225,0.226,0.227,0.228,0.229,0.23,0.231,0.232,0.233,0.234,0.235,0.236,0.237,0.238,0.239,0.24,0.241,0.242,0.243,0.244,0.245,0.246,0.247,0.248,0.249,0.25,0.251,0.252,0.253,0.254,0.255,0.256,0.257,0.258,0.259,0.26,0.261,0.262,0.263,0.264,0.265,0.266,0.267,0.268,0.269,0.27,0.271,0.272,0.273,0.274,0.275,0.276,0.277,0.278,0.279,0.28,0.281,0.282,0.283,0.284,0.285,0.286,0.287,0.288,0.289,0.29,0.291,0.292,0.293,0.294,0.295,0.296,0.297,0.298,0.299,0.3,0.301,0.302,0.303,0.304,0.305,0.306,0.307,0.308,0.309,0.31,0.311,0.312,0.313,0.314,0.315,0.316,0.317,0.318,0.319,0.32,0.321,0.322,0.323,0.324,0.325,0.326,0.327,0.328,0.329,0.33,0.331,0.332,0.333,0.334,0.335,0.336,0.337,0.338,0.339,0.34,0.341,0.342,0.343,0.344,0.345,0.346,0.347,0.348,0.349,0.35,0.351,0.352,0.353,0.354,0.355,0.356,0.357,0.358,0.359,0.36,0.361,0.362,0.363,0.364,0.365,0.366,0.367,0.368,0.369,0.37,0.371,0.372,0.373,0.374,0.375,0.376,0.377,0.378,0.379,0.38,0.381,0.382,0.383,0.384,0.385,0.386,0.387,0.388,0.389,0.39,0.391,0.392,0.393,0.394,0.395,0.396,0.397,0.398,0.399,0.4,0.401,0.402,0.403,0.404,0.405,0.406,0.407,0.408,0.409,0.41,0.411,0.412,0.413,0.414,0.415,0.416,0.417,0.418,0.419,0.42,0.421,0.422,0.423,0.424,0.425,0.426,0.427,0.428,0.429,0.43,0.431,0.432,0.433,0.434,0.435,0.436,0.437,0.438,0.439,0.44,0.441,0.442,0.443,0.444,0.445,0.446,0.447,0.448,0.449,0.45,0.451,0.452,0.453,0.454,0.455,0.456,0.457,0.458,0.459,0.46,0.461,0.462,0.463,0.464,0.465,0.466,0.467,0.468,0.469,0.47,0.471,0.472,0.473,0.474,0.475,0.476,0.477,0.478,0.479,0.48,0.481,0.482,0.483,0.484,0.485,0.486,0.487,0.488,0.489,0.49,0.491,0.492,0.493,0.494,0.495,0.496,0.497,0.498,0.499,0.5,0.501,0.502,0.503,0.504,0.505,0.506,0.507,0.508,0.509,0.51,0.511,0.512,0.513,0.514,0.515,0.516,0.517,0.518,0.519,0.52,0.521,0.522,0.523,0.524,0.525,0.526,0.527,0.528,0.529,0.53,0.531,0.532,0.533,0.534,0.535,0.536,0.537,0.538,0.539,0.54,0.541,0.542,0.543,0.544,0.545,0.546,0.547,0.548,0.549,0.55,0.551,0.552,0.553,0.554,0.555,0.556,0.557,0.558,0.559,0.56,0.561,0.562,0.563,0.564,0.565,0.566,0.567,0.568,0.569,0.57,0.571,0.572,0.573,0.574,0.575,0.576,0.577,0.578,0.579,0.58,0.581,0.582,0.583,0.584,0.585,0.586,0.587,0.588,0.589,0.59,0.591,0.592,0.593,0.594,0.595,0.596,0.597,0.598,0.599,0.6,0.601,0.602,0.603,0.604,0.605,0.606,0.607,0.608,0.609,0.61,0.611,0.612,0.613,0.614,0.615,0.616,0.617,0.618,0.619,0.62,0.621,0.622,0.623,0.624,0.625,0.626,0.627,0.628,0.629,0.63,0.631,0.632,0.633,0.634,0.635,0.636,0.637,0.638,0.639,0.64,0.641,0.642,0.643,0.644,0.645,0.646,0.647,0.648,0.649,0.65,0.651,0.652,0.653,0.654,0.655,0.656,0.657,0.658,0.659,0.66,0.661,0.662,0.663,0.664,0.665,0.666,0.667,0.668,0.669,0.67,0.671,0.672,0.673,0.674,0.675,0.676,0.677,0.678,0.679,0.68,0.681,0.682,0.683,0.684,0.685,0.686,0.687,0.688,0.689,0.69,0.691,0.692,0.693,0.694,0.695,0.696,0.697,0.698,0.699,0.7,0.701,0.702,0.703,0.704,0.705,0.706,0.707,0.708,0.709,0.71,0.711,0.712,0.713,0.714,0.715,0.716,0.717,0.718,0.719,0.72,0.721,0.722,0.723,0.724,0.725,0.726,0.727,0.728,0.729,0.73,0.731,0.732,0.733,0.734,0.735,0.736,0.737,0.738,0.739,0.74,0.741,0.742,0.743,0.744,0.745,0.746,0.747,0.748,0.749,0.75,0.751,0.752,0.753,0.754,0.755,0.756,0.757,0.758,0.759,0.76,0.761,0.762,0.763,0.764,0.765,0.766,0.767,0.768,0.769,0.77,0.771,0.772,0.773,0.774,0.775,0.776,0.777,0.778,0.779,0.78,0.781,0.782,0.783,0.784,0.785,0.786,0.787,0.788,0.789,0.79,0.791,0.792,0.793,0.794,0.795,0.796,0.797,0.798,0.799,0.8,0.801,0.802,0.803,0.804,0.805,0.806,0.807,0.808,0.809,0.81,0.811,0.812,0.813,0.814,0.815,0.816,0.817,0.818,0.819,0.82,0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829,0.83,0.831,0.832,0.833,0.834,0.835,0.836,0.837,0.838,0.839,0.84,0.841,0.842,0.843,0.844,0.845,0.846,0.847,0.848,0.849,0.85,0.851,0.852,0.853,0.854,0.855,0.856,0.857,0.858,0.859,0.86,0.861,0.862,0.863,0.864,0.865,0.866,0.867,0.868,0.869,0.87,0.871,0.872,0.873,0.874,0.875,0.876,0.877,0.878,0.879,0.88,0.881,0.882,0.883,0.884,0.885,0.886,0.887,0.888,0.889,0.89,0.891,0.892,0.893,0.894,0.895,0.896,0.897,0.898,0.899,0.9,0.901,0.902,0.903,0.904,0.905,0.906,0.907,0.908,0.909,0.91,0.911,0.912,0.913,0.914,0.915,0.916,0.917,0.918,0.919,0.92,0.921,0.922,0.923,0.924,0.925,0.926,0.927,0.928,0.929,0.93,0.931,0.932,0.933,0.934,0.935,0.936,0.937,0.938,0.939,0.94,0.941,0.942,0.943,0.944,0.945,0.946,0.947,0.948,0.949,0.95,0.951,0.952,0.953,0.954,0.955,0.956,0.957,0.958,0.959,0.96,0.961,0.962,0.963,0.964,0.965,0.966,0.967,0.968,0.969,0.97,0.971,0.972,0.973,0.974,0.975,0.976,0.977,0.978,0.979,0.98,0.981,0.982,0.983,0.984,0.985,0.986,0.987,0.988,0.989,0.99,0.991,0.992,0.993,0.994,0.995,0.996,0.997,0.998,0.999,1.0],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,0.98],"title":{"text":"rmse"}},"yaxis":{"anchor":"x","domain":[0.0,0.3133333333333333],"title":{"text":"probability"},"rangemode":"tozero"},"xaxis2":{"anchor":"y2","domain":[0.0,0.98],"matches":"x","showticklabels":false},"yaxis2":{"anchor":"x2","domain":[0.34333333333333327,0.6566666666666665],"matches":"y","title":{"text":"probability"},"rangemode":"tozero"},"xaxis3":{"anchor":"y3","domain":[0.0,0.98],"matches":"x","showticklabels":false},"yaxis3":{"anchor":"x3","domain":[0.6866666666666665,0.9999999999999998],"matches":"y","title":{"text":"probability"},"rangemode":"tozero"},"annotations":[{"font":{},"showarrow":false,"text":"k=20","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.15666666666666665,"yanchor":"middle","yref":"paper"},{"font":{},"showarrow":false,"text":"k=15","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.4999999999999999,"yanchor":"middle","yref":"paper"},{"font":{},"showarrow":false,"text":"k=10","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.8433333333333332,"yanchor":"middle","yref":"paper"}],"legend":{"title":{"text":"algo"},"tracegroupgap":0},"title":{"text":"ECDF of the sparse approximation RMSE"},"height":700},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('3dbb5e4c-5006-4b0a-827e-daaab2222511');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>There is a clear pattern: MP performs the worst, MOMP performs the best, and OMP falls in between. The differences between the algorithms become more pronounced as <img src="https://latex.codecogs.com/png.latex?k"> increases.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, we explored greedy algorithms for tackling the sparse approximation problem. We introduced a scoring method to evaluate the quality of a proposed support, which naturally led to the development of a new greedy algorithm, MOMP. This algorithm is arguably simpler to understand than both MP and OMP and demonstrated superior performance in the example presented.</p>
<p>Theoretical results exist for MP and OMP, such as exact support recovery guarantees under specific conditions. In the future, I plan to investigate whether similar results can be extended to MOMP and to discuss strategies for accelerating its computation.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>While these are standard results for least squares problems, I find that they are not always presented in a clear way. I like proving such results without using gradients (which obscure the intuition, in my opinion) and without inverting matrices (which adds unnecessary caveats about rank and conditioning).↩︎</p></li>
<li id="fn2"><p>Convex relaxation (e.g.&nbsp;Basis Pursuit, LASSO) is another approach, but it is not the focus of this post.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/omp/omp.html</guid>
  <pubDate>Thu, 05 Dec 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Efficient leave one out cross validation - part 2</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/loocv_part2/loocv_part2.html</link>
  <description><![CDATA[ 





<p>In the <a href="https://tomshlomo.github.io/blog/posts/loocv/loocv_part1.html">first part</a>, we developed a method for performing efficient leave-one-out cross-validation (LOOCV). This method was precise but mandated that the loss and regularization functions be quadratic. Here, we’ll introduce a similar technique that provides an approximation, but eliminates the necessity for the loss and regularization to be quadratic. Additionally, we’ll code this method in Python using JAX and showcase its application on a sample dataset.</p>
<section id="notation-same-as-part-1" class="level1">
<h1>Notation (same as part 1)</h1>
<p>We denote the number of samples in the training dataset as <img src="https://latex.codecogs.com/png.latex?n">.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?m">-dimensional feature vectors are represented as <img src="https://latex.codecogs.com/png.latex?x_1"> to <img src="https://latex.codecogs.com/png.latex?x_n">, forming the rows of matrix <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>Targets are denoted as <img src="https://latex.codecogs.com/png.latex?y_1"> to <img src="https://latex.codecogs.com/png.latex?y_n">, forming the vector <img src="https://latex.codecogs.com/png.latex?y">. The model’s prediction for the <img src="https://latex.codecogs.com/png.latex?i">-th training sample is <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%20=%20x_i%5ET%20%5Ctheta">, where <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the coefficients vector. <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20=%20X%20%5Ctheta"> represents the vector containing all predictions.</p>
<p>We fit <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to the training data by minimizing the combined loss and regularization terms: <span id="eq-theta-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20:=%20%5Carg%5Cmin_%7B%5Ctheta'%7D%20f(%5Ctheta').%0A%5Ctag%7B1%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%0Af(%5Ctheta')%20:=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20l(x_i%5ET%20%5Ctheta';%20y_i)%20+%20r(%5Ctheta').%0A"> Here, <img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)"> represents the loss function, quantifying the difference between the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> and the true target <img src="https://latex.codecogs.com/png.latex?y_i">, while <img src="https://latex.codecogs.com/png.latex?r"> is the regularization function. We assume <img src="https://latex.codecogs.com/png.latex?l"> (as a function of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">) and <img src="https://latex.codecogs.com/png.latex?r"> are convex and twice differentiable. Special cases of this model include ordinary least squares (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2">, <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta')%20=%200">), ridge regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2">, <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta')%20=%20%5Calpha%20%5C%7C%20%5Ctheta'%20%5C%7C%5E2">), logistic regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;y_i)%20=%20%5Clog%20%5Cleft(%201%20+%20e%5E%7B-y_i%20%5Chat%7By%7D_i%7D%5Cright)"> with <img src="https://latex.codecogs.com/png.latex?y_i%20%5Cin%20%5C%7B%20-1,%201%5C%7D">), and Poisson regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;y_i)%20=%20y_i%20%5Chat%7By%7D_i%20-%20e%5E%7B%5Chat%7By%7D_i%7D">).</p>
<p>To denote the coefficients obtained by excluding the <img src="https://latex.codecogs.com/png.latex?j">-th example, we use <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%5E%7B(j)%7D%20=%20%5Carg%5Cmin_%7B%5Ctheta'%7D%20f%5E%7B(j)%7D%20(%5Ctheta')%0A"> where <img src="https://latex.codecogs.com/png.latex?%20f%5E%7B(j)%7D(%5Ctheta')%20:=%20%5Csum_%7Bi%20%5Cneq%20j%7D%20l(x_i%5ET%20%5Ctheta';%20y_i)%20+%20r(%5Ctheta')%20"> Similarly, <img src="https://latex.codecogs.com/png.latex?X%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?y%5E%7B(j)%7D">, represent <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> with the <img src="https://latex.codecogs.com/png.latex?j">-th row removed, respectively. We denote by <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> the predicted label for sample <img src="https://latex.codecogs.com/png.latex?j"> when it is left out: <span id="eq-y-tilde-j-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7By%7D_j%20:=%20x_j%20%5ET%20%5Ctheta%5E%7B(j)%7D%0A%5Ctag%7B2%7D"></span> Our goal is calculating <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">, for all <img src="https://latex.codecogs.com/png.latex?j">, efficiently.</p>
</section>
<section id="deriving-efficient-loocv-for-the-non-quadratic-case" class="level1">
<h1>Deriving efficient LOOCV for the non-quadratic case</h1>
<p>In this section, we extend our approach to scenarios where <img src="https://latex.codecogs.com/png.latex?l"> or <img src="https://latex.codecogs.com/png.latex?r"> are not quadratic. Although solving equation Equation&nbsp;1 is not simplified to solving a linear equation as it did in part 1, we can resort to the following approximation: <span id="eq-newton-approx"><img src="https://latex.codecogs.com/png.latex?%0AH%5E%7B(j)%7D%20(%5Ctheta%5E%7B(j)%7D%20-%20%5Ctheta)%20%5Capprox%20-g%5E%7B(j)%7D%0A%5Ctag%7B3%7D"></span> where <img src="https://latex.codecogs.com/png.latex?H%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?g%5E%7B(j)%7D"> represent the Hessian and gradient of <img src="https://latex.codecogs.com/png.latex?f%5E%7B(j)%7D"> at <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, respectively. The rationale here is that <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> should be relatively close (and closer as <img src="https://latex.codecogs.com/png.latex?n"> increases), making it likely that Newton’s method on <img src="https://latex.codecogs.com/png.latex?f%5E%7B(j)%7D"> converges in a single iteration when initialized on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p>Similar to the quadratic case, we can relate <img src="https://latex.codecogs.com/png.latex?H%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?g%5E%7B(j)%7D"> to <img src="https://latex.codecogs.com/png.latex?H"> and <img src="https://latex.codecogs.com/png.latex?g">, the Hessian and gradient of <img src="https://latex.codecogs.com/png.latex?f"> at <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AH%5E%7B(j)%7D%20&amp;=%20H%20-%20x_j%20l''(%5Chat%7By%7D_i%20;%20y_i)%20x_j%5ET%0A%5C%5C%0Ag%5E%7B(j)%7D%20&amp;=%20g%20-%20x_j%20l'(%5Chat%7By%7D_i%20;%20y_i)%20=%20-%20x_j%20l'(%5Chat%7By%7D_i%20;%20y_i)%0A%5Cend%7Balign*%7D"> allowing us to rewrite Equation&nbsp;3 as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%0A%20%20%20%20H%20-%20x_j%20l''%5Cleft(%5Chat%7By%7D_i%20;%20y_i%5Cright)%20x_j%5ET%0A%5Cright)%0A%5Cleft(%20%5Ctheta%5E%7B(j)%7D%20-%20%5Ctheta%20%5Cright)%0A%5Capprox%20%20x_j%20l'(%5Chat%7By%7D_i%20;%20y_i).%0A"> Next, we introduce the second equation: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AH%20%5Ctheta%5E%7B(j)%7D%0A%20%20%20%20-%20x_j%20l''(%5Chat%7By%7D_i%20;%20y_i)%20%5Ctilde%7By%7D_j%0A%20%20%20%20-%20H%20%5Ctheta%0A%20%20%20%20+%20x_j%20l''(%5Chat%7By%7D_i%20;%20y_i)%20%5Chat%7By%7D_j%0A%20%20%20%20&amp;%5Capprox%0A%20%20%20%20x_j%20l'(%5Chat%7By%7D_i%20;%20y_i)%0A%20%20%20%20%5C%5C%0A%20%20%20%20%5Ctilde%7By%7D_j%20&amp;=%20x_j%20%5ET%20%5Ctheta%5E%7B(j)%7D.%0A%5Cend%7Balign*%7D"> Now, we can eliminate <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> and solve for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Ctheta%5E%7B(j)%7D%20&amp;%5Capprox%20%5Ctheta%20+%20t_j%20(l'(%5Chat%7By%7D_i%20;%20y_i)%20+%20%20l''(%5Chat%7By%7D_i%20;%20y_i)%20(%5Ctilde%7By%7D_j%20-%20%5Chat%7By%7D_j))%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;%5Capprox%20x_j%20%5ET%20%5Cleft(%0A%20%20%20%20%5Ctheta%20+%20t_j%20(l'(%5Chat%7By%7D_i%20;%20y_i)%20+%20%20l''(%5Chat%7By%7D_i%20;%20y_i)%20(%5Ctilde%7By%7D_j%20-%20%5Chat%7By%7D_j))%0A%20%20%20%20%5Cright)%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;%5Capprox%0A%20%20%20%20%20%5Chat%7By%7D_j%0A%20%20%20%20+%20%5Cfrac%7Bh_j%7D%7B1%20-%20h_j%20l''(%5Chat%7By%7D_i%20;%20y_i)%7D%20%20l'(%5Chat%7By%7D_i%20;%20y_i)%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?t_j%20:=%20H%5E%7B-1%7D%20x_j"> and <img src="https://latex.codecogs.com/png.latex?h_j%20:=%20x_j%5ET%20t_j">.</p>
<p>It’s worth noting the resemblance between the expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> here and the expression obtained for the quadratic case.</p>
</section>
<section id="python-implementation" class="level1">
<h1>Python implementation</h1>
<p>Once more, we’ll turn to JAX, leveraging its automatic differentiation capabilities. Our estimator will take as inputs the loss and regularization functions, along with an optional “inverse link” function. This function can be employed to transform the predicted labels (e.g.&nbsp;a sigmoid to convert log-odds to probabilities in logistic regression, or an exponent to convert log-rate to rate in Poisson regression).</p>
<div id="cell-5" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Callable</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> jax</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy.typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> npt</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scipy</span>
<span id="cb1-7"></span>
<span id="cb1-8">Array <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> npt.NDArray[np.float64]</span>
<span id="cb1-9"></span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> GLMWithLOOCV:</span>
<span id="cb1-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(</span>
<span id="cb1-13">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb1-14">        loss: Callable[[Array, Array], Array],</span>
<span id="cb1-15">        reg: Callable[[Array], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>],</span>
<span id="cb1-16">        inverse_link: Callable[[Array], Array],</span>
<span id="cb1-17">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb1-18">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss</span>
<span id="cb1-19">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg</span>
<span id="cb1-20">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.inverse_link <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inverse_link</span>
<span id="cb1-21"></span>
<span id="cb1-22">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> f(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, theta: Array, X: Array, y: Array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb1-23">        y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta</span>
<span id="cb1-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.loss(y_hat, y).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.reg(theta)</span>
<span id="cb1-25"></span>
<span id="cb1-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: Array, y: Array):</span>
<span id="cb1-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We optimize f with L-BFGS-B as it has reasonable performance with the data below,</span></span>
<span id="cb1-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  but any other convex optimization algorithm can be used here.</span></span>
<span id="cb1-29">        result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.optimize.minimize(</span>
<span id="cb1-30">            jax.value_and_grad(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> theta: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.f(theta, X, y)),</span>
<span id="cb1-31">            x0<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.zeros(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]),</span>
<span id="cb1-32">            method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"L-BFGS-B"</span>,</span>
<span id="cb1-33">            jac<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-34">        )</span>
<span id="cb1-35">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> result.x</span>
<span id="cb1-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb1-37"></span>
<span id="cb1-38">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: Array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Array:</span>
<span id="cb1-39">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.inverse_link(X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_)</span>
<span id="cb1-40"></span>
<span id="cb1-41">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit_loocv_predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: Array, y: Array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Array:</span>
<span id="cb1-42">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fit(X, y)</span>
<span id="cb1-43">        y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_</span>
<span id="cb1-44">        l_prime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.vmap(jax.grad(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.loss, argnums<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))(y_hat, y)</span>
<span id="cb1-45">        l_prime_prime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.vmap(jax.hessian(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.loss, argnums<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))(y_hat, y)</span>
<span id="cb1-46">        H <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.hessian(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.f, argnums<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_, X, y)</span>
<span id="cb1-47">        t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.solve(</span>
<span id="cb1-48">            H,</span>
<span id="cb1-49">            X.T,</span>
<span id="cb1-50">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-51">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb1-52">        )</span>
<span id="cb1-53">        h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ij,ji-&gt;i"</span>, X, t)</span>
<span id="cb1-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.inverse_link(y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> l_prime_prime)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> l_prime)</span></code></pre></div>
</div>
</section>
<section id="example" class="level1">
<h1>Example</h1>
<p>To illustrate the concepts discussed above, we train a classifier on a dataset for predicting heart disease events. From a quick glance over Kaggle, it appears that achieving an AUC of approximately 0.9 is feasible.</p>
<div id="cell-7" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.compose <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ColumnTransformer</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler, FunctionTransformer</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> set_config</span>
<span id="cb2-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb2-8"></span>
<span id="cb2-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data/heart.csv"</span>)</span>
<span id="cb2-10">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">Sex</th>
<th data-quarto-table-cell-role="th">ChestPainType</th>
<th data-quarto-table-cell-role="th">RestingBP</th>
<th data-quarto-table-cell-role="th">Cholesterol</th>
<th data-quarto-table-cell-role="th">FastingBS</th>
<th data-quarto-table-cell-role="th">RestingECG</th>
<th data-quarto-table-cell-role="th">MaxHR</th>
<th data-quarto-table-cell-role="th">ExerciseAngina</th>
<th data-quarto-table-cell-role="th">Oldpeak</th>
<th data-quarto-table-cell-role="th">ST_Slope</th>
<th data-quarto-table-cell-role="th">HeartDisease</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>40</td>
<td>M</td>
<td>ATA</td>
<td>140</td>
<td>289</td>
<td>0</td>
<td>Normal</td>
<td>172</td>
<td>N</td>
<td>0.0</td>
<td>Up</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>49</td>
<td>F</td>
<td>NAP</td>
<td>160</td>
<td>180</td>
<td>0</td>
<td>Normal</td>
<td>156</td>
<td>N</td>
<td>1.0</td>
<td>Flat</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>37</td>
<td>M</td>
<td>ATA</td>
<td>130</td>
<td>283</td>
<td>0</td>
<td>ST</td>
<td>98</td>
<td>N</td>
<td>0.0</td>
<td>Up</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>48</td>
<td>F</td>
<td>ASY</td>
<td>138</td>
<td>214</td>
<td>0</td>
<td>Normal</td>
<td>108</td>
<td>Y</td>
<td>1.5</td>
<td>Flat</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>54</td>
<td>M</td>
<td>NAP</td>
<td>150</td>
<td>195</td>
<td>0</td>
<td>Normal</td>
<td>122</td>
<td>N</td>
<td>0.0</td>
<td>Up</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">913</td>
<td>45</td>
<td>M</td>
<td>TA</td>
<td>110</td>
<td>264</td>
<td>0</td>
<td>Normal</td>
<td>132</td>
<td>N</td>
<td>1.2</td>
<td>Flat</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">914</td>
<td>68</td>
<td>M</td>
<td>ASY</td>
<td>144</td>
<td>193</td>
<td>1</td>
<td>Normal</td>
<td>141</td>
<td>N</td>
<td>3.4</td>
<td>Flat</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">915</td>
<td>57</td>
<td>M</td>
<td>ASY</td>
<td>130</td>
<td>131</td>
<td>0</td>
<td>Normal</td>
<td>115</td>
<td>Y</td>
<td>1.2</td>
<td>Flat</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">916</td>
<td>57</td>
<td>F</td>
<td>ATA</td>
<td>130</td>
<td>236</td>
<td>0</td>
<td>LVH</td>
<td>174</td>
<td>N</td>
<td>0.0</td>
<td>Flat</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">917</td>
<td>38</td>
<td>M</td>
<td>NAP</td>
<td>138</td>
<td>175</td>
<td>0</td>
<td>Normal</td>
<td>173</td>
<td>N</td>
<td>0.0</td>
<td>Up</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>918 rows × 12 columns</p>
</div>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HeartDisease"</span>]</span>
<span id="cb3-2">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HeartDisease"</span>])</span>
<span id="cb3-3">X[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"one"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># an all-ones column to implicitly fit an intercept term</span></span>
<span id="cb3-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.get_dummies(X, drop_first<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-5">x_train, x_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(</span>
<span id="cb3-6">    X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span></span>
<span id="cb3-7">)</span>
<span id="cb3-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x_train<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x_train.shape=(642, 16), x_test.shape=(276, 16)</code></pre>
</div>
</div>
<p>Our approach involves using stratified logistic regression with a combination of Laplacian and sum of squares regularization. While this may not be the optimal model for this specific problem, it serves well for demonstrating the concepts.</p>
<p>In our model, we stratify over the sex of the patient, meaning we fit two coefficient vectors: one for males and one for females. The Laplacian regularization promotes similarity between the coefficient vectors for females and males. You can read more about stratified models with Laplacian regularization <a href="https://web.stanford.edu/~boyd/papers/pdf/strat_models.pdf">here</a>.</p>
<div id="cell-10" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> stratify(X: Array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Array:</span>
<span id="cb5-2">    z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex_M"</span>].values[:, np.newaxis]</span>
<span id="cb5-3">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex_M"</span>]).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>).values</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.hstack([X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> z, X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>z])</span>
<span id="cb5-5"></span>
<span id="cb5-6"></span>
<span id="cb5-7">transformer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline(</span>
<span id="cb5-8">    [</span>
<span id="cb5-9">        (</span>
<span id="cb5-10">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scale"</span>,</span>
<span id="cb5-11">            ColumnTransformer(</span>
<span id="cb5-12">                [</span>
<span id="cb5-13">                    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"passthrough"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"one"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex_M"</span>]),</span>
<span id="cb5-14">                    (</span>
<span id="cb5-15">                        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scale"</span>,</span>
<span id="cb5-16">                        StandardScaler(),</span>
<span id="cb5-17">                        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(x_train.columns) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"one"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex_M"</span>}),</span>
<span id="cb5-18">                    ),</span>
<span id="cb5-19">                ],</span>
<span id="cb5-20">                verbose_feature_names_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb5-21">            ).set_output(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pandas"</span>),</span>
<span id="cb5-22">        ),</span>
<span id="cb5-23">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"stratify"</span>, FunctionTransformer(stratify)),</span>
<span id="cb5-24">    ]</span>
<span id="cb5-25">)</span>
<span id="cb5-26"></span>
<span id="cb5-27">x_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> transformer.fit_transform(x_train)</span>
<span id="cb5-28">x_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> transformer.transform(x_test)</span></code></pre></div>
</div>
<p>Next we define the regularization matrices:</p>
<div id="cell-12" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb6-2">laplacian <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]])</span>
<span id="cb6-3">laplacian <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.kron(laplacian, np.eye(m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb6-4">ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.eye(m)</span>
<span id="cb6-5">ridge[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># no penalty on the intercept</span></span>
<span id="cb6-6">ridge[m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span></code></pre></div>
</div>
<p>We have two hyperparameters in our model: the strength of the sum of squares (ridge) regularization and the strength of the Laplacian regularization.</p>
<p>For hyperparameter optimization, we utilize Optuna.</p>
<div id="cell-14" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> optuna</span>
<span id="cb7-2"></span>
<span id="cb7-3"></span>
<span id="cb7-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> model_factory(alpha: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>, beta: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> GLMWithLOOCV:</span>
<span id="cb7-5">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> laplacian</span>
<span id="cb7-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> GLMWithLOOCV(</span>
<span id="cb7-7">        loss<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> y_hat, y: <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>jax.nn.log_sigmoid((y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> y_hat),</span>
<span id="cb7-8">        reg<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> theta: theta.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta,</span>
<span id="cb7-9">        inverse_link<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>jax.nn.sigmoid,</span>
<span id="cb7-10">    )</span>
<span id="cb7-11"></span>
<span id="cb7-12"></span>
<span id="cb7-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> objective(trial: optuna.Trial):</span>
<span id="cb7-14">    alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"alpha"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e3</span>, log<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-15">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e3</span>, log<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-16">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_factory(alpha, beta)</span>
<span id="cb7-17">    y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit_loocv_predict(x_train, y_train.values)</span>
<span id="cb7-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>roc_auc_score(</span>
<span id="cb7-19">        y_train, y_tilde</span>
<span id="cb7-20">    )  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># minus since optuna minimizes the objective and we need to maximize</span></span>
<span id="cb7-21"></span>
<span id="cb7-22"></span>
<span id="cb7-23">study: optuna.Study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study()</span>
<span id="cb7-24">study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb7-25">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_factory(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>study.best_params)</span>
<span id="cb7-26">roc_auc_score(</span>
<span id="cb7-27">    y_test,</span>
<span id="cb7-28">    model.fit(x_train, y_train.values).predict(x_test),</span>
<span id="cb7-29">)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2024-03-24 08:56:29,175] A new study created in memory with name: no-name-61410be4-4d1d-4a9b-8cc6-068004336d5c
[I 2024-03-24 08:56:30,700] Trial 0 finished with value: -0.9092301389105666 and parameters: {'alpha': 0.012775258780126265, 'beta': 0.002191596541067304}. Best is trial 0 with value: -0.9092301389105666.
[I 2024-03-24 08:56:31,058] Trial 1 finished with value: -0.909025284844701 and parameters: {'alpha': 0.0006298752554559054, 'beta': 2.1509611015314225e-05}. Best is trial 0 with value: -0.9092301389105666.
[I 2024-03-24 08:56:31,605] Trial 2 finished with value: -0.9118639769002653 and parameters: {'alpha': 4.938878297009436e-06, 'beta': 780.3122728275399}. Best is trial 2 with value: -0.9118639769002653.
[I 2024-03-24 08:56:31,926] Trial 3 finished with value: -0.914858748244108 and parameters: {'alpha': 3.1356031871802617, 'beta': 30.256480198543056}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:32,239] Trial 4 finished with value: -0.9104885281723115 and parameters: {'alpha': 0.005919567636794085, 'beta': 0.21420343169280254}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:32,557] Trial 5 finished with value: -0.9091325893553925 and parameters: {'alpha': 0.0029083107541165135, 'beta': 0.003468355333742003}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:32,947] Trial 6 finished with value: -0.9103422038395502 and parameters: {'alpha': 0.25663606336322276, 'beta': 2.027770483000284e-06}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:33,339] Trial 7 finished with value: -0.912537068830966 and parameters: {'alpha': 1.0538711667069038, 'beta': 0.0003360283526189701}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:33,622] Trial 8 finished with value: -0.9089862650226315 and parameters: {'alpha': 1.079098816654311e-05, 'beta': 1.3846427704434615e-06}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:34,060] Trial 9 finished with value: -0.9089667551115967 and parameters: {'alpha': 1.2646277638647537e-06, 'beta': 5.240465554229351e-05}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:34,378] Trial 10 finished with value: -0.8929003433744341 and parameters: {'alpha': 521.943351068861, 'beta': 21.128403850663364}. Best is trial 3 with value: -0.914858748244108.
[I 2024-03-24 08:56:34,601] Trial 11 finished with value: -0.9151416419541125 and parameters: {'alpha': 7.849754690834663, 'beta': 0.6886856542708458}. Best is trial 11 with value: -0.9151416419541125.
[I 2024-03-24 08:56:34,839] Trial 12 finished with value: -0.9127614328078664 and parameters: {'alpha': 36.17696304784756, 'beta': 0.9382223092187879}. Best is trial 11 with value: -0.9151416419541125.
[I 2024-03-24 08:56:35,023] Trial 13 finished with value: -0.9154538005306695 and parameters: {'alpha': 20.819919697832866, 'beta': 18.009933396875475}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:35,430] Trial 14 finished with value: -0.9134150148275324 and parameters: {'alpha': 50.2350037043773, 'beta': 3.819223887756615}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:35,728] Trial 15 finished with value: -0.8607480099890744 and parameters: {'alpha': 768.6568030874997, 'beta': 0.07013747752246598}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:36,291] Trial 16 finished with value: -0.9138930076478852 and parameters: {'alpha': 9.155957878379764, 'beta': 819.8096902730499}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:36,608] Trial 17 finished with value: -0.9130540814733884 and parameters: {'alpha': 0.1485485239581707, 'beta': 38.97815021766497}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:36,858] Trial 18 finished with value: -0.9053866864367098 and parameters: {'alpha': 78.72829138458891, 'beta': 0.016036059061954787}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:37,161] Trial 19 finished with value: -0.9136783986265022 and parameters: {'alpha': 0.1375243107258574, 'beta': 1.8291549312508841}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:37,528] Trial 20 finished with value: -0.9129760418292493 and parameters: {'alpha': 1.0707674807216208, 'beta': 117.83261301793227}. Best is trial 13 with value: -0.9154538005306695.
[I 2024-03-24 08:56:37,795] Trial 21 finished with value: -0.9159903230841268 and parameters: {'alpha': 4.416371155644641, 'beta': 9.316946205587612}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:38,065] Trial 22 finished with value: -0.9146831590447948 and parameters: {'alpha': 12.426070601603788, 'beta': 0.3853665693475585}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:38,316] Trial 23 finished with value: -0.8677228031840175 and parameters: {'alpha': 319.5534395847857, 'beta': 2.7762991166087905}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:38,620] Trial 24 finished with value: -0.9158342437958482 and parameters: {'alpha': 3.112413409324069, 'beta': 7.37150462979163}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:38,914] Trial 25 finished with value: -0.9150733572654908 and parameters: {'alpha': 0.7868633423237494, 'beta': 8.431547546648925}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:39,290] Trial 26 finished with value: -0.9122541751209613 and parameters: {'alpha': 0.03174734240904159, 'beta': 114.13914385182184}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:39,658] Trial 27 finished with value: -0.9116103480568128 and parameters: {'alpha': 90.62207805800448, 'beta': 202.6257677752164}. Best is trial 21 with value: -0.9159903230841268.
[I 2024-03-24 08:56:39,917] Trial 28 finished with value: -0.9160781176837834 and parameters: {'alpha': 6.553554396630455, 'beta': 11.167094954503991}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:40,310] Trial 29 finished with value: -0.9098349461526456 and parameters: {'alpha': 0.029887003146111108, 'beta': 0.05116561128441574}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:40,633] Trial 30 finished with value: -0.9140198220696114 and parameters: {'alpha': 2.942224930336697, 'beta': 0.00513193420243311}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:40,934] Trial 31 finished with value: -0.915219681598252 and parameters: {'alpha': 22.919512362299404, 'beta': 12.27623192594196}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:41,189] Trial 32 finished with value: -0.9157464491961917 and parameters: {'alpha': 2.757918964666127, 'beta': 6.038269195534732}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:41,617] Trial 33 finished with value: -0.9142149211799594 and parameters: {'alpha': 2.838682256536785, 'beta': 0.22519746478261113}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:41,949] Trial 34 finished with value: -0.9148392383330732 and parameters: {'alpha': 0.3616241392613928, 'beta': 5.830178783914518}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:42,382] Trial 35 finished with value: -0.9120005462775089 and parameters: {'alpha': 0.0008822564977771567, 'beta': 307.9388398609111}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:42,562] Trial 36 finished with value: -0.9134930544716716 and parameters: {'alpha': 0.04187871349145953, 'beta': 1.8734690287950477}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:42,925] Trial 37 finished with value: -0.9120590760106134 and parameters: {'alpha': 120.58206602832338, 'beta': 38.5688359496023}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:43,277] Trial 38 finished with value: -0.9141856563134072 and parameters: {'alpha': 2.86004817515422, 'beta': 0.14090599669599804}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:43,658] Trial 39 finished with value: -0.9131028562509754 and parameters: {'alpha': 0.46403986565570593, 'beta': 48.64240100580483}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:44,034] Trial 40 finished with value: -0.9092106289995319 and parameters: {'alpha': 3.056882608727012e-05, 'beta': 0.013794522533314474}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:44,311] Trial 41 finished with value: -0.9159513032620571 and parameters: {'alpha': 7.453204156267973, 'beta': 18.41659964949616}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:44,593] Trial 42 finished with value: -0.916068362728266 and parameters: {'alpha': 5.494711869607197, 'beta': 6.6964359014868124}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:44,981] Trial 43 finished with value: -0.9152391915092868 and parameters: {'alpha': 6.789849837478972, 'beta': 0.9088200937340541}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:45,353] Trial 44 finished with value: -0.9095422974871235 and parameters: {'alpha': 272.45143282464653, 'beta': 86.15445718504921}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:45,689] Trial 45 finished with value: -0.9129857967847667 and parameters: {'alpha': 1.245276289906343, 'beta': 494.7421191070618}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:46,090] Trial 46 finished with value: -0.9099032308412675 and parameters: {'alpha': 0.0911593221200086, 'beta': 0.0005329474451518324}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:46,388] Trial 47 finished with value: -0.9154050257530826 and parameters: {'alpha': 19.418957034796815, 'beta': 17.869675293948607}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:46,745] Trial 48 finished with value: -0.9114152489464649 and parameters: {'alpha': 0.004900207869786724, 'beta': 0.5092857463474738}. Best is trial 28 with value: -0.9160781176837834.
[I 2024-03-24 08:56:47,053] Trial 49 finished with value: -0.9153757608865303 and parameters: {'alpha': 5.6025164558499885, 'beta': 1.4684843094190325}. Best is trial 28 with value: -0.9160781176837834.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>0.9398954703832751</code></pre>
</div>
</div>
</section>
<section id="verification-of-implementation" class="level1">
<h1>Verification of implementation</h1>
<p>To assess the accuracy of our implementation, we compare the approximated leave-one-out predictions with the actual leave-one-out predictions:</p>
<div id="cell-16" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> time <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.express <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> px</span>
<span id="cb10-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.io <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pio</span>
<span id="cb10-5"></span>
<span id="cb10-6">pio.renderers.default <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"notebook"</span></span>
<span id="cb10-7"></span>
<span id="cb10-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LeaveOneOut</span>
<span id="cb10-9"></span>
<span id="cb10-10">t_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time()</span>
<span id="cb10-11">y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(y_train.shape)</span>
<span id="cb10-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i, (train_index, val_index) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(LeaveOneOut().split(x_train)):</span>
<span id="cb10-13">    X_loo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_train[train_index, :]</span>
<span id="cb10-14">    y_loo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_train.values[train_index]</span>
<span id="cb10-15">    model.fit(X_loo, y_loo)</span>
<span id="cb10-16">    y_tilde[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(x_train[val_index, :])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb10-17">standard_loocv_runtime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> t_start</span>
<span id="cb10-18"></span>
<span id="cb10-19">t_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time()</span>
<span id="cb10-20">y_tilde_approx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit_loocv_predict(x_train, y_train.values)</span>
<span id="cb10-21">efficient_loocv_runtime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> t_start</span>
<span id="cb10-22"></span>
<span id="cb10-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_tilde_approx)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb10-24">px.scatter(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_tilde, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_tilde_approx)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>np.abs(y_tilde - y_tilde_approx).mean()=4.465176e-05</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="ecbdd84e-604b-4eb1-9306-ef14fc04ad80" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ecbdd84e-604b-4eb1-9306-ef14fc04ad80")) {                    Plotly.newPlot(                        "ecbdd84e-604b-4eb1-9306-ef14fc04ad80",                        [{"hovertemplate":"x=%{x}\u003cbr\u003ey=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","orientation":"v","showlegend":false,"x":[0.2087310403585434,0.951321005821228,0.892578125,0.9492283463478088,0.6897168755531311,0.8895952105522156,0.057480331510305405,0.23683156073093414,0.8105262517929077,0.9072948694229126,0.4536738991737366,0.06033267080783844,0.10185585916042328,0.07936308532953262,0.0703546404838562,0.7259936332702637,0.10135380923748016,0.1026042178273201,0.825215756893158,0.1844044029712677,0.9412919878959656,0.3393535017967224,0.08525629341602325,0.040929585695266724,0.36935651302337646,0.10833979398012161,0.8948796391487122,0.5802428722381592,0.10267791152000427,0.6525864005088806,0.9136537909507751,0.0833110436797142,0.05976325646042824,0.7122671604156494,0.8504403829574585,0.16025599837303162,0.46891725063323975,0.7724125385284424,0.20057350397109985,0.1749124526977539,0.06272809207439423,0.6853570342063904,0.8739985823631287,0.9606488943099976,0.8582143783569336,0.31899288296699524,0.968699038028717,0.6505769491195679,0.4328449070453644,0.8997237682342529,0.2661797106266022,0.19862237572669983,0.8321604132652283,0.6485037207603455,0.9042520523071289,0.8799607753753662,0.08250493556261063,0.9313024282455444,0.7011739015579224,0.7757863402366638,0.8470264077186584,0.6725320219993591,0.42317652702331543,0.4804948568344116,0.7179282307624817,0.7257971167564392,0.7150857448577881,0.7720867991447449,0.16145862638950348,0.7748014330863953,0.18231891095638275,0.10540591925382614,0.911888062953949,0.07707317173480988,0.06324166059494019,0.9431082010269165,0.07852508872747421,0.09072653949260712,0.3859798014163971,0.07407274842262268,0.8612204790115356,0.7250381112098694,0.8421931266784668,0.04659707471728325,0.9294630885124207,0.11661204695701599,0.9220478534698486,0.9279577136039734,0.8292301893234253,0.7606818079948425,0.813230037689209,0.10530895739793777,0.7741605043411255,0.9083536863327026,0.07693560421466827,0.8614555597305298,0.9513072371482849,0.12733982503414154,0.8903078436851501,0.05285748839378357,0.8654146790504456,0.07511915266513824,0.6991581916809082,0.07012956589460373,0.922487735748291,0.07205508649349213,0.6723937392234802,0.8845747113227844,0.8393978476524353,0.07828027755022049,0.9181877970695496,0.07827742397785187,0.9323244690895081,0.1822272539138794,0.18095725774765015,0.04384802654385567,0.8322041630744934,0.719239354133606,0.06631715595722198,0.04979793354868889,0.9164431095123291,0.08540880680084229,0.9424579739570618,0.3083292245864868,0.04228374361991882,0.8856253623962402,0.07880861312150955,0.9411454200744629,0.8755931854248047,0.46339428424835205,0.07331972569227219,0.2218138575553894,0.3143472373485565,0.13897372782230377,0.299835741519928,0.5352551341056824,0.07403014600276947,0.850985050201416,0.7966851592063904,0.24168190360069275,0.37758350372314453,0.8387112021446228,0.9403632283210754,0.04968101903796196,0.04815135523676872,0.800367534160614,0.257630854845047,0.9557643532752991,0.078946553170681,0.2140844464302063,0.8513705730438232,0.08160232752561569,0.733271062374115,0.8302977085113525,0.06655026972293854,0.8408424854278564,0.06738677620887756,0.9476971626281738,0.7754809260368347,0.31508544087409973,0.5104517936706543,0.9586314558982849,0.10553105920553207,0.8249387145042419,0.29967421293258667,0.27396735548973083,0.8852274417877197,0.039901748299598694,0.4456704556941986,0.8770008683204651,0.9586985111236572,0.18430499732494354,0.8999195098876953,0.11574368923902512,0.0882386714220047,0.9006831645965576,0.7757165431976318,0.9024726748466492,0.2033347636461258,0.11284390091896057,0.7530911564826965,0.7884171009063721,0.4643567204475403,0.8949801921844482,0.9577525854110718,0.9452322125434875,0.9515891075134277,0.7714871168136597,0.04680183157324791,0.12554046511650085,0.8753000497817993,0.9071999788284302,0.3479093611240387,0.07352201640605927,0.8862862586975098,0.09767239540815353,0.43989118933677673,0.0657842606306076,0.8828748464584351,0.04941267892718315,0.7769240736961365,0.9421719312667847,0.5085277557373047,0.8559432029724121,0.8673598766326904,0.05676526948809624,0.8358811140060425,0.8786158561706543,0.5347985029220581,0.8478772044181824,0.0513913631439209,0.9469733238220215,0.051493093371391296,0.42895573377609253,0.09101572632789612,0.07516443729400635,0.9688757658004761,0.8847766518592834,0.29739710688591003,0.8906638622283936,0.793807327747345,0.06650164723396301,0.16865800321102142,0.9684415459632874,0.9009794592857361,0.3801676034927368,0.420207142829895,0.8819141983985901,0.4685937464237213,0.40700316429138184,0.1606413722038269,0.9602925777435303,0.9659398198127747,0.9120914340019226,0.9271039366722107,0.1020229235291481,0.15988604724407196,0.07381155341863632,0.09444916993379593,0.8645961284637451,0.7683597803115845,0.9311071634292603,0.2676723003387451,0.9507537484169006,0.9392818808555603,0.08717995136976242,0.8255689740180969,0.08612480759620667,0.7650277614593506,0.9133515357971191,0.9461516737937927,0.3343157470226288,0.8930245041847229,0.8326566815376282,0.7959510684013367,0.3993188738822937,0.8698099255561829,0.5299842953681946,0.05578729137778282,0.05286641791462898,0.9243903160095215,0.966105043888092,0.9426194429397583,0.9351277947425842,0.1554795652627945,0.8789321780204773,0.278093159198761,0.9174546599388123,0.7523866891860962,0.8489865660667419,0.7928344011306763,0.050687503069639206,0.10257282853126526,0.845484733581543,0.06024947017431259,0.24843578040599823,0.532315194606781,0.06744884699583054,0.36367669701576233,0.7386084198951721,0.17062193155288696,0.6374344229698181,0.8738610148429871,0.9180182814598083,0.15131217241287231,0.8441857695579529,0.08777207881212234,0.7028859853744507,0.7284144759178162,0.7907246351242065,0.9147592782974243,0.4831897020339966,0.7327115535736084,0.15669891238212585,0.23638449609279633,0.6223838925361633,0.058921415358781815,0.6103551983833313,0.9597985744476318,0.34214693307876587,0.1353352963924408,0.9169896245002747,0.912632942199707,0.14265838265419006,0.9444606900215149,0.04064545780420303,0.7504770159721375,0.614197850227356,0.9419201612472534,0.14354941248893738,0.9558071494102478,0.1068350225687027,0.7047193050384521,0.12632480263710022,0.04748031497001648,0.915087103843689,0.8286321759223938,0.7522391080856323,0.07999377697706223,0.941343367099762,0.13272547721862793,0.05903242528438568,0.12826597690582275,0.4002159535884857,0.3519414961338043,0.9310546517372131,0.18755027651786804,0.9537779688835144,0.8967862129211426,0.056566350162029266,0.18274234235286713,0.0588318333029747,0.7079342007637024,0.06269782036542892,0.4904952049255371,0.9464264512062073,0.10462086647748947,0.7697261571884155,0.836976170539856,0.18552283942699432,0.5664607286453247,0.9148613810539246,0.3977566361427307,0.06202172487974167,0.12439121305942535,0.9763551354408264,0.819310188293457,0.3135751187801361,0.6449512839317322,0.23856158554553986,0.8797604441642761,0.0952906459569931,0.9541757106781006,0.8904979228973389,0.10889361053705215,0.8957836627960205,0.8374279737472534,0.9725998044013977,0.714676022529602,0.6141152381896973,0.17929528653621674,0.8387454152107239,0.6043301820755005,0.8114116191864014,0.9853599667549133,0.08082623779773712,0.4074491858482361,0.0694747120141983,0.385276198387146,0.8368853330612183,0.8491184711456299,0.15699487924575806,0.04419117420911789,0.07449550926685333,0.8256090879440308,0.17058640718460083,0.07288192212581635,0.9611362814903259,0.40136030316352844,0.7884674072265625,0.10022105276584625,0.608020007610321,0.42577841877937317,0.8568881750106812,0.19269205629825592,0.5207127928733826,0.06460121273994446,0.6030045747756958,0.06109505519270897,0.9412218928337097,0.8817570209503174,0.1869865208864212,0.8696405291557312,0.09424431622028351,0.8223956227302551,0.7480613589286804,0.908442497253418,0.9359220862388611,0.5999656915664673,0.7382714748382568,0.6551924347877502,0.9433837532997131,0.3882337212562561,0.11246078461408615,0.9357706904411316,0.04605536162853241,0.7209967970848083,0.1571369171142578,0.5478659868240356,0.9260368943214417,0.7416079044342041,0.0500793382525444,0.8864167928695679,0.9119592308998108,0.6394474506378174,0.7461156845092773,0.06661690771579742,0.08342648297548294,0.9341509342193604,0.6767424941062927,0.5432235598564148,0.9446871280670166,0.057267170399427414,0.7389323115348816,0.927149772644043,0.9652608633041382,0.9466450214385986,0.20884199440479279,0.48305264115333557,0.1803155541419983,0.6451157927513123,0.7476624250411987,0.843915581703186,0.8233970999717712,0.9590121507644653,0.19773106276988983,0.24518492817878723,0.7610183358192444,0.1224537044763565,0.07019595056772232,0.9148505926132202,0.2555682957172394,0.8386254906654358,0.7933174967765808,0.15356841683387756,0.8478567004203796,0.4867485463619232,0.9668745994567871,0.8910166621208191,0.8378475904464722,0.2948984205722809,0.5555121898651123,0.6329130530357361,0.2658010423183441,0.20782767236232758,0.06334889680147171,0.48179277777671814,0.9338757395744324,0.9135794639587402,0.9395966529846191,0.8998000025749207,0.9352732300758362,0.8372915387153625,0.5801140666007996,0.1213110014796257,0.22934572398662567,0.054755646735429764,0.7762110829353333,0.05315486341714859,0.18201948702335358,0.7429190278053284,0.2707709074020386,0.92886883020401,0.5386443734169006,0.9069165587425232,0.9402324557304382,0.05989225581288338,0.30954673886299133,0.9592004418373108,0.7766126394271851,0.9469029903411865,0.9379135966300964,0.3588274419307709,0.046530582010746,0.8512879014015198,0.9567961692810059,0.9365308284759521,0.10143044590950012,0.11151644587516785,0.06293460726737976,0.7131668329238892,0.8289114832878113,0.09775511175394058,0.5193256735801697,0.9012763500213623,0.27010372281074524,0.3142625093460083,0.1512625813484192,0.9314224123954773,0.9626637697219849,0.9243719577789307,0.07100965827703476,0.8251475095748901,0.18687129020690918,0.1095801293849945,0.7991307377815247,0.08189631998538971,0.866504967212677,0.13081295788288116,0.9361091256141663,0.05592336878180504,0.9785115122795105,0.09302143007516861,0.7538305521011353,0.5837286710739136,0.9408089518547058,0.8560580611228943,0.8727142810821533,0.8722611665725708,0.19251365959644318,0.973849356174469,0.0516316182911396,0.17178358137607574,0.6470378041267395,0.7619329690933228,0.0671367421746254,0.055675093084573746,0.8602417707443237,0.09880056232213974,0.8291338086128235,0.49565359950065613,0.9205543994903564,0.20841659605503082,0.574397087097168,0.9484546184539795,0.15364161133766174,0.775540828704834,0.8561344146728516,0.7877108454704285,0.30655723810195923,0.8552945852279663,0.19873909652233124,0.8013047575950623,0.06792011857032776,0.940460741519928,0.716119110584259,0.9469349384307861,0.13341452181339264,0.4421258568763733,0.08186925202608109,0.8637968897819519,0.6686155200004578,0.05441712588071823,0.8610818982124329,0.6742730140686035,0.38748738169670105,0.04858855530619621,0.9384456276893616,0.1493416130542755,0.8967837691307068,0.9158006906509399,0.402496874332428,0.5507477521896362,0.09308303892612457,0.07850475609302521,0.39740023016929626,0.08483925461769104,0.8551640510559082,0.8491129279136658,0.940919816493988,0.9499386548995972,0.961499035358429,0.14979340136051178,0.7077821493148804,0.936251163482666,0.8441528081893921,0.8273400068283081,0.9065074920654297,0.8656537532806396,0.3968763053417206,0.9135011434555054,0.9403337240219116,0.11157646775245667,0.960383415222168,0.40963807702064514,0.07164869457483292,0.45042070746421814,0.4080970287322998,0.953468918800354,0.061946772038936615,0.9304147362709045,0.07093528658151627,0.0660928413271904,0.9055554866790771,0.9045584201812744,0.28464412689208984,0.868806004524231,0.5053316950798035,0.9219005107879639,0.9600469470024109,0.33409807085990906,0.28937187790870667,0.19642846286296844,0.1923009157180786,0.10216984152793884,0.0505615659058094,0.1576240211725235,0.41026821732521057,0.5152388215065002,0.4226077198982239,0.3549664318561554,0.9202710390090942,0.6289896368980408,0.08547379076480865,0.9300019145011902,0.2667236328125,0.6546074151992798,0.8598851561546326,0.8988086581230164,0.07601482421159744,0.5729012489318848,0.14666122198104858,0.0659128949046135,0.6627146601676941,0.8648512959480286,0.06316628307104111,0.6179089546203613,0.7171225547790527,0.9500004053115845,0.483040452003479,0.904744565486908,0.90304034948349,0.06644175946712494,0.7087041735649109,0.06201086938381195,0.11764969676733017,0.07991551607847214,0.15846621990203857,0.20078295469284058,0.5096142292022705,0.6186903119087219,0.5760588645935059],"xaxis":"x","y":[0.20881271362304688,0.9513320326805115,0.8925749659538269,0.949222207069397,0.6896360516548157,0.8895849585533142,0.05748468637466431,0.23685672879219055,0.8105260133743286,0.9072884917259216,0.4535759687423706,0.06037912517786026,0.10190793871879578,0.07938918471336365,0.07048572599887848,0.7259840965270996,0.10137408971786499,0.10267859697341919,0.8252162337303162,0.18450245261192322,0.9413001537322998,0.3393480181694031,0.0853429064154625,0.040955208241939545,0.3694741427898407,0.10838959366083145,0.8949134349822998,0.580194354057312,0.1026880145072937,0.6526398062705994,0.9136612415313721,0.08333364129066467,0.05981060862541199,0.7123024463653564,0.8504348993301392,0.1602715253829956,0.46892139315605164,0.7724337577819824,0.20083819329738617,0.1749444603919983,0.06275565922260284,0.6854019165039062,0.8740319609642029,0.9606456756591797,0.8582000732421875,0.31907618045806885,0.9686936736106873,0.6505436897277832,0.43300899863243103,0.899736225605011,0.2662566900253296,0.19872240722179413,0.8321266770362854,0.6484683156013489,0.9043005108833313,0.8799678683280945,0.08250605314970016,0.9312641024589539,0.7012209296226501,0.7757439613342285,0.8469902873039246,0.6724795699119568,0.4233388900756836,0.4805191457271576,0.7179068922996521,0.7257726192474365,0.715078592300415,0.7721201181411743,0.16145896911621094,0.7748555541038513,0.18249128758907318,0.10541243851184845,0.9118986129760742,0.07713992893695831,0.06325259804725647,0.9431084394454956,0.07858330011367798,0.09076830744743347,0.3858097791671753,0.0740862563252449,0.8612176775932312,0.7250295877456665,0.8421753644943237,0.046599552035331726,0.9294527173042297,0.11704239249229431,0.9220462441444397,0.9278144836425781,0.8292397856712341,0.7606609463691711,0.8133007884025574,0.10532557964324951,0.7741959095001221,0.9083476662635803,0.07693656533956528,0.861452043056488,0.9513100385665894,0.12733499705791473,0.8903295993804932,0.05286671593785286,0.8654205799102783,0.07512307912111282,0.6992356181144714,0.070211261510849,0.9225009083747864,0.07209663838148117,0.6724193692207336,0.8845586776733398,0.8392863273620605,0.07831557095050812,0.9180727601051331,0.07828203588724136,0.9323300719261169,0.18225277960300446,0.18095238506793976,0.04388591647148132,0.8322439789772034,0.7191786766052246,0.06632766127586365,0.04980502277612686,0.9164416193962097,0.08544188737869263,0.9424626231193542,0.30830127000808716,0.04229571670293808,0.885668158531189,0.0788145661354065,0.941144585609436,0.875678539276123,0.46332040429115295,0.07334686070680618,0.2218611240386963,0.3144778311252594,0.1389739066362381,0.29991382360458374,0.5353689789772034,0.07404511421918869,0.8509618043899536,0.7966556549072266,0.24169614911079407,0.3776378035545349,0.8386906981468201,0.9403938055038452,0.04973340034484863,0.04817157983779907,0.8003730177879333,0.25758716464042664,0.9557614326477051,0.07894811034202576,0.21419937908649445,0.8513518571853638,0.0817389190196991,0.7332214713096619,0.8302866220474243,0.06657660007476807,0.8408544063568115,0.06744541227817535,0.94769686460495,0.7754923701286316,0.31508347392082214,0.5106629729270935,0.9586226940155029,0.10555842518806458,0.8249292969703674,0.29977405071258545,0.27405840158462524,0.8852354288101196,0.03992580622434616,0.4455355703830719,0.8770005106925964,0.9587066173553467,0.18435899913311005,0.8997020721435547,0.1157447099685669,0.08832462877035141,0.9007088541984558,0.7757254242897034,0.9024724960327148,0.20340512692928314,0.11284368485212326,0.7531752586364746,0.788374125957489,0.464335173368454,0.8949795961380005,0.9577494859695435,0.9452335834503174,0.951599657535553,0.7713225483894348,0.04682831093668938,0.12560681998729706,0.8753226399421692,0.9071928262710571,0.3477814495563507,0.0735224112868309,0.8862327933311462,0.09767420589923859,0.4398779571056366,0.0657854825258255,0.882890522480011,0.049432359635829926,0.7769089341163635,0.9421652555465698,0.5084729194641113,0.8559197187423706,0.8673926591873169,0.056807782500982285,0.835658073425293,0.878646969795227,0.5347926616668701,0.8478575944900513,0.051395341753959656,0.9469709992408752,0.05152153596282005,0.4290662705898285,0.09105200320482254,0.07524540275335312,0.9688785076141357,0.884742021560669,0.29753297567367554,0.8906642198562622,0.7938673496246338,0.06651300191879272,0.16867515444755554,0.9684572219848633,0.9009934663772583,0.38021084666252136,0.42020559310913086,0.8817282319068909,0.4687666893005371,0.4070727825164795,0.1606479436159134,0.960281491279602,0.9659401774406433,0.9120995402336121,0.9271305799484253,0.10207000374794006,0.160027876496315,0.0738179087638855,0.0944400355219841,0.8645737171173096,0.7684723734855652,0.9311044812202454,0.2677207887172699,0.950743556022644,0.9392867088317871,0.0872737243771553,0.8254914879798889,0.08615461736917496,0.7649087309837341,0.9133500456809998,0.9461485743522644,0.3344389498233795,0.8929979801177979,0.8326436877250671,0.79591304063797,0.39932388067245483,0.8697435855865479,0.5299418568611145,0.05579005181789398,0.05289741978049278,0.9243901968002319,0.9661120772361755,0.9426066279411316,0.9351360201835632,0.15555159747600555,0.8789267539978027,0.2781098484992981,0.9174623489379883,0.7524051666259766,0.8490431904792786,0.7925418615341187,0.0506882481276989,0.1026267409324646,0.8455237150192261,0.060276422649621964,0.24841076135635376,0.532096803188324,0.06745217740535736,0.3636438846588135,0.738609790802002,0.17066770792007446,0.6374011635780334,0.8738792538642883,0.9180273413658142,0.15141861140727997,0.8442221879959106,0.08777330070734024,0.7032155394554138,0.7283612489700317,0.7907013893127441,0.9147962927818298,0.483273983001709,0.7326121926307678,0.15679490566253662,0.23638686537742615,0.6223955750465393,0.05892515555024147,0.6103231906890869,0.9597927927970886,0.34214237332344055,0.13543762266635895,0.9169808030128479,0.9124013781547546,0.1427617371082306,0.9444532990455627,0.04069174453616142,0.7504640817642212,0.6142399311065674,0.9418036937713623,0.14362195134162903,0.9558125734329224,0.10684502124786377,0.7046923041343689,0.1264585554599762,0.047530677169561386,0.91510409116745,0.8286635875701904,0.7521408796310425,0.08001682907342911,0.9413414597511292,0.13272817432880402,0.05907902494072914,0.128291055560112,0.4002157151699066,0.35198289155960083,0.9310389757156372,0.18762661516666412,0.9537710547447205,0.8967505097389221,0.05656983703374863,0.18274475634098053,0.058831989765167236,0.7078648805618286,0.06270206719636917,0.49059727787971497,0.9464336633682251,0.10465537756681442,0.7695614695549011,0.8369042873382568,0.18559573590755463,0.5664185881614685,0.9148639440536499,0.39757591485977173,0.06207308918237686,0.12443456798791885,0.9763500094413757,0.819335401058197,0.31368115544319153,0.6449428796768188,0.23852436244487762,0.879755437374115,0.09529099613428116,0.9541775584220886,0.8904914855957031,0.10893438011407852,0.8957547545433044,0.8374505043029785,0.9726020693778992,0.7146766185760498,0.6141398549079895,0.17929613590240479,0.8387801051139832,0.6043639779090881,0.8113516569137573,0.9853556752204895,0.08084917813539505,0.40754249691963196,0.0694785788655281,0.38532009720802307,0.8369805216789246,0.8491550087928772,0.1570885330438614,0.044212013483047485,0.07451018691062927,0.8256165981292725,0.17060473561286926,0.07288406044244766,0.9611305594444275,0.40147069096565247,0.7884632349014282,0.10026892274618149,0.6081586480140686,0.42590466141700745,0.8568853735923767,0.1926935911178589,0.5207059383392334,0.06460113078355789,0.6031097769737244,0.06117284670472145,0.9412423968315125,0.8818057775497437,0.18701109290122986,0.8695022463798523,0.09427580237388611,0.8223866820335388,0.748016357421875,0.908536434173584,0.9359210729598999,0.6001187562942505,0.7381746768951416,0.6551400423049927,0.9434144496917725,0.38831210136413574,0.11279447376728058,0.9357694387435913,0.04605508968234062,0.7208983302116394,0.15716306865215302,0.5477830767631531,0.9260399341583252,0.7415449023246765,0.0501028411090374,0.8864037990570068,0.9119580984115601,0.6394118666648865,0.7462170124053955,0.06661861389875412,0.08348967134952545,0.9341460466384888,0.6765541434288025,0.5431535840034485,0.9445835947990417,0.057310689240694046,0.7389031648635864,0.9271435737609863,0.9652634859085083,0.9466570019721985,0.20891596376895905,0.48310136795043945,0.18034744262695312,0.645107090473175,0.7476393580436707,0.8439081311225891,0.8234414458274841,0.9590114951133728,0.19773536920547485,0.24527215957641602,0.7610089778900146,0.12256307154893875,0.07020494341850281,0.9148247241973877,0.25555551052093506,0.8386479616165161,0.7933417558670044,0.15357749164104462,0.8478538990020752,0.4868239164352417,0.9668654799461365,0.8910195827484131,0.8378349542617798,0.294893354177475,0.5554808378219604,0.6329167485237122,0.26589328050613403,0.2080051451921463,0.06346342712640762,0.48180535435676575,0.9338594079017639,0.9133750200271606,0.939594566822052,0.8998050689697266,0.9352743029594421,0.8372474312782288,0.5800704956054688,0.12134118378162384,0.22939908504486084,0.054756585508584976,0.7762144207954407,0.05315162613987923,0.1820334792137146,0.7428171038627625,0.27095845341682434,0.9288747906684875,0.540243923664093,0.9069194793701172,0.9402177929878235,0.05991613492369652,0.3097062408924103,0.9591966271400452,0.776609480381012,0.946892499923706,0.9379061460494995,0.3588322401046753,0.04652580991387367,0.8513222932815552,0.9567942023277283,0.9365211129188538,0.10146526247262955,0.11156592518091202,0.06297130882740021,0.7129422426223755,0.8289264440536499,0.0978359505534172,0.5192147493362427,0.9012952446937561,0.2701217830181122,0.31434640288352966,0.151628315448761,0.9314212799072266,0.9626722931861877,0.924365758895874,0.0710282251238823,0.825163722038269,0.18689100444316864,0.10959228128194809,0.7989249229431152,0.08189743012189865,0.8665435910224915,0.13083191215991974,0.9361076951026917,0.05592901632189751,0.9785090088844299,0.09309247881174088,0.7538374066352844,0.5837388038635254,0.940809428691864,0.8560743927955627,0.8727018237113953,0.8722572922706604,0.19256317615509033,0.9738581776618958,0.0516432449221611,0.17181475460529327,0.6470927596092224,0.7619705200195312,0.06719350069761276,0.05569757521152496,0.860224723815918,0.09886189550161362,0.8291257619857788,0.4956566393375397,0.920520544052124,0.2084575891494751,0.5743658542633057,0.9484492540359497,0.15365883708000183,0.775488555431366,0.8561397790908813,0.787706196308136,0.30677440762519836,0.8552928566932678,0.1987571120262146,0.801331102848053,0.06793152540922165,0.9404851198196411,0.7160624265670776,0.9469279050827026,0.13345809280872345,0.442157506942749,0.08188409358263016,0.8637924194335938,0.6685880422592163,0.05442613735795021,0.8610696792602539,0.6739864349365234,0.3874923288822174,0.04859668016433716,0.9384490847587585,0.14936037361621857,0.8968279957771301,0.9157925248146057,0.4027458727359772,0.5510004162788391,0.0931038111448288,0.07852045446634293,0.3974081873893738,0.0849258154630661,0.8550891280174255,0.84912109375,0.9409250020980835,0.949938952922821,0.961517333984375,0.1498233526945114,0.7077810168266296,0.9362486600875854,0.8442588448524475,0.8273375034332275,0.9065294861793518,0.8656604290008545,0.3969230651855469,0.9134256839752197,0.9403350949287415,0.11160767823457718,0.9603689908981323,0.40966880321502686,0.07165078818798065,0.45047590136528015,0.4081386625766754,0.9534816741943359,0.06195290759205818,0.9304134845733643,0.07099467515945435,0.0661480501294136,0.9054683446884155,0.9045064449310303,0.284624308347702,0.8688107132911682,0.5052328705787659,0.921850860118866,0.9600608944892883,0.3341192603111267,0.28937482833862305,0.19646301865577698,0.1923293173313141,0.10219164192676544,0.05056522786617279,0.15764978528022766,0.4103583097457886,0.5153073668479919,0.4224603772163391,0.35501325130462646,0.9202693700790405,0.6290051341056824,0.08550739288330078,0.9300270676612854,0.2667844295501709,0.654553234577179,0.8598834276199341,0.8987990021705627,0.07606564462184906,0.5729049444198608,0.14678891003131866,0.06593304127454758,0.6627298593521118,0.8646998405456543,0.06318028271198273,0.6180554032325745,0.7170581817626953,0.9500036239624023,0.48295825719833374,0.904757559299469,0.9030317664146423,0.06644544005393982,0.7087639570236206,0.062022458761930466,0.11760295927524567,0.07993821799755096,0.15848146378993988,0.2008272260427475,0.5096403956413269,0.6187102198600769,0.5760621428489685],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"y"}},"legend":{"tracegroupgap":0},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ecbdd84e-604b-4eb1-9306-ef14fc04ad80');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">px.histogram(y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_tilde_approx)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="0decf4eb-1ede-4fc3-8740-6aab6bd7bb1c" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("0decf4eb-1ede-4fc3-8740-6aab6bd7bb1c")) {                    Plotly.newPlot(                        "0decf4eb-1ede-4fc3-8740-6aab6bd7bb1c",                        [{"alignmentgroup":"True","bingroup":"x","hovertemplate":"variable=0\u003cbr\u003evalue=%{x}\u003cbr\u003ecount=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"0","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"0","offsetgroup":"0","orientation":"v","showlegend":true,"x":[-8.1673264503479e-05,-1.1026859283447266e-05,3.159046173095703e-06,6.139278411865234e-06,8.082389831542969e-05,1.0251998901367188e-05,-4.3548643589019775e-06,-2.516806125640869e-05,2.384185791015625e-07,6.377696990966797e-06,9.79304313659668e-05,-4.645437002182007e-05,-5.207955837249756e-05,-2.609938383102417e-05,-0.00013108551502227783,9.5367431640625e-06,-2.028048038482666e-05,-7.437914609909058e-05,-4.76837158203125e-07,-9.804964065551758e-05,-8.165836334228516e-06,5.4836273193359375e-06,-8.66129994392395e-05,-2.5622546672821045e-05,-0.0001176297664642334,-4.979968070983887e-05,-3.3795833587646484e-05,4.851818084716797e-05,-1.0102987289428711e-05,-5.340576171875e-05,-7.450580596923828e-06,-2.259761095046997e-05,-4.735216498374939e-05,-3.528594970703125e-05,5.4836273193359375e-06,-1.5527009963989258e-05,-4.1425228118896484e-06,-2.1219253540039062e-05,-0.0002646893262863159,-3.2007694244384766e-05,-2.7567148208618164e-05,-4.488229751586914e-05,-3.337860107421875e-05,3.2186508178710938e-06,1.430511474609375e-05,-8.32974910736084e-05,5.364418029785156e-06,3.325939178466797e-05,-0.0001640915870666504,-1.245737075805664e-05,-7.697939872741699e-05,-0.00010003149509429932,3.3736228942871094e-05,3.540515899658203e-05,-4.845857620239258e-05,-7.092952728271484e-06,-1.1175870895385742e-06,3.832578659057617e-05,-4.70280647277832e-05,4.2378902435302734e-05,3.612041473388672e-05,5.245208740234375e-05,-0.00016236305236816406,-2.428889274597168e-05,2.1338462829589844e-05,2.4497509002685547e-05,7.152557373046875e-06,-3.331899642944336e-05,-3.427267074584961e-07,-5.412101745605469e-05,-0.0001723766326904297,-6.51925802230835e-06,-1.055002212524414e-05,-6.67572021484375e-05,-1.093745231628418e-05,-2.384185791015625e-07,-5.821138620376587e-05,-4.176795482635498e-05,0.00017002224922180176,-1.35079026222229e-05,2.8014183044433594e-06,8.52346420288086e-06,1.7762184143066406e-05,-2.477318048477173e-06,1.0371208190917969e-05,-0.0004303455352783203,1.6093254089355469e-06,0.00014322996139526367,-9.59634780883789e-06,2.086162567138672e-05,-7.075071334838867e-05,-1.662224531173706e-05,-3.540515899658203e-05,6.020069122314453e-06,-9.611248970031738e-07,3.516674041748047e-06,-2.8014183044433594e-06,4.827976226806641e-06,-2.1755695343017578e-05,-9.227544069290161e-06,-5.900859832763672e-06,-3.926455974578857e-06,-7.742643356323242e-05,-8.169561624526978e-05,-1.3172626495361328e-05,-4.155188798904419e-05,-2.562999725341797e-05,1.6033649444580078e-05,0.00011152029037475586,-3.5293400287628174e-05,0.0001150369644165039,-4.61190938949585e-06,-5.602836608886719e-06,-2.5525689125061035e-05,4.872679710388184e-06,-3.788992762565613e-05,-3.981590270996094e-05,6.0677528381347656e-05,-1.0505318641662598e-05,-7.0892274379730225e-06,1.4901161193847656e-06,-3.30805778503418e-05,-4.649162292480469e-06,2.7954578399658203e-05,-1.1973083019256592e-05,-4.279613494873047e-05,-5.953013896942139e-06,8.344650268554688e-07,-8.535385131835938e-05,7.387995719909668e-05,-2.7135014533996582e-05,-4.7266483306884766e-05,-0.00013059377670288086,-1.7881393432617188e-07,-7.808208465576172e-05,-0.0001138448715209961,-1.496821641921997e-05,2.3245811462402344e-05,2.950429916381836e-05,-1.424551010131836e-05,-5.429983139038086e-05,2.0503997802734375e-05,-3.057718276977539e-05,-5.2381306886672974e-05,-2.022460103034973e-05,-5.4836273193359375e-06,4.369020462036133e-05,2.9206275939941406e-06,-1.55717134475708e-06,-0.00011493265628814697,1.8715858459472656e-05,-0.00013659149408340454,4.9591064453125e-05,1.1086463928222656e-05,-2.633035182952881e-05,-1.1920928955078125e-05,-5.863606929779053e-05,2.980232238769531e-07,-1.1444091796875e-05,1.9669532775878906e-06,-0.00021117925643920898,8.761882781982422e-06,-2.736598253250122e-05,9.417533874511719e-06,-9.98377799987793e-05,-9.104609489440918e-05,-7.987022399902344e-06,-2.405792474746704e-05,0.00013488531112670898,3.5762786865234375e-07,-8.106231689453125e-06,-5.4001808166503906e-05,0.000217437744140625,-1.0207295417785645e-06,-8.59573483467102e-05,-2.568960189819336e-05,-8.881092071533203e-06,1.7881393432617188e-07,-7.036328315734863e-05,2.1606683731079102e-07,-8.410215377807617e-05,4.297494888305664e-05,2.154707908630371e-05,5.960464477539062e-07,3.0994415283203125e-06,-1.3709068298339844e-06,-1.055002212524414e-05,0.00016456842422485352,-2.6479363441467285e-05,-6.635487079620361e-05,-2.2590160369873047e-05,7.152557373046875e-06,0.00012791156768798828,-3.948807716369629e-07,5.346536636352539e-05,-1.8104910850524902e-06,1.3232231140136719e-05,-1.2218952178955078e-06,-1.5676021575927734e-05,-1.9680708646774292e-05,1.5139579772949219e-05,6.67572021484375e-06,5.4836273193359375e-05,2.3484230041503906e-05,-3.2782554626464844e-05,-4.251301288604736e-05,0.00022304058074951172,-3.1113624572753906e-05,5.841255187988281e-06,1.9609928131103516e-05,-3.978610038757324e-06,2.3245811462402344e-06,-2.8442591428756714e-05,-0.00011053681373596191,-3.627687692642212e-05,-8.096545934677124e-05,-2.7418136596679688e-06,3.463029861450195e-05,-0.00013586878776550293,-3.5762786865234375e-07,-6.002187728881836e-05,-1.1354684829711914e-05,-1.7151236534118652e-05,-1.5676021575927734e-05,-1.4007091522216797e-05,-4.32431697845459e-05,1.5497207641601562e-06,0.00018596649169921875,-0.0001729428768157959,-6.961822509765625e-05,-6.571412086486816e-06,1.1086463928222656e-05,-3.5762786865234375e-07,-8.106231689453125e-06,-2.664327621459961e-05,-4.708021879196167e-05,-0.000141829252243042,-6.355345249176025e-06,9.134411811828613e-06,2.2411346435546875e-05,-0.00011259317398071289,2.682209014892578e-06,-4.8488378524780273e-05,1.0192394256591797e-05,-4.827976226806641e-06,-9.37730073928833e-05,7.748603820800781e-05,-2.9809772968292236e-05,0.00011903047561645508,1.4901161193847656e-06,3.0994415283203125e-06,-0.00012320280075073242,2.6524066925048828e-05,1.2993812561035156e-05,3.802776336669922e-05,-5.0067901611328125e-06,6.633996963500977e-05,4.2438507080078125e-05,-2.7604401111602783e-06,-3.100186586380005e-05,1.1920928955078125e-07,-7.033348083496094e-06,1.2814998626708984e-05,-8.225440979003906e-06,-7.203221321105957e-05,5.424022674560547e-06,-1.6689300537109375e-05,-7.68899917602539e-06,-1.8477439880371094e-05,-5.6624412536621094e-05,0.0002925395965576172,-7.450580596923828e-07,-5.391240119934082e-05,-3.898143768310547e-05,-2.6952475309371948e-05,2.5019049644470215e-05,0.00021839141845703125,-3.330409526824951e-06,3.281235694885254e-05,-1.3709068298339844e-06,-4.57763671875e-05,3.325939178466797e-05,-1.823902130126953e-05,-9.059906005859375e-06,-0.00010643899440765381,-3.641843795776367e-05,-1.2218952178955078e-06,-0.00032955408096313477,5.322694778442383e-05,2.3245811462402344e-05,-3.701448440551758e-05,-8.428096771240234e-05,9.936094284057617e-05,-9.59932804107666e-05,-2.3692846298217773e-06,-1.1682510375976562e-05,-3.7401914596557617e-06,3.2007694244384766e-05,5.781650543212891e-06,4.559755325317383e-06,-0.00010232627391815186,8.821487426757812e-06,0.00023156404495239258,-0.00010335445404052734,7.3909759521484375e-06,-4.628673195838928e-05,1.2934207916259766e-05,-4.208087921142578e-05,0.00011646747589111328,-7.253885269165039e-05,-5.424022674560547e-06,-9.998679161071777e-06,2.7000904083251953e-05,-0.00013375282287597656,-5.0362199544906616e-05,-1.6987323760986328e-05,-3.141164779663086e-05,9.822845458984375e-05,-2.3052096366882324e-05,1.9073486328125e-06,-2.6971101760864258e-06,-4.659965634346008e-05,-2.5078654289245605e-05,2.384185791015625e-07,-4.139542579650879e-05,1.5676021575927734e-05,-7.633864879608154e-05,6.9141387939453125e-06,3.5703182220458984e-05,-3.4868717193603516e-06,-2.4139881134033203e-06,-1.564621925354004e-07,6.93202018737793e-05,-4.246830940246582e-06,-0.00010207295417785645,-7.212162017822266e-06,-3.451108932495117e-05,0.0001646876335144043,7.18832015991211e-05,-7.289648056030273e-05,4.214048385620117e-05,-2.562999725341797e-06,0.00018072128295898438,-5.136430263519287e-05,-4.3354928493499756e-05,5.125999450683594e-06,-2.5212764739990234e-05,-0.00010603666305541992,8.404254913330078e-06,3.7223100662231445e-05,5.0067901611328125e-06,-3.501772880554199e-07,-1.8477439880371094e-06,6.4373016357421875e-06,-4.076957702636719e-05,2.8908252716064453e-05,-2.2530555725097656e-05,-2.2649765014648438e-06,-5.960464477539062e-07,-2.4616718292236328e-05,-8.493661880493164e-07,-3.4689903259277344e-05,-3.3795833587646484e-05,5.996227264404297e-05,4.291534423828125e-06,-2.2940337657928467e-05,-9.331107139587402e-05,-3.866851329803467e-06,-4.3898820877075195e-05,-9.518861770629883e-05,-3.653764724731445e-05,-9.365379810333252e-05,-2.0839273929595947e-05,-1.4677643775939941e-05,-7.510185241699219e-06,-1.8328428268432617e-05,-2.1383166313171387e-06,5.7220458984375e-06,-0.00011038780212402344,4.172325134277344e-06,-4.7869980335235596e-05,-0.0001386404037475586,-0.00012624263763427734,2.8014183044433594e-06,-1.5348196029663086e-06,6.854534149169922e-06,8.195638656616211e-08,-0.00010520219802856445,-7.779151201248169e-05,-2.0503997802734375e-05,-4.875659942626953e-05,-2.4572014808654785e-05,0.00013828277587890625,-3.14861536026001e-05,8.940696716308594e-06,4.500150680541992e-05,-9.393692016601562e-05,1.0132789611816406e-06,-0.00015306472778320312,9.679794311523438e-05,5.239248275756836e-05,-3.069639205932617e-05,-7.838010787963867e-05,-0.0003336891531944275,1.2516975402832031e-06,2.7194619178771973e-07,9.846687316894531e-05,-2.6151537895202637e-05,8.291006088256836e-05,-3.039836883544922e-06,6.300210952758789e-05,-2.3502856492996216e-05,1.2993812561035156e-05,1.1324882507324219e-06,3.55839729309082e-05,-0.00010132789611816406,-1.7061829566955566e-06,-6.318837404251099e-05,4.887580871582031e-06,0.00018835067749023438,6.99758529663086e-05,0.00010353326797485352,-4.351884126663208e-05,2.9146671295166016e-05,6.198883056640625e-06,-2.6226043701171875e-06,-1.1980533599853516e-05,-7.396936416625977e-05,-4.8726797103881836e-05,-3.1888484954833984e-05,8.702278137207031e-06,2.3066997528076172e-05,7.450580596923828e-06,-4.4345855712890625e-05,6.556510925292969e-07,-4.306435585021973e-06,-8.723139762878418e-05,9.357929229736328e-06,-0.00010936707258224487,-8.99285078048706e-06,2.586841583251953e-05,1.2785196304321289e-05,-2.2470951080322266e-05,-2.4259090423583984e-05,-9.074807167053223e-06,2.8014183044433594e-06,-7.537007331848145e-05,9.119510650634766e-06,-2.9206275939941406e-06,1.2636184692382812e-05,5.066394805908203e-06,3.135204315185547e-05,-3.6954879760742188e-06,-9.223818778991699e-05,-0.00017747282981872559,-0.00011453032493591309,-1.2576580047607422e-05,1.633167266845703e-05,0.00020444393157958984,2.086162567138672e-06,-5.066394805908203e-06,-1.0728836059570312e-06,4.410743713378906e-05,4.357099533081055e-05,-3.0182301998138428e-05,-5.336105823516846e-05,-9.387731552124023e-07,-3.337860107421875e-06,3.2372772693634033e-06,-1.399219036102295e-05,0.00010192394256591797,-0.0001875460147857666,-5.9604644775390625e-06,-0.0015995502471923828,-2.9206275939941406e-06,1.4662742614746094e-05,-2.387911081314087e-05,-0.0001595020294189453,3.814697265625e-06,3.159046173095703e-06,1.049041748046875e-05,7.450580596923828e-06,-4.798173904418945e-06,4.772096872329712e-06,-3.439188003540039e-05,1.9669532775878906e-06,9.715557098388672e-06,-3.481656312942505e-05,-4.947930574417114e-05,-3.670156002044678e-05,0.00022459030151367188,-1.4960765838623047e-05,-8.083879947662354e-05,0.00011092424392700195,-1.8894672393798828e-05,-1.806020736694336e-05,-8.38935375213623e-05,-0.0003657341003417969,1.1324882507324219e-06,-8.52346420288086e-06,6.198883056640625e-06,-1.856684684753418e-05,-1.621246337890625e-05,-1.971423625946045e-05,-1.2151896953582764e-05,0.00020581483840942383,-1.1101365089416504e-06,-3.8623809814453125e-05,-1.895427703857422e-05,1.430511474609375e-06,-5.647540092468262e-06,2.5033950805664062e-06,-7.104873657226562e-05,-6.854534149169922e-06,-1.0132789611816406e-05,-4.76837158203125e-07,-1.633167266845703e-05,1.245737075805664e-05,3.874301910400391e-06,-4.951655864715576e-05,-8.821487426757812e-06,-1.1626631021499634e-05,-3.11732292175293e-05,-5.4955482482910156e-05,-3.7550926208496094e-05,-5.675852298736572e-05,-2.248212695121765e-05,1.704692840576172e-05,-6.133317947387695e-05,8.046627044677734e-06,-3.039836883544922e-06,3.3855438232421875e-05,-4.09930944442749e-05,3.123283386230469e-05,5.364418029785156e-06,-1.722574234008789e-05,5.227327346801758e-05,-5.364418029785156e-06,4.649162292480469e-06,-0.00021716952323913574,1.7285346984863281e-06,-1.8015503883361816e-05,-2.6345252990722656e-05,-1.1406838893890381e-05,-2.4378299713134766e-05,5.6684017181396484e-05,7.033348083496094e-06,-4.357099533081055e-05,-3.165006637573242e-05,-1.4841556549072266e-05,4.470348358154297e-06,2.7477741241455078e-05,-9.01147723197937e-06,1.2218952178955078e-05,0.0002865791320800781,-4.947185516357422e-06,-8.124858140945435e-06,-3.4570693969726562e-06,-1.87605619430542e-05,-4.4226646423339844e-05,8.165836334228516e-06,-0.00024899840354919434,-0.00025266408920288086,-2.0772218704223633e-05,-1.5698373317718506e-05,-7.957220077514648e-06,-8.656084537506104e-05,7.492303848266602e-05,-8.165836334228516e-06,-5.185604095458984e-06,-2.980232238769531e-07,-1.8298625946044922e-05,-2.995133399963379e-05,1.1324882507324219e-06,2.5033950805664062e-06,-0.00010603666305541992,2.5033950805664062e-06,-2.199411392211914e-05,-6.67572021484375e-06,-4.6759843826293945e-05,7.545948028564453e-05,-1.3709068298339844e-06,-3.1210482120513916e-05,1.4424324035644531e-05,-3.072619438171387e-05,-2.0936131477355957e-06,-5.519390106201172e-05,-4.163384437561035e-05,-1.2755393981933594e-05,-6.1355531215667725e-06,1.2516975402832031e-06,-5.9388577938079834e-05,-5.5208802223205566e-05,8.71419906616211e-05,5.1975250244140625e-05,1.9818544387817383e-05,-4.708766937255859e-06,9.882450103759766e-05,4.965066909790039e-05,-1.3947486877441406e-05,-2.1189451217651367e-05,-2.950429916381836e-06,-3.4555792808532715e-05,-2.8401613235473633e-05,-2.180039882659912e-05,-3.6619603633880615e-06,-2.5764107704162598e-05,-9.009242057800293e-05,-6.854534149169922e-05,0.00014734268188476562,-4.6819448471069336e-05,1.6689300537109375e-06,-1.5497207641601562e-05,-3.3602118492126465e-05,-2.5153160095214844e-05,-6.079673767089844e-05,5.418062210083008e-05,1.7285346984863281e-06,9.655952453613281e-06,-5.082041025161743e-05,-3.6954879760742188e-06,-0.00012768805027008057,-2.014636993408203e-05,-1.519918441772461e-05,0.00015145540237426758,-1.3999640941619873e-05,-0.00014644861221313477,6.437301635742188e-05,-3.2186508178710938e-06,8.219480514526367e-05,-1.2993812561035156e-05,8.58306884765625e-06,-3.680586814880371e-06,-5.97834587097168e-05,-1.1589378118515015e-05,4.6737492084503174e-05,-2.2701919078826904e-05,-1.5243887901306152e-05,-4.427134990692139e-05,-2.6166439056396484e-05,-1.990795135498047e-05,-3.2782554626464844e-06],"xaxis":"x","yaxis":"y","type":"histogram"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"value"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"count"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"margin":{"t":60},"barmode":"relative"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('0decf4eb-1ede-4fc3-8740-6aab6bd7bb1c');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>The approximation demonstrates high accuracy in this instance. Now, let’s compare the runtimes:</p>
<div id="cell-19" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>standard_loocv_runtime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>efficient_loocv_runtime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>standard_loocv_runtime<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>efficient_loocv_runtime <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.0f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>standard_loocv_runtime = 4.7e+01
efficient_loocv_runtime = 1.3e-01
standard_loocv_runtime/efficient_loocv_runtime = 357.</code></pre>
</div>
</div>
<p>A significant speedup! However, it’s important to note that there is room for further optimization. For instance, the BFGS iterations could be initialized with a previous solution, or we could <a href="https://tomshlomo.github.io/blog/posts/loocv/loocv_part1.html#python-implementation">utilize JIT compilation as we did in part 1</a>.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>Efficient LOOCV for ordinary least squares and ridge regression is mentioned in several well known books like <a href="https://hastie.su.domains/Papers/ESLII.pdf">The Elements of Statistical Learning</a> and <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>. I first encountered it in a brief mention in <a href="https://egrcc.github.io/docs/math/all-of-statistics.pdf">All of Statistics</a>.</p>
<p>The only reference I am aware of that discusses the general quadratic case, and a similar approach for the non-quadratic approximation, is <a href="https://repository.tudelft.nl/islandora/object/uuid:d9b5456d-722a-401d-9f1a-c530c46d6491/datastream/OBJ/download.#:~:text=The%20most%20important%20cross%2Dvalidation,results%20can%20then%20be%20averaged.">this theses by Rosa Meijer</a>.</p>


</section>

 ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/loocv_part2/loocv_part2.html</guid>
  <pubDate>Fri, 29 Mar 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Efficient leave one out cross validation - part 1</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/loocv/loocv_part1.html</link>
  <description><![CDATA[ 





<p>Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.</p>
<p>Leave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.</p>
<p>For linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it’s <code>RidgeCV</code> estimator. Notably, this same trick extends beyond these linear models to any quadratically regularized least squares regression — a fact not widely recognized.</p>
<p>Taking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.</p>
<p>In this initial segment, we derive efficient LOOCV for the quadratic scenario and demonstrate its implementation in Python.</p>
<p>In part 2, we will build upon this derivation to cover non-quadratic scenarios and showcase these findings with a practical example dataset.</p>
<section id="notation" class="level1">
<h1>Notation</h1>
<p>We denote the number of samples in the training dataset as <img src="https://latex.codecogs.com/png.latex?n">.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?m">-dimensional feature vectors are represented as <img src="https://latex.codecogs.com/png.latex?x_1"> to <img src="https://latex.codecogs.com/png.latex?x_n">, forming the rows of matrix <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>Targets are denoted as <img src="https://latex.codecogs.com/png.latex?y_1"> to <img src="https://latex.codecogs.com/png.latex?y_n">, forming the vector <img src="https://latex.codecogs.com/png.latex?y">. The model’s prediction for the <img src="https://latex.codecogs.com/png.latex?i">-th training sample is <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%20=%20x_i%5ET%20%5Ctheta">, where <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the coefficients vector. <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20=%20X%20%5Ctheta"> represents the vector containing all predictions.</p>
<p>We fit <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to the training data by minimizing the combined loss and regularization terms: <span id="eq-theta-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20:=%20%5Carg%5Cmin_%7B%5Ctheta'%7D%20f(%5Ctheta').%0A%5Ctag%7B1%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%0Af(%5Ctheta')%20:=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20l(x_i%5ET%20%5Ctheta';%20y_i)%20+%20r(%5Ctheta').%0A"> Here, <img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)"> represents the loss function, quantifying the difference between the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> and the true target <img src="https://latex.codecogs.com/png.latex?y_i">, while <img src="https://latex.codecogs.com/png.latex?r"> is the regularization function. We assume <img src="https://latex.codecogs.com/png.latex?l"> (as a function of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">) and <img src="https://latex.codecogs.com/png.latex?r"> are convex and twice differentiable. Special cases of this model include ordinary least squares (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2">, <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta')%20=%200">), ridge regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2">, <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta')%20=%20%5Calpha%20%5C%7C%20%5Ctheta'%20%5C%7C%5E2">), logistic regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;y_i)%20=%20%5Clog%20%5Cleft(%201%20+%20e%5E%7B-y_i%20%5Chat%7By%7D_i%7D%5Cright)"> with <img src="https://latex.codecogs.com/png.latex?y_i%20%5Cin%20%5C%7B%20-1,%201%5C%7D">), and Poisson regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;y_i)%20=%20y_i%20%5Chat%7By%7D_i%20-%20e%5E%7B%5Chat%7By%7D_i%7D">).</p>
<p>To denote the coefficients obtained by excluding the <img src="https://latex.codecogs.com/png.latex?j">-th example, we use <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%5E%7B(j)%7D%20=%20%5Carg%5Cmin_%7B%5Ctheta'%7D%20f%5E%7B(j)%7D%20(%5Ctheta')%0A"> where <img src="https://latex.codecogs.com/png.latex?%20f%5E%7B(j)%7D(%5Ctheta')%20:=%20%5Csum_%7Bi%20%5Cneq%20j%7D%20l(x_i%5ET%20%5Ctheta';%20y_i)%20+%20r(%5Ctheta')%20"> Similarly, <img src="https://latex.codecogs.com/png.latex?X%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?y%5E%7B(j)%7D">, represent <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> with the <img src="https://latex.codecogs.com/png.latex?j">-th row removed, respectively. We denote by <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> the predicted label for sample <img src="https://latex.codecogs.com/png.latex?j"> when it is left out: <span id="eq-y-tilde-j-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7By%7D_j%20:=%20x_j%20%5ET%20%5Ctheta%5E%7B(j)%7D%0A%5Ctag%7B2%7D"></span> Our goal is calculating <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">, for all <img src="https://latex.codecogs.com/png.latex?j">, efficiently.</p>
</section>
<section id="deriving-efficient-loocv-for-the-quadratic-case" class="level1">
<h1>Deriving efficient LOOCV for the quadratic case</h1>
<p>In scenarios where the loss function is the sum of squares loss, <img src="https://latex.codecogs.com/png.latex?%0Al(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2,%0A"> and the regularizer is quadratic <img src="https://latex.codecogs.com/png.latex?%0Ar(%5Ctheta')%20=%20%5Ctheta'%5ET%20R%20%5Ctheta'%0A"> where <img src="https://latex.codecogs.com/png.latex?R"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20m"> semi-positive definite matrix, the solution to the optimization problem Equation&nbsp;1 is obtained by solving the linear equation <sup>1</sup>: <span id="eq-theta-solve"><img src="https://latex.codecogs.com/png.latex?%0AA%20%5Ctheta%20=%20b.%0A%5Ctag%7B3%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20A%20&amp;:=%20X%5ET%20X%20+%20R%20%5C%5C%0A%20%20%20%20b%20&amp;:=%20X%5ET%20y.%0A%5Cend%7Balign*%7D"></p>
<p>Similarly, obtaining <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> requires solving <span id="eq-theta-j-solve"><img src="https://latex.codecogs.com/png.latex?%0AA%5E%7B(j)%7D%20%5Ctheta%5E%7B(j)%7D%20=%20b%5E%7B(j)%7D.%0A%5Ctag%7B4%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20A%5E%7B(j)%7D%20&amp;:=%20X%5E%7B(j)T%7D%20X%5E%7B(j)%7D%20+%20R%20%5C%5C%0A%20%20%20%20b%5E%7B(j)%7D%20&amp;:=%20X%5E%7B(j)T%7D%20y%5E%7B(j)%7D.%0A%5Cend%7Balign*%7D"> Forming and solving Equation&nbsp;4 for each <img src="https://latex.codecogs.com/png.latex?j"> has a time complexity of <img src="https://latex.codecogs.com/png.latex?O(m%5E3%20+%20n%20m%5E2)">. Thus, in a naive implementation, the overall complexity of LOOCV becomes <img src="https://latex.codecogs.com/png.latex?O(n%20m%5E3%20+%20n%5E2%20m%5E2)">, posing a significant computational challenge, particularly when <img src="https://latex.codecogs.com/png.latex?n"> is large.</p>
<p>Efficient LOOCV leverages the solution for Equation&nbsp;3 to calculate the solution for Equation&nbsp;4. We exploit the idea from computational linear algebra that solving multiple <img src="https://latex.codecogs.com/png.latex?m"> by <img src="https://latex.codecogs.com/png.latex?m"> equations with the same matrix has a time complexity similar to solving a single such equation. Thus, we solve, in addition to Equation&nbsp;3, the following <img src="https://latex.codecogs.com/png.latex?n"> equations: <img src="https://latex.codecogs.com/png.latex?%0AA%20t_j%20=%20x_j.%0A"></p>
<!-- The key idea behind efficient LOOCV lies in leveraging the solution for @eq-theta-solve to calculate the solution for @eq-theta-j-solve.
We will utilize an important idea from computational linear algebra: 
even though the complexity of solving a single $m$ by $m$ equation is $O(m^3)$, the complexity of solving $n$ such equations is not $O(nm^3)$, but $O(m^3 + nm^2)$, if all the equations share the same matrix. -->
<!-- the time required to solve multiple $m$ by $m$ equations that share the same matrix is almost identical to the time it takes to solve a single $m$ by $m$ equation. -->
<!-- Specifically, we will solve, in additional to @eq-theta-solve, the following $n$ equations: -->
<p>We start by noting that <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AX%5ETX%20&amp;=%20X%5E%7B(j)%5ET%7D%20X%5E%7B(j)%7D%20+%20x_j%20x_j%5ET%20%20%20%20%5C%5C%0AX%5ETy%20&amp;=%20X%5E%7B(j)%5ET%7D%20y%5E%7B(j)%7D%20+%20x_j%20y_j,%0A%5Cend%7Balign*%7D"> so we can write Equation&nbsp;4 like so: <img src="https://latex.codecogs.com/png.latex?%0A(A%20-%20x_j%20x_j%5ET)%20%5Ctheta%5E%7B(j)%7D%20=%20b%20-%20x_j%20y_j.%0A"> The usual way forward involves employing Sherman-Morrison formula, solving for <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> and substituting it in Equation&nbsp;2 to obtain an expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D">. However, there’s a better approach <sup>2</sup>: We rewrite Equation&nbsp;4 as <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20A%20%5Ctheta%5E%7B(j)%7D%20-%20x_j%20%5Ctilde%7By%7D_j%20&amp;=%20b%20-%20x_j%20y_j%20%5C%5C%0A%20%20%20%20%5Ctilde%7By%7D_j%20&amp;=%20x_j%20%5ET%20%5Ctheta%5E%7B(j)%7D%0A%5Cend%7Balign*%7D"> so instead of a single equation with one unknown (<img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D">), we now have two equations with two unknowns (<img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">). At first this seems more complicated, but notice that since the coefficient of <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> in the first equation is <img src="https://latex.codecogs.com/png.latex?A">, we can eliminate it: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Ctheta%5E%7B(j)%7D%20%20&amp;=%20A%5E%7B-1%7D%20(%20b%20-%20x_j%20y_j%20+%20x_j%20%5Ctilde%7By%7D_j%20)%20%5C%5C%0A&amp;=%20%5Ctheta%20-%20t_j%20(%20%20y_j%20-%20%5Ctilde%7By%7D_j%20)%0A%5Cend%7Balign*%7D"> substituting in the bottom equation, we can solve for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Ctilde%7By%7D_j%20&amp;=%20x_j%20%5ET%20%5Cleft(%20%5Ctheta%20-%20t_j%20(%20%20y_j%20-%20%5Ctilde%7By%7D_j%20)%20%5Cright)%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;=%20%5Chat%7By%7D_j%20-%20h_j%20(y_j%20-%20%5Ctilde%7By%7D_j)%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;=%20%5Cfrac%7B%5Chat%7By%7D_j%20-%20h_j%20y_j%7D%7B1-h_j%7D%0A%25%20%5C%5C%0A%25%20%5Ctilde%7By%7D_j%20&amp;=%20%5Cfrac%7B%5Chat%7By%7D_j%20-h_j%20%5Chat%7By%7D_j%20+%20h_j%20%5Chat%7By%7D_j%20-%20h_j%20y_j%7D%7B1-h_j%7D%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;=%20%5Chat%7By%7D_j%20+%20%5Cfrac%7Bh_j%20%7D%7B1-h_j%7D%20%5Cleft(%20%5Chat%7By%7D_j%20-%20y_j%20%5Cright)%0A%25%20%5C%5C%0A%25%20%5Ctilde%7By%7D_j%20&amp;=%20%5Cfrac%7B%5Chat%7By%7D_j%20-%20y_j%7D%7B1-h_j%7D%20+%20y_j%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?%0Ah_j%20:=%20x_j%20%5ET%20t_j.%0A"></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminder
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?y_j"> is the true label.<br>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_j"> is the prediction using all the data.<br>
<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> is the leave-one-out prediction.</p>
</div>
</div>
<p>That’s it! we got an expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> that doesn’t require inverting any matrix other than <img src="https://latex.codecogs.com/png.latex?A">. It also has a nice interpretation: the difference between the prediction and the LOO prediction is the difference between the prediction an the true label, “amplified” by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bh_j%20%7D%7B1-h_j%7D">.</p>
</section>
<section id="python-implementation" class="level1">
<h1>Python implementation</h1>
<p>The approach outlined above adapts seamlessly into code. We’ll construct an estimator resembling the sklearn style, featuring standard fit and predict methods, alongside a function to compute <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D">, the leave-one-out predictions:</p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Self</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scipy</span>
<span id="cb1-5"></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> LinearRegressionWithQuadraticRegularization:</span>
<span id="cb1-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> R</span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb1-12">        A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R</span>
<span id="cb1-13">        b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb1-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.solve(</span>
<span id="cb1-15">            A,</span>
<span id="cb1-16">            b,</span>
<span id="cb1-17">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-18">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-19">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb1-20">        )</span>
<span id="cb1-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb1-22"></span>
<span id="cb1-23">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb1-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_</span>
<span id="cb1-25"></span>
<span id="cb1-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit_loocv_predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb1-27">        A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R</span>
<span id="cb1-28">        b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb1-29">        temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.solve(</span>
<span id="cb1-30">            A,</span>
<span id="cb1-31">            np.vstack([b, X]).T,</span>
<span id="cb1-32">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-33">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-34">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb1-35">        )</span>
<span id="cb1-36">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-37">        t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb1-38">        h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ij,ji-&gt;i"</span>, X, t)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h[i] = np.dot(X[i, :], t[:, i])</span></span>
<span id="cb1-39">        y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict(X)</span>
<span id="cb1-40">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> h)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y)</span></code></pre></div>
</div>
<p>Let’s check that our method for calculating the leave-one-out predictions is correct on random data, and compare it’s run time to the usual leave-one-out procedure.</p>
<div id="cell-7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LeaveOneOut</span>
<span id="cb2-2"></span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> standard_loocv(model, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb2-5">    y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty_like(y)</span>
<span id="cb2-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i, (train_index, test_index) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(LeaveOneOut().split(X)):</span>
<span id="cb2-7">        X_loo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[train_index, :]</span>
<span id="cb2-8">        y_loo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[train_index]</span>
<span id="cb2-9">        model.fit(X_loo, y_loo)</span>
<span id="cb2-10">        y_tilde[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X[test_index, :])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_tilde</span>
<span id="cb2-12"></span>
<span id="cb2-13"></span>
<span id="cb2-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> gen_random_data(n: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, m: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb2-15">    rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb2-16">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.standard_normal((n, m))</span>
<span id="cb2-17">    L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.standard_normal((m, m))</span>
<span id="cb2-18">    theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> rng.standard_normal(m)</span>
<span id="cb2-19">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rng.standard_normal(n)</span>
<span id="cb2-20">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> L.T  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># random positive definite matrix</span></span>
<span id="cb2-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X, y, R</span>
<span id="cb2-22"></span>
<span id="cb2-23"></span>
<span id="cb2-24">X, y, R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gen_random_data(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-25">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegressionWithQuadraticRegularization(R<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>R)</span>
<span id="cb2-26"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(</span>
<span id="cb2-27">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"max absolute error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(model.fit_loocv_predict(X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> standard_loocv(model, X, y)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb2-28">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>max absolute error: 1.243e-14</code></pre>
</div>
</div>
<p>Good, the two methods to calculate <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D"> give the same result. Let’s also compare the runtime:</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit model.fit_loocv_predict(X, y) </span>
<span id="cb4-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit standard_loocv(model, X, y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>34.6 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
2.39 ms ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</code></pre>
</div>
</div>
<p>Nice, a significant speedup. But that’s quite fast to begin with. Let’s increase <code>n</code> and <code>m</code>:</p>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">X, y, R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gen_random_data(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb6-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegressionWithQuadraticRegularization(R<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>R)</span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'max absolute error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(model.fit_loocv_predict(X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> standard_loocv(model, X, y)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb6-4"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit model.fit_loocv_predict(X, y) </span>
<span id="cb6-5"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit standard_loocv(model, X, y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>max absolute error: 8.527e-14
138 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
The slowest run took 4.24 times longer than the fastest. This could mean that an intermediate result is being cached.
822 ms ± 461 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p>Hmm… Much less impressive. In theory the speedup should improve as the problem size increases. This is likely due to some python inefficiencies, not the algorithm itself. Let’s try to improve by using JAX’s just-in-time compilation feature:</p>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> jax</span>
<span id="cb8-2"></span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> JitLinearRegressionWithQuadraticRegularization:</span>
<span id="cb8-5">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb8-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> R</span>
<span id="cb8-7"></span>
<span id="cb8-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb8-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._fit(X, y, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R)</span>
<span id="cb8-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb8-11"></span>
<span id="cb8-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._predict(X, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_)</span>
<span id="cb8-14"></span>
<span id="cb8-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit_loocv_predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-16">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_, y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._fit_loocv_predict(X, y, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R)</span>
<span id="cb8-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_tilde</span>
<span id="cb8-18">    </span>
<span id="cb8-19">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb8-20">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@jax.jit</span></span>
<span id="cb8-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _fit(X, y, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> jax.scipy.linalg.solve(</span>
<span id="cb8-23">            X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> R, </span>
<span id="cb8-24">            X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y,</span>
<span id="cb8-25">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-26">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-27">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb8-28">        )</span>
<span id="cb8-29"></span>
<span id="cb8-30">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb8-31">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@jax.jit</span></span>
<span id="cb8-32">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _predict(X, theta) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta</span>
<span id="cb8-34"></span>
<span id="cb8-35">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb8-36">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@jax.jit</span></span>
<span id="cb8-37">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _fit_loocv_predict(X, y, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-38">        temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.scipy.linalg.solve(</span>
<span id="cb8-39">            X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> R,</span>
<span id="cb8-40">            jax.numpy.vstack([X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y, X]).T,</span>
<span id="cb8-41">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-42">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-43">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb8-44">        )</span>
<span id="cb8-45">        theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-46">        t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb8-47">        h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.numpy.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ij,ji-&gt;i"</span>, X, t)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h[i] = np.dot(X[i, :], t[:, i])</span></span>
<span id="cb8-48">        y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta</span>
<span id="cb8-49">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> theta, y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> h)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y)</span>
<span id="cb8-50">    </span>
<span id="cb8-51">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> JitLinearRegressionWithQuadraticRegularization(R<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>R)</span>
<span id="cb8-52"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'max absolute error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(model.fit_loocv_predict(X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> standard_loocv(model, X, y)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb8-53"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit model.fit_loocv_predict(X, y).block_until_ready()</span>
<span id="cb8-54"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit standard_loocv(model, X, y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>max absolute error: 4.780e-05
1.75 ms ± 232 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
353 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p>Much better!</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I am deliberately avoiding writing <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20A%5E%7B-1%7D%20b">, as <img src="https://latex.codecogs.com/png.latex?A"> does not have to be invertible for this equation to have a solution, and it allows me to avoid the usual “assuming full rank” caveats people tend to use here. Furthermore, it can mislead people into implementations like <code>np.linalg.inv(A) @ b</code>, which are less stable and efficient than implementations like <code>np.linalg.solve(A, b)</code>.↩︎</p></li>
<li id="fn2"><p>This approach translates better into code, as we get the expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> directly, without going through an expression for <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> first. I also think Sherman-Morisson is a bit too strong here and can obscure some insights, so it’s nice to avoid it. But actually the other approach is just halfway it’s proof (see for example <a href="https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf">here</a>).↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/loocv/loocv_part1.html</guid>
  <pubDate>Mon, 26 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>MUSIC as a sparse decomposition method</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/music/music.html</link>
  <description><![CDATA[ 





<p>MUSIC (MUltiple SIgnal Classification) is a widely used algorithm for estimating the directions of arrival (DOA) of waves recorded by an array of sensors.</p>
<p>While it is highly effective for this specific task, MUSIC is, in fact, a more general parameter estimation method. Unfortunately, conventional introductions to MUSIC often focus on equations tailored specifically for DOA estimation. These derivations, often filled with complex exponents and trigonometric identities, can overwhelm readers and obscure the fundamental principles underlying the algorithm.</p>
<p>Most derivations of MUSIC make the critical assumption that the autocorrelation matrix of the signals is available. In practice, however, only an estimate of this matrix is typically accessible—often derived from a limited number of samples. Furthermore, in many real-world scenarios, such as speech signals, the signals themselves are non-stationary, making the concept of an autocorrelation matrix less well-defined. Adding to these challenges, most explanations also assume that the noise is white, a condition that is rarely satisfied in practical applications.</p>
<p>Despite these limitations, MUSIC often performs remarkably well, even when these assumptions are violated. This suggests that there is an alternative and more general way to derive the algorithm.</p>
<p>In this post, I aim to address these issues by presenting MUSIC as a general method for approximately solving the multi-snapshot sparse decomposition problem.</p>
<section id="a-quick-introduction-to-sparse-decompositions" class="level3">
<h3 class="anchored" data-anchor-id="a-quick-introduction-to-sparse-decompositions">A quick introduction to sparse decompositions</h3>
<p>You obtained an <img src="https://latex.codecogs.com/png.latex?n">-dimensional vector <img src="https://latex.codecogs.com/png.latex?y">, and you know that it is a linear combination of several “atoms” from a given set <img src="https://latex.codecogs.com/png.latex?a_1,%20%5Cdots,%20a_m"> known as the dictionary. The goal is to decompose <img src="https://latex.codecogs.com/png.latex?y"> to it’s atoms, that is, find the atoms that participate in the linear combination. In matrix notation we can write this as a linear equation: <img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20Ax%0A"> where <img src="https://latex.codecogs.com/png.latex?A"> is the (known) dictionary matrix, with columns <img src="https://latex.codecogs.com/png.latex?a_1,%20%5Cdots,%20a_m">, and <img src="https://latex.codecogs.com/png.latex?x"> contains the (unknown) coefficient for each atom. The set of non-zero indices of <img src="https://latex.codecogs.com/png.latex?x">, which we also call the support, correspond to the atoms that participate in the linear combination.<br>
It might be tempting to simply solve for <img src="https://latex.codecogs.com/png.latex?x"> as both <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?y"> are known, but (at least for the interesting cases) <img src="https://latex.codecogs.com/png.latex?m%20%3E%20n"> and the system is under determined, that is, there are infinite ways to decompose <img src="https://latex.codecogs.com/png.latex?y"> as a linear combination of atoms.</p>
<p>In the setting of sparse decompositions, we add an additional prior to the problem: <img src="https://latex.codecogs.com/png.latex?y"> is composed of at most <img src="https://latex.codecogs.com/png.latex?k%20%3C%20n"> atoms, which means <img src="https://latex.codecogs.com/png.latex?x"> is <img src="https://latex.codecogs.com/png.latex?k">-sparse (has at most <img src="https://latex.codecogs.com/png.latex?k"> non zeros).</p>
<p>For example, in DOA estimation problems, we can use <img src="https://latex.codecogs.com/png.latex?y"> to represent a signal recorded by an array of <img src="https://latex.codecogs.com/png.latex?n">-sensors, <img src="https://latex.codecogs.com/png.latex?a_i"> the response of the array to a unit amplitude wave coming from the <img src="https://latex.codecogs.com/png.latex?i">’th direction, and <img src="https://latex.codecogs.com/png.latex?x_i"> the amplitude of the wave coming from the <img src="https://latex.codecogs.com/png.latex?i">’th direction. <img src="https://latex.codecogs.com/png.latex?k">-sparsity of <img src="https://latex.codecogs.com/png.latex?x"> is equivalent to having at most <img src="https://latex.codecogs.com/png.latex?k"> waves active simultaneously, and decomposing <img src="https://latex.codecogs.com/png.latex?y"> into it’s atoms reveals their directions.</p>
<p>There are 2 important extensions to the basic sparse decomposition problem. The first is increasing robustness to noise or modeling errors, by looking for an approximate sparse decomposition instead of an exact one.<br>
For example, in machine learning, approximate sparse decomposition can be used for automatic feature selection in linear regression problems. Here <img src="https://latex.codecogs.com/png.latex?y"> contains the training data labels, <img src="https://latex.codecogs.com/png.latex?A"> contains the training data features, <img src="https://latex.codecogs.com/png.latex?x"> is the coefficient of each feature, and <img src="https://latex.codecogs.com/png.latex?k"> is the number of features to select.</p>
<p>The second extension is the multisnapshot (aka joint sparsity) problem, where instead of observing a single data vector <img src="https://latex.codecogs.com/png.latex?y">, we get <img src="https://latex.codecogs.com/png.latex?p"> vectors <img src="https://latex.codecogs.com/png.latex?y_1,%20%5Cdots,%20y_p">. In matrix notation: <img src="https://latex.codecogs.com/png.latex?%0AY%20=%20AX%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20Y%20&amp;:=%20%5Cbegin%7Bbmatrix%7D%20y_1%20&amp;&amp;%20%5Ccdots%20&amp;&amp;%20y_p%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D"> is the data matrix, and <img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D"> is the (unknown) coefficient of atom <img src="https://latex.codecogs.com/png.latex?a_i"> in <img src="https://latex.codecogs.com/png.latex?y_j">. Here, not only the columns of <img src="https://latex.codecogs.com/png.latex?X"> are <img src="https://latex.codecogs.com/png.latex?k">-sparse, they also share the same support. This means that the matrix <img src="https://latex.codecogs.com/png.latex?X"> is <img src="https://latex.codecogs.com/png.latex?k">-row-sparse, that is, has up to <img src="https://latex.codecogs.com/png.latex?k"> non-zero rows.<br>
In DOA estimation, the multisnapshot problem can be obtained by observing the signals at <img src="https://latex.codecogs.com/png.latex?p"> different (usually consecutive) times.<br>
In the feature selection for linear regression example, the multisnapshot problem is obtained when we have multiple labels to predict, and we want to select the same <img src="https://latex.codecogs.com/png.latex?k"> feature for each.</p>
<p>Solving sparse decomposition problems is in general a hard problem. It turns out that you can’t do much better than enumerating over all <img src="https://latex.codecogs.com/png.latex?m%20%5Cchoose%20k"> possibilities for the support, so in practice approximation methods are often used, e.g. Matching Pursuit, Orthogonal Matching Pursuit, Basis Pursuit, and LASSO. Sometimes, under additional assumptions, they provide some exactness guarantees. Although usually not presented as such, MUSIC is also an approximation method for noisy multisnapshot sparse decomposition, with some guarantees under additional assumptions.</p>
</section>
<section id="solving-the-noiseless-multisnapshot-case" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-noiseless-multisnapshot-case">Solving the noiseless multisnapshot case</h3>
<p>We will start by describing a method that can, under several assumptions, efficiently solve the noiseless joint sparsity problem. MUSIC can be viewed as an extension of this method for the noisy case.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?S"> denote the (unknown) support of <img src="https://latex.codecogs.com/png.latex?X">. We will denote by <img src="https://latex.codecogs.com/png.latex?X_S"> the sub-matrix of <img src="https://latex.codecogs.com/png.latex?X"> obtained by keeping only the rows in <img src="https://latex.codecogs.com/png.latex?S">, and by <img src="https://latex.codecogs.com/png.latex?A_S"> the sub-matrix of <img src="https://latex.codecogs.com/png.latex?A"> obtained by keeping only the columns in <img src="https://latex.codecogs.com/png.latex?S">. Note that with this notation, we have <img src="https://latex.codecogs.com/png.latex?%0AY%20=%20AX%20=%20A_S%20X_S.%0A"></p>
<p>MUSIC is based on two assumptions:</p>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D%20%5Cleft(X%20%5Cright)%20=%20%5Cleft%7C%20S%20%5Cright%7C"> (or equivalently, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D%20%5Cleft(X_S%20%5Cright)%20=%20%5Cleft%7C%20S%20%5Cright%7C">, as the two matrices obviously have the same row space).</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?a_i%20%5Cin%20%5Ctext%7BRange%7D%20%5Cleft(A_S%20%5Cright)"> if and only if <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20S">.</p></li>
</ol>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminder
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our goal is to find <img src="https://latex.codecogs.com/png.latex?S"> from <img src="https://latex.codecogs.com/png.latex?Y">.</p>
</div>
</div>
<p>Assumption 1 implies that <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRange%7D(Y)%0A=%0A%5Ctext%7BRange%7D(A_S%20X_S)%0A=%0A%5Ctext%7BRange%7D(A_S),%0A"> so we can get <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)"> from <img src="https://latex.codecogs.com/png.latex?Y">. Assumption 2 means that once we have <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_s)">, we can reconstruct <img src="https://latex.codecogs.com/png.latex?S"> simply by checking which atoms are in it. The implied algorithm is simple: given the observation <img src="https://latex.codecogs.com/png.latex?Y"> and the dictionary <img src="https://latex.codecogs.com/png.latex?A">, set <img src="https://latex.codecogs.com/png.latex?S"> as: <img src="https://latex.codecogs.com/png.latex?%0AS%20=%20%5Cleft%5C%7B%20i%20%5Cmid%20a_i%20%5Cin%20%5Ctext%7BRange%7D(Y)%20%5Cright%5C%7D.%0A"> <!-- 
1. Calculate $\text{Range}(Y)$.

2. $S=\emptyset$,

3. for each $i$, if $a_i \in \text{Range}(Y)$, add $i$ to $S$. --></p>
<p>Although correct and efficient, this is a terrible algorithm. Calculating the range of a matrix is numerically unstable, and even the slightest perturbation (e.g.&nbsp;a roundoff error) can change it drastically. But before we continue to the more noise-robust MUSIC, let’s discuss the implications of our two assumptions.</p>
<p>Assumption 2 means that to build an atom from a linear combination of other atoms, you need more than <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> atoms. This is related to something called the <a href="https://en.wikipedia.org/wiki/Spark_(mathematics)">spark</a> of <img src="https://latex.codecogs.com/png.latex?A">. We won’t get into it here, but conditions on the dictionary spark are elementary in basically every sparse decomposition method. For certain dictionaries, it can be shown that assumption 2 holds for any <img src="https://latex.codecogs.com/png.latex?S"> of size less than <img src="https://latex.codecogs.com/png.latex?n">. Specifically, it holds for the dictionary in DOA estimation <sup>1</sup>.</p>
<p>Assumption 1 is more restrictive. It means that no row of <img src="https://latex.codecogs.com/png.latex?X_S"> is a linear combination of the other rows. A necessary (but not sufficient) condition is <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C%20%5Cleq%20p">.<br>
In the DOA estimation, each rows of <img src="https://latex.codecogs.com/png.latex?X_S"> contains the samples of a different source. If the sources are uncorrelated (e.g.&nbsp;different speakers) and <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C%20%5Cleq%20p">, it is very unlikely that one is a linear combination of the others. If the sources are correlated, this doesn’t hold, and MUSIC can not be applied. This happens, for example, when one source is an echo of another, due to multi-path propagation.</p>
</section>
<section id="music" class="level3">
<h3 class="anchored" data-anchor-id="music">MUSIC</h3>
<p>The method above relies on the equation <span id="eq-range"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRange%7D(Y)%20=%20%5Ctext%7BRange%7D(A_S)%0A%5Ctag%7B1%7D"></span> which is true if <img src="https://latex.codecogs.com/png.latex?Y=AX">, but in practice the best we can hope for is <img src="https://latex.codecogs.com/png.latex?Y=AX+W">, where the noise term <img src="https://latex.codecogs.com/png.latex?W"> is very small compared to <img src="https://latex.codecogs.com/png.latex?AX">. Unfortunately, no matter how small <img src="https://latex.codecogs.com/png.latex?W"> is, due to the discontinuity of <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D">, Equation&nbsp;1 won’t even hold approximately. In fact, if <img src="https://latex.codecogs.com/png.latex?p%20%5Cgeq%20n">, we will almost surely have <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(Y)%20=%20%5Cmathbb%7BR%7D%5En">, and the algorithm above would just yield <img src="https://latex.codecogs.com/png.latex?S=%5Cleft%5C%7B1,%20%5Cdots,%20%20m%20%5Cright%5C%7D">.</p>
<p>MUSIC makes 2 modifications the the algorithm above.<br>
First, we replace <img src="https://latex.codecogs.com/png.latex?Y"> with <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D">, a rank-<img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation of <img src="https://latex.codecogs.com/png.latex?Y">.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> is assumed known is MUSIC. It can be avoided, sometimes, using model selection methods.</p>
</div>
</div>
<p>Since <img src="https://latex.codecogs.com/png.latex?AX"> has rank <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C">, taking a rank <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation of <img src="https://latex.codecogs.com/png.latex?Y"> has a denoising effect<sup>2</sup>. Indeed, unlike <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20Y%20%5Cright)">, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)"> is a good estimate for <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20A_S%20%5Cright)"> when <img src="https://latex.codecogs.com/png.latex?W"> is small, but it is not exact: almost surely, none of the atoms would lie exactly in it. So the second modification soften the requirement that <img src="https://latex.codecogs.com/png.latex?a_i%20%5Cin%20%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)"> to add <img src="https://latex.codecogs.com/png.latex?i"> to <img src="https://latex.codecogs.com/png.latex?S">. Instead, we will require that <img src="https://latex.codecogs.com/png.latex?a_i"> is “almost in” <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)">, by checking if it looses little magnitude when projected onto it: <img src="https://latex.codecogs.com/png.latex?%0Ac_i%20:=%20%5Cfrac%7B%5C%7C%20%5Ctext%7BProj%7D_%7B%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)%7D(a_i)%20%5C%7C%5E2%7D%0A%7B%5C%7C%20a_i%20%5C%7C%5E2%20%7D%0A%5Ctext%7B%20is%20close%20to%201%7D%0A%5Cimplies%0A%5Ctext%7B%20add%20$i$%20to%20$S$%7D%0A"> (what “is close” means exactly differs between implementations. When the atoms can be ordered, like in DOA estimation, it is common to use a peak selection algorithm).</p>
<p>As we said above, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D"> is a rank-<img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation to <img src="https://latex.codecogs.com/png.latex?Y">. In MUSIC, we use the best rank-<img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation in the least squares sense, which is given by the truncated singular value decomposition (SVD) of <img src="https://latex.codecogs.com/png.latex?Y">. Note that we don’t really need to calculate <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D"> itself. Since the first <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> left singular vectors of <img src="https://latex.codecogs.com/png.latex?Y"> form an orthonormal basis for <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)">, we have: <img src="https://latex.codecogs.com/png.latex?%0A%5Clabel%7Bmusic_final%7D%0Ac_i%20=%20%5Cfrac%7B%5C%7C%20U%5EH%20a_i%5C%7C%5E2%7D%7B%5C%7C%20a_i%5C%7C%5E2%7D.%0A"> where the columns of <img src="https://latex.codecogs.com/png.latex?U"> are the first <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> left singular vectors.</p>
<p>To wrap things up, a few notes to connect the above to the “usual” MUSIC derivation:</p>
<ul>
<li><p>The left singular vectors of <img src="https://latex.codecogs.com/png.latex?Y"> are the eigenvectors of <img src="https://latex.codecogs.com/png.latex?p%5E%7B-1%7D%20YY%5ET">, which, in a stochastic setting, can be viewed as an estimate of the autocorrelation matrix.</p></li>
<li><p>The usual MUSIC formula use the last <img src="https://latex.codecogs.com/png.latex?n-%5Cleft%7C%20S%20%5Cright%7C"> left singular vectors (which we stack to the columns of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BU%7D">) instead of the first <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C">. From the Pythagorean theorem <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%20a_i%20%5C%7C%20%5E2%20=%20%5C%7CU%5EH%20a_i%20%5C%7C%5E2%20+%20%5C%7C%20%5Cbar%7BU%7D%5EH%20a_i%20%5C%7C%5E2,%0A"> so we can write <img src="https://latex.codecogs.com/png.latex?c_i"> as follows: <img src="https://latex.codecogs.com/png.latex?%0Ac_i%20=%201%20-%5Cfrac%7B%0A%5C%7C%20%5Cbar%7BU%7D%5EH%20a_i%20%5C%7C%5E2%0A%7D%7B%5C%7C%20a_i%5C%7C%5E2%7D.%0A"></p></li>
<li><p>In MUSIC for DOA/spectral estimation, it is common to plot <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B1-c_i%7D">, and call it the “pseudo-spectrum”. The 1-over-1-minus transform maps numbers close to 1 to very large numbers, which often results in very beautiful and pointy (but somewhat misleading) plots.</p></li>
</ul>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>With linear, equally spaced array of sensors, if the usual anti-aliasing conditions hold: the spacing between the sensors is smaller than half the wavelength, and no 2 directions lie on the same cone who’s axis contains the array.↩︎</p></li>
<li id="fn2"><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20A_S%20%5Cright)"> is sometimes called the signal subspace, and the subspace orthogonal to it the noise subspace.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/music/music.html</guid>
  <pubDate>Mon, 29 Jan 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>A practical interpertation of the Pearson correlation coefficient</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/pearson_correlation/pearson_correlation.html</link>
  <description><![CDATA[ 





<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crenewcommand%7B%5CE%7D%5B1%5D%7B%5Coperatorname%7BE%7D%5Cleft%5B#1%5Cright%5D%7D%0A%5Crenewcommand%7B%5Cvar%7D%5B1%5D%7B%5Coperatorname%7BVar%7D%20%5Cleft%5B#1%20%5Cright%5D%7D%0A%5Crenewcommand%7B%5Ccov%7D%5B1%5D%7B%5Coperatorname%7BCov%7D%20%5Cleft%5B#1%20%5Cright%5D%20%7D%0A"></p>
<p>My goal is to explain the Pearson correlation coefficient without using the word “correlation,” which is often used to describe it.</p>
<p>The Pearson correlation coefficient of two random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Crho%20:=%20%5Cfrac%7B%5Csigma_%7BXY%7D%7D%7B%5Csigma_X%20%5Csigma_Y%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Csigma_X"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_Y"> are the standard deviations of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> respectively, and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7BXY%7D"> is their covariance.</p>
<p>A motivation for this definition stems from the problem of estimating <img src="https://latex.codecogs.com/png.latex?Y"> from an observation of <img src="https://latex.codecogs.com/png.latex?X">. It turns out that in the optimal (lowest MSE) linear estimation, <em>the number of standard deviations <img src="https://latex.codecogs.com/png.latex?Y"> is above its mean is <img src="https://latex.codecogs.com/png.latex?%5Crho"> times the number of standard deviations <img src="https://latex.codecogs.com/png.latex?X"> is above its mean.</em></p>
<p>For example, consider a population where height and weight are correlated with <img src="https://latex.codecogs.com/png.latex?%5Crho=0.72">, heights are distributed with a mean of <img src="https://latex.codecogs.com/png.latex?170">cm and a standard deviation of <img src="https://latex.codecogs.com/png.latex?10">cm, weights are distributed with a mean of <img src="https://latex.codecogs.com/png.latex?70">Kg and a standard deviation of <img src="https://latex.codecogs.com/png.latex?20">Kg. If we know that a certain person’s height is <img src="https://latex.codecogs.com/png.latex?190">cm, a good estimate for their weight would be <img src="https://latex.codecogs.com/png.latex?70%20+%202%20%5Ccdot%200.72%20%5Ccdot%2020%20=%2098.8">Kg.</p>
<p>The proof is straightforward. Since we are dealing with linear (actually, affine) estimators, we need to show that the <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?b"> that would minimize <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BMSE%7D%20:=%20%5CE%7B%20%5Cleft(%20%5Chat%7BY%7D%20-%20Y%20%5Cright)%20%5E2%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20:=%20a%20(X%20-%20%5Cmu_x)%20+%20b">, are <img src="https://latex.codecogs.com/png.latex?%5Crho%20%5Csigma_Y%20/%20%5Csigma_X"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu_Y">.</p>
<p>The MSE is the sum of the square of the bias and the variance. The variance doesn’t depend on <img src="https://latex.codecogs.com/png.latex?b">, and the bias is <img src="https://latex.codecogs.com/png.latex?%5CE%7B%20%20%5Chat%7BY%7D%20-%20Y%20%7D%20=%20b%20-%20%5Cmu_Y"> which doesn’t depend on <img src="https://latex.codecogs.com/png.latex?a">, so <img src="https://latex.codecogs.com/png.latex?b=%5Cmu_Y">. To minimize the variance, we simplify: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cvar%7B%5Chat%7BY%7D%20-%20Y%7D%0A&amp;=%20%5Cvar%7B%5Chat%7BY%7D%7D%20+%20%5Cvar%7BY%7D%20-%202%20%5Ccov%7B%5Chat%7BY%7D,%20Y%7D%0A%5C%5C&amp;=%20%5Csigma_x%20%5E%202%20a%5E2%0A%20%20%20+%20%5Csigma_Y%20%5E2%0A%20%20%20-2%20%20%5Csigma_%7BXY%7D%20a.%0A%5Cend%7Balign*%7D%0A"> This is simply a parabola in <img src="https://latex.codecogs.com/png.latex?a">, so the optimal <img src="https://latex.codecogs.com/png.latex?a"> is <img src="https://latex.codecogs.com/png.latex?%0Aa=%5Cfrac%7B2%20%5Csigma_%7BXY%7D%7D%20%7B2%20%5Csigma_X%20%5E2%7D%0A=%0A%5Crho%20%5Cfrac%7B%5Csigma_Y%20%7D%20%7B%5Csigma_X%20%7D%0A"> (which is what we wanted to show).</p>
<p>The estimator is unbiased, so its MSE is equal to its variance: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BMSE%7D%20=%20%5Csigma_Y%20%5E2%20(1%20-%20%5Crho%20%5E%202).%0A"> This equation provides another concrete interpretation of <img src="https://latex.codecogs.com/png.latex?%5Crho">: *If <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are correlated with coefficient <img src="https://latex.codecogs.com/png.latex?%5Crho">, observing <img src="https://latex.codecogs.com/png.latex?X"> will decrease the standard deviation of a <img src="https://latex.codecogs.com/png.latex?Y"> estimate by a factor of at least <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B1%20-%20%5Crho%5E2%7D"> (“At least” since the optimal linear estimator is equal to or worse than the optimal estimator). In the example above, knowing the height decreases weight estimation standard deviation from 20Kg to <img src="https://latex.codecogs.com/png.latex?20%20%20%5Csqrt%7B1%20-%200.72%5E2%7D%20=%2013.9">Kg.</p>
<p>Randomly ordered notes:</p>
<ol type="1">
<li><p>If <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are jointly Gaussian, the optimal linear estimator is also the optimal estimator.</p></li>
<li><p>The “mean” in “MSE” represents an average over the joint distribution of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, which differs from the distribution of <img src="https://latex.codecogs.com/png.latex?Y"> given <img src="https://latex.codecogs.com/png.latex?X">, for which our estimator is not the optimal linear estimator (and is biased).</p>
<p>In our example, we estimated the weight to be <img src="https://latex.codecogs.com/png.latex?98.8">Kg with variance <img src="https://latex.codecogs.com/png.latex?9.6%5E2">. This doesn’t mean that if we sample random people with a height of <img src="https://latex.codecogs.com/png.latex?190">cm, we would get a mean weight of <img src="https://latex.codecogs.com/png.latex?98.8">Kg and variance smaller than <img src="https://latex.codecogs.com/png.latex?9.6%5E2">. Instead, it means that if we sample random people and estimate their weight from their height using the optimal linear estimator, our error will be zero on average, with a variance of <img src="https://latex.codecogs.com/png.latex?9.6%5E2">. If we use the optimal estimator, <img src="https://latex.codecogs.com/png.latex?9.6%5E2"> is an upper bound on the variance.</p></li>
<li><p>The statement “<img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are not correlated” now has a concrete meaning: it means the optimal linear estimator of <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X"> will be the mean of <img src="https://latex.codecogs.com/png.latex?Y">, completely ignoring <img src="https://latex.codecogs.com/png.latex?X">.</p></li>
<li><p>The discussion above is “Bayesian” in the sense that it assumes prior knowledge about the distribution of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">. In practice, we typically obtain <img src="https://latex.codecogs.com/png.latex?n"> samples of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> pairs and use plug-in estimators to estimate the means, variances, and covariance, which we then use to construct a linear estimator of <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>Machine learning practitioners would say: we can use the samples to train a linear regression model to predict <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X"> directly. This sounds more “end-to-end,” but it actually yields exactly the same result<sup>1</sup>.</p>
<p>Proof:<br>
Let <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> denote the vectors of samples of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B1%7D"> a vector of ones, and <img src="https://latex.codecogs.com/png.latex?A"> as the matrix whose first column is <img src="https://latex.codecogs.com/png.latex?x"> and the second column is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B1%7D">. The coefficients of the linear model are given by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20%5Ctheta_%7B%5Ctext%7Bslope%7D%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Ctheta_%7B%5Ctext%7Bintercept%7D%7D%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20&amp;:=%0A%20%20%20%20%5Ctext%7Bargmin%7D_%5Ctheta%20%5C%7C%20A%20%5Ctheta%20-%20y%20%5C%7C%5E2%0A%20%20%20%20%5C%5C&amp;=%0A%20%20%20%20%5Cleft(%20A%20%5ET%20A%20%5Cright)%5E%7B-1%7D%20A%5ET%20y%0A%20%20%20%20%5C%5C&amp;=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20%5C%7Cx%5C%7C%5E2%20&amp;&amp;%20%5Cmathbf%7B1%7D%5ETx%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Cmathbf%7B1%7D%5ET%20x%20%20&amp;&amp;%20%5Cmathbf%7B1%7D%5ET%20%5Cmathbf%7B1%7D%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5E%7B-1%7D%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20x%5ET%20y%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Cmathbf%7B1%7D%20%5ET%20y%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5C%5C&amp;=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20%5Csigma_X%5E2%20+%20%5Cmu_X%5E2%20&amp;&amp;%20%5Cmu_X%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Cmu_X%20%20&amp;&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5E%7B-1%7D%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20%5Csigma_%7BXY%7D%20+%20%5Cmu_X%20%5Cmu_Y%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Cmu_Y%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5C%5C&amp;=%0A%20%20%20%20%5Cfrac%7B1%7D%7B%5Csigma_X%20%5E2%7D%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%201%20&amp;&amp;%20-%5Cmu_X%20%5C%5C%0A%20%20%20%20%20%20%20%20-%5Cmu_X%20%20&amp;&amp;%20%5Csigma_X%5E2%20+%20%5Cmu_X%5E2%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20%5Csigma_%7BXY%7D%20+%20%5Cmu_X%20%5Cmu_Y%20%5C%5C%0A%20%20%20%20%20%20%20%20%5Cmu_Y%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5C%5C&amp;=%0A%20%20%20%20%5Cfrac%7B1%7D%7B%5Csigma_X%20%5E2%7D%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20%5Csigma_%7BXY%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20-%5Cmu_X%20%5Csigma_%7BXY%7D%20+%20%5Csigma_X%5E2%20%5Cmu_Y%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%20%20%5C%5C&amp;=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%20%20%20a%20%5C%5C%0A%20%20%20%20%20%20%20%20-%5Cmu_X%20a%20+%20b%0A%20%20%20%20%5Cend%7Bbmatrix%7D.%0A%5Cend%7Balign*%7D%0A"> Note also that the r2-score of this fit is equal to <img src="https://latex.codecogs.com/png.latex?%5Crho%5E2">: <img src="https://latex.codecogs.com/png.latex?%0Ar%5E2%20:=%201%20-%20%5Cfrac%7B%5Ctext%7BMSE%7D%7D%7B%5Csigma_Y%5E2%7D%20=%201%20-%20%5Cfrac%7B%5Csigma_Y%20%5E2%20%5Cleft(1-%5Crho%5E2%5Cright)%7D%7B%5Csigma_Y%20%5E2%7D%20=%20%5Crho%5E2.%0A"></p></li>
</ol>
<!-- 
The optima
The optimal $a=\frac{2 \sigma_{XY}} {2 \sigma_X ^ 2}$

$$
\begin{align*}
a 
&= \text{argmin}_{a'} \var{\hat{Y} - Y\right] \
&= \text{argmin}_{a'} \var{a (X - \mu_X) - Y\right] \
&= \text{argmin}_{a'} 
    \var{a \left(X - \mu_X\right) \right] 
    + \var{ Y\right] 
    -2 \mathrm{Cov}\left[a \left(X - \mu_X\right), Y\right] \
&= \text{argmin}_{a'} 
    a^2 \sigma_x ^ 2
    + \sigma_Y ^2
    -2 a \sigma_{XY}
\end{align*}
$$
and the variance do
We with expanding the MSE as the sum of the squared bias and variance
$$
\begin{align*}
\text{MSE} &=
\left(\E{ \left[\hat{Y} - Y \right] \right)^2
+ \mathrm{Var} \left[\hat{Y} - Y \right]
\&=
b ^ 2
+ \mathrm{Var} \left[a (x - \mu_X) - Y \right]
\end{align*}
$$ -->
<!-- 
Suppose the Pearson correlation coefficient is $\rho$, 
and you wish to estimate $Y$ based on a given observation of $X$
that is $n$ standard deviations away from the mean.
The optimal linear estimate is $n \rho$ standard deviations away from the mean.

If the observation of $X$ is $n$
It turns out that the optimal linear estimation is $\rho$ 
then the optimal linear estimation of $Y$ given a sample of $X$ is
then the optimal linear estimation of $Y$ from $X$ is obtained by 
1. Calculate by how many standard deviations the sampled $X$ is above it's mean.
2. multiply by $\rho$.
3. This is by how many standard deviations the estimate of $Y$ is above it's mean. -->
<!-- $$
\begin{align*}
\E{ \left[ \left(a (x - \mu_x) + b - y \right) ^ 2 \right]
&= 
a ^ 2 \E{ \left[ \left( x - \mu_x \right) ^ 2 \right]
+
\E{ \left[ \left(b - y \right) ^ 2 \right]
+
a \E{ \left[ \left(x - \mu_x\right) \left(b - y \right) \right]
\&=
a ^ 2 \sigma_x ^2
+
\sigma_y ^ 2
+
a \, \sigma_{xy}
\end{align*}
$$ -->




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Assuming we don’t use <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/pearson_correlation/pearson_correlation.html</guid>
  <pubDate>Fri, 19 Jan 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Augmentation is Regularization</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/augmentation_is_regularization/augmentation_is_regularization.html</link>
  <description><![CDATA[ 





<p>Training data augmentation enhances the training dataset by applying transformations to existing training data instances. The specific transformations vary depending on the type of data involved, and this flexibility allows to leverage domain knowledge, such as known invariants, effectively. The goal is to introduce variability and increase the diversity of the training set, allowing the model to better generalize to unseen data and exhibit improved robustness. Despite the advantages, training data augmentation introduces an inherent computational cost: the increased volume of data requires additional computational resources, impacting both training time and memory requirements.</p>
<p>As we will show below, for linear models with the sum of squares loss, training data augmentation is equivalent to adding quadratic regularization term, which implies that the computational cost of fitting a model to an augmented dataset is the same as using no augmentation at all!</p>
<p>This link between augmentation and regularization is useful in the other direction as well: it gives a concrete interpretation to the value of regularization hyperparameters, and can be used to avoid costly hyperparameters tuning (<code>np.logspace(-6, 6, 100)</code> much?), and to design regularizers that are more appropriate to the data than the simple ones (i.e sum of squares regularization used in ridge regression).</p>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<p>Suppose we have a training data set comprised of <img src="https://latex.codecogs.com/png.latex?n"> pairs <img src="https://latex.codecogs.com/png.latex?x_i,%5C,y_i"> for <img src="https://latex.codecogs.com/png.latex?i=0,%20%5Cdots,%20n-1">, where <img src="https://latex.codecogs.com/png.latex?x_i"> is the <img src="https://latex.codecogs.com/png.latex?d"> dimensional feature vector of the <img src="https://latex.codecogs.com/png.latex?i">’th training data, and <img src="https://latex.codecogs.com/png.latex?y_i"> is the corresponding label. Here we will assume <img src="https://latex.codecogs.com/png.latex?y_i%20%5Cin%20%5Cmathrm%7BR%7D">, however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by <img src="https://latex.codecogs.com/png.latex?X"> the <img src="https://latex.codecogs.com/png.latex?n">-by-<img src="https://latex.codecogs.com/png.latex?d"> matrix with rows <img src="https://latex.codecogs.com/png.latex?x_0%5ET,%20%5Cdots,%20x_%7Bn-1%7D%5ET"> and by <img src="https://latex.codecogs.com/png.latex?y"> the <img src="https://latex.codecogs.com/png.latex?n">-vector with entries <img src="https://latex.codecogs.com/png.latex?y_0,%20%5Cdots,%20y_%7Bn-1%7D">.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?a:%5Cmathrm%7BR%7D%5Ed%20%5Ctimes%20%5Cmathcal%7BP%7D%20%20%5Cmapsto%20%5Cmathrm%7BR%7D%5Ed"> denote the augmentation function that given the augmentation params <img src="https://latex.codecogs.com/png.latex?p%20%5Cin%20%5Cmathcal%7BP%7D">, maps a feature vector <img src="https://latex.codecogs.com/png.latex?x"> to a transformed feature vector. The augmentation parameters <img src="https://latex.codecogs.com/png.latex?p"> are usually sampled randomly from a given distribution. For example, for image data, <img src="https://latex.codecogs.com/png.latex?a"> is often a composition of small shifts, rotations, brightness changes, etc. while <img src="https://latex.codecogs.com/png.latex?p"> specifies the amount of shifting, rotation and brightness change.</p>
</section>
<section id="ordinary-least-squares-ols" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-least-squares-ols">Ordinary least squares (OLS)</h2>
<p>Let’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.<br>
To fit an OLS model, we find a vector of coefficients <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%5Ctext%7BOLS%7D"> that minimizes the sum of squared training errors: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BOLS%7D%20&amp;:=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20%5Cleft(%0A%20%20%20%20x_i%20%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%5Cright)%20%5E2%0A%20%20%20%20%5C%5C&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5C%7C%20X%20%5Ctheta%20-%20y%20%5C%7C%5E2%20%5Ctag%7B1%7D%0A%5Cend%7Balign*%7D"> To solve the optimization problem (1), we solve the equation <img src="https://latex.codecogs.com/png.latex?X%5ETX%20%5Ctheta_%5Ctext%7BOLS%7D%20=%20X%5ET%20y">, which has time complexity <img src="https://latex.codecogs.com/png.latex?O(n%20d%5E2)">.</p>
</section>
<section id="augmented-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="augmented-least-squares">Augmented least squares</h2>
<p>We will now fit a model by finding coefficients <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%5Ctext%7BALS%7D"> that minimize the expected error over the augmented training dataset: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BALS%7D%20&amp;:=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5Cmathrm%7BE%7D%20%20%0A%20%20%20%20%5Cleft%5B%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20%5Cleft(%0A%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%5Cright)%20%5E2%0A%20%20%20%20%5Cright%5D,%20%5Ctag%7B2%7D%0A%5Cend%7Balign*%7D"> where the expectation is over <img src="https://latex.codecogs.com/png.latex?p_0,%5Cdots,%20p_%7Bn-1%7D">, the random augmentation parameters. As we will see below, <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%5Ctext%7BALS%7D"> depends on <img src="https://latex.codecogs.com/png.latex?a"> and the distribution of <img src="https://latex.codecogs.com/png.latex?p"> only through the 2nd order moments, which we denote by <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cmu_i%20&amp;:=%20%5Cmathrm%7BE%7D%20%5Cleft%5Ba(x_i,%20p_i)%20%5Cright%5D%5C%5C%0A%20%20%20%20R_i%20&amp;:=%20%5Cmathrm%7BC%7D%5Ctext%7Bov%7D%20%5Cleft%5B%20a(x_i,%20p_i)%20%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>Continuing from (2), we use the standard trick of subtracting and adding the mean: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BALS%7D%20&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5Cmathrm%7BE%7D%20%20%0A%20%20%20%20%5Cleft%5B%0A%20%20%20%20%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%0A%20%20%20%20%20%20%20%20%20%20%20%20+%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20-%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%5ET%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%20%5E2%0A%20%20%20%20%5Cright%5D%0A%5Cend%7Balign*%7D"> Note that the first term <img src="https://latex.codecogs.com/png.latex?%5Cmu_i%5ET%20%5Ctheta%20-%20y_i"> is deterministic, while the second term <img src="https://latex.codecogs.com/png.latex?%5Cleft(a%5Cleft(x_i,%20p_i%5Cright)%20-%20%5Cmu_i%5Cright)%5ET%5Ctheta"> has zero mean. Therefore <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BALS%7D%20&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%0A%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%20%20%20%20%5Cright)%0A%20%20%20%20%20%20%20%20+%0A%20%20%20%20%20%20%20%20%5Cmathrm%7BE%7D%20%5Cleft%5B%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20-%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%5ET%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%5E2%20%5Cright%5D%20%5C%5C%0A%20%20%20%20&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%0A%20%20%20%20%5C%7C%20M%20%5Ctheta%20-%20y%5C%7C%5E2%20+%20%5Ctheta%20%5ET%20R%20%5Ctheta,%0A%20%20%20%20%5Ctag%7B3%7D%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?M"> is the <img src="https://latex.codecogs.com/png.latex?n">-by-<img src="https://latex.codecogs.com/png.latex?d"> matrix whose rows are <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%5ET,%20%5Cdots,%20%5Cmu_%7Bn-1%7D%5ET">, and <img src="https://latex.codecogs.com/png.latex?%0AR%20:=%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20R_i.%0A"> Equation (3) shows exactly what we set to prove - fitting a model on an augmented training dataset is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace <img src="https://latex.codecogs.com/png.latex?X"> with it’s mean, and use the sum of all the covariances as the regularization matrix.</p>
<p>To solve the optimization problem (3), we solve the equation <img src="https://latex.codecogs.com/png.latex?(X%5ET%20X%20+%20R)%20%5Ctheta_%5Ctext%7BALS%7D%20=%20X%5ET%20y">, which has the same <img src="https://latex.codecogs.com/png.latex?O(n%20d%5E2)"> complexity as OLS.</p>
</section>
<section id="ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression">Ridge regression</h2>
<p>Ride regression (aka Tykhonov regularization) has the form (3) with <img src="https://latex.codecogs.com/png.latex?M=X"> and <img src="https://latex.codecogs.com/png.latex?R=%5Clambda%20I">. As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance <img src="https://latex.codecogs.com/png.latex?%5Clambda/n">, uncorrelated across features.<br>
This interpretation of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> can be used to set it (at least roughly): just think what level of perturbation <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is reasonable for your features, and set <img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20n%20%5Csigma%5E2">.<br>
This also shows that when different features are scaled differently, ridge regression is perhaps not the suitable. A standard deviation of 100 might be reasonable for perturbing a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal <img src="https://latex.codecogs.com/png.latex?R">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20R=%20n%20%5C,%20%5Ctext%7Bdiag%7D%20%5Cleft(%0A%20%20%20%20%20%20%20%20%5Csigma_0%5E2,%20%5Cdots,%20%5Csigma_%7Bd-1%7D%5E2%0A%20%20%20%20%5Cright)%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Csigma_i"> is the standard deviation of the perturbation of feature <img src="https://latex.codecogs.com/png.latex?i">.</p>
<p>Another option is to scale the features before fitting, e.g using sklearn’s <code>StandardScaler</code> transformer. With all features scaled to have unit variance, setting <img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20n%20%5C,%2010%20%5E%7B-6%7D"> is sensible as rule of thumb, as it is often reasonable to assume at least <img src="https://latex.codecogs.com/png.latex?0.1%5C%25"> perturbation.</p>
<p>Note that often the model includes an intercept (aka constant) term by adding a column of ones to <img src="https://latex.codecogs.com/png.latex?X">. Since this column remain unchanged through any augmenting transformation, the corresponding row and column of <img src="https://latex.codecogs.com/png.latex?R"> should be all zeros.</p>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>For the example we are gonna use the <a href="https://www.kaggle.com/datasets/harlfoxem/housesalesprediction/data">House Sales in King County, USA dataset</a>. Each row describes a house sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and location.</p>
<p>Note: several decisions outlined below weren’t necessarily the most effective; rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.</p>
<p>Let’s begin by importing everything we will need, loading our data, and adding some columns.</p>
<div id="cell-7" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Callable, Hashable, Self</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> numpy.typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> NDArray</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scipy</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.base <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseEstimator</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KMeans</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.compose <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ColumnTransformer</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression, RidgeCV</span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> r2_score</span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-15"></span>
<span id="cb1-16">Array <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NDArray[np.float64]</span>
<span id="cb1-17"></span>
<span id="cb1-18">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data/kc_house_data.csv.zip"</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date"</span>])</span>
<span id="cb1-19">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"long_scaled"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"long"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(</span>
<span id="cb1-20">    np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(np.cos(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lat"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">180</span>))</span>
<span id="cb1-21">)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># earth-curvature correction for (approximate) distance calculations</span></span>
<span id="cb1-22">df.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">id</th>
<th data-quarto-table-cell-role="th">date</th>
<th data-quarto-table-cell-role="th">price</th>
<th data-quarto-table-cell-role="th">bedrooms</th>
<th data-quarto-table-cell-role="th">bathrooms</th>
<th data-quarto-table-cell-role="th">sqft_living</th>
<th data-quarto-table-cell-role="th">sqft_lot</th>
<th data-quarto-table-cell-role="th">floors</th>
<th data-quarto-table-cell-role="th">waterfront</th>
<th data-quarto-table-cell-role="th">view</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">sqft_above</th>
<th data-quarto-table-cell-role="th">sqft_basement</th>
<th data-quarto-table-cell-role="th">yr_built</th>
<th data-quarto-table-cell-role="th">yr_renovated</th>
<th data-quarto-table-cell-role="th">zipcode</th>
<th data-quarto-table-cell-role="th">lat</th>
<th data-quarto-table-cell-role="th">long</th>
<th data-quarto-table-cell-role="th">sqft_living15</th>
<th data-quarto-table-cell-role="th">sqft_lot15</th>
<th data-quarto-table-cell-role="th">long_scaled</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>2.161300e+04</td>
<td>21613</td>
<td>2.161300e+04</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>2.161300e+04</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>...</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>4.580302e+09</td>
<td>2014-10-29 04:38:01.959931648</td>
<td>5.400881e+05</td>
<td>3.370842</td>
<td>2.114757</td>
<td>2079.899736</td>
<td>1.510697e+04</td>
<td>1.494309</td>
<td>0.007542</td>
<td>0.234303</td>
<td>...</td>
<td>1788.390691</td>
<td>291.509045</td>
<td>1971.005136</td>
<td>84.402258</td>
<td>98077.939805</td>
<td>47.560053</td>
<td>-122.213896</td>
<td>1986.552492</td>
<td>12768.455652</td>
<td>-82.471784</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">min</td>
<td>1.000102e+06</td>
<td>2014-05-02 00:00:00</td>
<td>7.500000e+04</td>
<td>0.000000</td>
<td>0.000000</td>
<td>290.000000</td>
<td>5.200000e+02</td>
<td>1.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>290.000000</td>
<td>0.000000</td>
<td>1900.000000</td>
<td>0.000000</td>
<td>98001.000000</td>
<td>47.155900</td>
<td>-122.519000</td>
<td>399.000000</td>
<td>651.000000</td>
<td>-82.677673</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">25%</td>
<td>2.123049e+09</td>
<td>2014-07-22 00:00:00</td>
<td>3.219500e+05</td>
<td>3.000000</td>
<td>1.750000</td>
<td>1427.000000</td>
<td>5.040000e+03</td>
<td>1.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>1190.000000</td>
<td>0.000000</td>
<td>1951.000000</td>
<td>0.000000</td>
<td>98033.000000</td>
<td>47.471000</td>
<td>-122.328000</td>
<td>1490.000000</td>
<td>5100.000000</td>
<td>-82.548783</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">50%</td>
<td>3.904930e+09</td>
<td>2014-10-16 00:00:00</td>
<td>4.500000e+05</td>
<td>3.000000</td>
<td>2.250000</td>
<td>1910.000000</td>
<td>7.618000e+03</td>
<td>1.500000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>1560.000000</td>
<td>0.000000</td>
<td>1975.000000</td>
<td>0.000000</td>
<td>98065.000000</td>
<td>47.571800</td>
<td>-122.230000</td>
<td>1840.000000</td>
<td>7620.000000</td>
<td>-82.482651</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">75%</td>
<td>7.308900e+09</td>
<td>2015-02-17 00:00:00</td>
<td>6.450000e+05</td>
<td>4.000000</td>
<td>2.500000</td>
<td>2550.000000</td>
<td>1.068800e+04</td>
<td>2.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>2210.000000</td>
<td>560.000000</td>
<td>1997.000000</td>
<td>0.000000</td>
<td>98118.000000</td>
<td>47.678000</td>
<td>-122.125000</td>
<td>2360.000000</td>
<td>10083.000000</td>
<td>-82.411796</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">max</td>
<td>9.900000e+09</td>
<td>2015-05-27 00:00:00</td>
<td>7.700000e+06</td>
<td>33.000000</td>
<td>8.000000</td>
<td>13540.000000</td>
<td>1.651359e+06</td>
<td>3.500000</td>
<td>1.000000</td>
<td>4.000000</td>
<td>...</td>
<td>9410.000000</td>
<td>4820.000000</td>
<td>2015.000000</td>
<td>2015.000000</td>
<td>98199.000000</td>
<td>47.777600</td>
<td>-121.315000</td>
<td>6210.000000</td>
<td>871200.000000</td>
<td>-81.865195</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">std</td>
<td>2.876566e+09</td>
<td>NaN</td>
<td>3.671272e+05</td>
<td>0.930062</td>
<td>0.770163</td>
<td>918.440897</td>
<td>4.142051e+04</td>
<td>0.539989</td>
<td>0.086517</td>
<td>0.766318</td>
<td>...</td>
<td>828.090978</td>
<td>442.575043</td>
<td>29.373411</td>
<td>401.679240</td>
<td>53.505026</td>
<td>0.138564</td>
<td>0.140828</td>
<td>685.391304</td>
<td>27304.179631</td>
<td>0.095033</td>
</tr>
</tbody>
</table>

<p>8 rows × 22 columns</p>
</div>
</div>
</div>
<p>We will want a polynomial (rather than linear) dependency on the age of the house:</p>
<div id="cell-9" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date"</span>].dt.year <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"yr_built"</span>]</span>
<span id="cb2-2">age_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>]</span>
<span id="cb2-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> power <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb2-4">    col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"age ^ </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>power<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb2-5">    df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> power</span>
<span id="cb2-6">    age_cols.append(col)</span></code></pre></div>
</div>
<p>We do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.</p>
<div id="cell-11" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"price"</span>])</span>
<span id="cb3-2">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"price"</span>])</span>
<span id="cb3-3">x_train, x_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(</span>
<span id="cb3-4">    X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span></span>
<span id="cb3-5">)</span>
<span id="cb3-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x_train<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x_train.shape=(2161, 25), x_test.shape=(19452, 25)</code></pre>
</div>
</div>
<p>There is no reason to expect a linear relationship between the house geographical coordinates and it’s price.<br>
However, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.<br>
One way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:</p>
<div id="cell-13" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.</span></span>
<span id="cb5-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> OneHotEncodedKMeansTransformer:</span>
<span id="cb5-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, columns: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>], name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb5-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> columns</span>
<span id="cb5-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k</span>
<span id="cb5-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> name</span>
<span id="cb5-7"></span>
<span id="cb5-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb5-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k, n_init<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"auto"</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb5-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_.fit(X[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.columns])</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb5-12"></span>
<span id="cb5-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> column_names(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]:</span>
<span id="cb5-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> [<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k)]</span>
<span id="cb5-15"></span>
<span id="cb5-16">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> transform(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame):</span>
<span id="cb5-17">        cluster_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_.predict(X[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.columns])</span>
<span id="cb5-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> pd.concat(</span>
<span id="cb5-19">            [</span>
<span id="cb5-20">                X,</span>
<span id="cb5-21">                pd.DataFrame(</span>
<span id="cb5-22">                    np.eye(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k)[cluster_index],</span>
<span id="cb5-23">                    columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.column_names(),</span>
<span id="cb5-24">                    index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X.index,</span>
<span id="cb5-25">                ),</span>
<span id="cb5-26">            ],</span>
<span id="cb5-27">            axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb5-28">        )</span>
<span id="cb5-29"></span>
<span id="cb5-30">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> clusters_adjacency_matrix(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb5-31">        edges <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(</span>
<span id="cb5-32">            scipy.spatial.Voronoi(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_.cluster_centers_).ridge_points</span>
<span id="cb5-33">        ).T</span>
<span id="cb5-34">        a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.coo_matrix(</span>
<span id="cb5-35">            (np.ones(edges.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), (edges[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], edges[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])),</span>
<span id="cb5-36">            shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k),</span>
<span id="cb5-37">        )</span>
<span id="cb5-38">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> a.T</span>
<span id="cb5-39"></span>
<span id="cb5-40"></span>
<span id="cb5-41">kmeans_transformer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OneHotEncodedKMeansTransformer(</span>
<span id="cb5-42">    k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>,</span>
<span id="cb5-43">    columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lat"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"long_scaled"</span>],</span>
<span id="cb5-44">    name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"geo_cluster"</span>,</span>
<span id="cb5-45">)</span>
<span id="cb5-46">x_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans_transformer.fit(x_train).transform(x_train)</span>
<span id="cb5-47">x_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans_transformer.transform(x_test)</span></code></pre></div>
</div>
<p>We will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.</p>
<div id="cell-15" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> evaluate_model(model) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-2">    y_train_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(x_train, y_train).predict(x_train)</span>
<span id="cb6-3">    r2_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_train, y_train_pred)</span>
<span id="cb6-4">    y_test_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(x_test)</span>
<span id="cb6-5">    r2_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_test, y_test_pred)</span>
<span id="cb6-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<p>Let’s start with a vanilla linear model, without any regularization/augmentations.</p>
<div id="cell-17" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-2">    [</span>
<span id="cb7-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bedrooms"</span>,</span>
<span id="cb7-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bathrooms"</span>,</span>
<span id="cb7-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"floors"</span>,</span>
<span id="cb7-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"waterfront"</span>,</span>
<span id="cb7-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"view"</span>,</span>
<span id="cb7-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"condition"</span>,</span>
<span id="cb7-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grade"</span>,</span>
<span id="cb7-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living"</span>,</span>
<span id="cb7-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot"</span>,</span>
<span id="cb7-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_above"</span>,</span>
<span id="cb7-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_basement"</span>,</span>
<span id="cb7-14">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot15"</span>,</span>
<span id="cb7-15">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living15"</span>,</span>
<span id="cb7-16">    ]</span>
<span id="cb7-17">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> age_cols</span>
<span id="cb7-18">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> kmeans_transformer.column_names()</span>
<span id="cb7-19">)</span>
<span id="cb7-20">columns_selector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ColumnTransformer(</span>
<span id="cb7-21">    [(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selector"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"passthrough"</span>, columns)],</span>
<span id="cb7-22">    remainder<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"drop"</span>,</span>
<span id="cb7-23">    verbose_feature_names_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb7-24">).set_output(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pandas"</span>)</span>
<span id="cb7-25"></span>
<span id="cb7-26">simple_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline(</span>
<span id="cb7-27">    [</span>
<span id="cb7-28">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selector"</span>, columns_selector),</span>
<span id="cb7-29">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"linear"</span>, LinearRegression(fit_intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)),</span>
<span id="cb7-30">    ]</span>
<span id="cb7-31">)</span>
<span id="cb7-32">evaluate_model(simple_linear)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>r2_train=0.918, r2_test=0.862</code></pre>
</div>
</div>
<p>Not bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:</p>
<div id="cell-19" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline(</span>
<span id="cb9-2">    [</span>
<span id="cb9-3">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selector"</span>, columns_selector),</span>
<span id="cb9-4">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scale"</span>, StandardScaler()),</span>
<span id="cb9-5">        (</span>
<span id="cb9-6">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"linear"</span>,</span>
<span id="cb9-7">            RidgeCV(</span>
<span id="cb9-8">                fit_intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb9-9">                alphas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.logspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>),</span>
<span id="cb9-10">            ),</span>
<span id="cb9-11">        ),</span>
<span id="cb9-12">    ]</span>
<span id="cb9-13">)</span>
<span id="cb9-14">evaluate_model(ridge)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>r2_train=0.918, r2_test=0.863</code></pre>
</div>
</div>
<p>That didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.<br>
Let’s see if we can do better by using augmentations that are more appropriate for our data.</p>
<p>First let’s build a class for linear models with augmentation via regularization.<br>
We will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we showed above these are all we need from the augmentations.<br>
Since often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e.&nbsp;a block diagonal covariance matrix).</p>
<div id="cell-21" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> AugmentedLinearModel:</span>
<span id="cb11-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(</span>
<span id="cb11-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb11-4">        augmentation_moments: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># one item for each group of features</span></span>
<span id="cb11-5">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[</span>
<span id="cb11-6">                <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[Hashable],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># column names of features in the group</span></span>
<span id="cb11-7">                Callable[[pd.DataFrame], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># maps X to M and R</span></span>
<span id="cb11-8">            ]</span>
<span id="cb11-9">        ],</span>
<span id="cb11-10">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb11-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.augmentation_moments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> augmentation_moments</span>
<span id="cb11-12"></span>
<span id="cb11-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame, y: pd.Series) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb11-14">        means, covs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(</span>
<span id="cb11-15">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(</span>
<span id="cb11-16">                moments(X.loc[:, columns])</span>
<span id="cb11-17">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> columns, moments <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.augmentation_moments</span>
<span id="cb11-18">            )</span>
<span id="cb11-19">        )</span>
<span id="cb11-20">        M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.hstack(means)</span>
<span id="cb11-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://scikit-learn.org/stable/developers/develop.html#estimated-attributes</span></span>
<span id="cb11-22">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.block_diag(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>covs)</span>
<span id="cb11-23">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.solve(M.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R_, M.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y)</span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb11-25"></span>
<span id="cb11-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> pd.Series:</span>
<span id="cb11-27">        cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [col <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> cols, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.augmentation_moments <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> cols]</span>
<span id="cb11-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X.loc[:, cols] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_</span></code></pre></div>
</div>
<p>Now we will use this class to fit a model, but with a more appropriate regularization.</p>
<p>For the bathrooms and bedrooms features, we set a 10% probability that a bathroom is counted as half a bedroom.</p>
<div id="cell-23" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> bedrooms_bathrooms_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb12-2">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb12-3">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>])</span>
<span id="cb12-4">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bathrooms"</span>]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-5">    M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(mask, X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> v, X)</span>
<span id="cb12-6">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> p) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.outer(v, v) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask.values.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb12-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> M, R</span>
<span id="cb12-8"></span>
<span id="cb12-9"></span>
<span id="cb12-10">augmentation_moments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bedrooms"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bathrooms"</span>], bedrooms_bathrooms_moments)]</span></code></pre></div>
</div>
<p>Next we set a 5% perturbation for the features<br>
<code>sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15</code>, uncorrelated across the features.</p>
<div id="cell-25" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> relative_perturbation_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb13-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X.values, np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(X.values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb13-3"></span>
<span id="cb13-4"></span>
<span id="cb13-5">augmentation_moments.extend(</span>
<span id="cb13-6">    ([column], relative_perturbation_moments)</span>
<span id="cb13-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> [</span>
<span id="cb13-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living"</span>,</span>
<span id="cb13-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot"</span>,</span>
<span id="cb13-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_above"</span>,</span>
<span id="cb13-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_basement"</span>,</span>
<span id="cb13-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot15"</span>,</span>
<span id="cb13-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living15"</span>,</span>
<span id="cb13-14">    ]</span>
<span id="cb13-15">)</span></code></pre></div>
</div>
<p>Next, we set a perturbation of 0.01 for the features <code>floors, waterfront, view, condition, grade</code>, uncorrelated across the features.</p>
<div id="cell-27" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> absolute_perturbation_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb14-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X.values, X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb14-3"></span>
<span id="cb14-4"></span>
<span id="cb14-5">augmentation_moments.extend(</span>
<span id="cb14-6">    ([column], absolute_perturbation_moments)</span>
<span id="cb14-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"floors"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"waterfront"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"view"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"condition"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grade"</span>]</span>
<span id="cb14-8">)</span></code></pre></div>
</div>
<p>Next, we perturb <code>age</code> with a uniform distribution between -1 and 1. We need to calculate the moments for the different powers of age accordingly.</p>
<div id="cell-29" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> age_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb15-2">    a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>]].values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb15-3">    b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>]].values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb15-4">    max_power <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb15-5">    np1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> max_power <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb15-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments</span></span>
<span id="cb15-7">    mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>np1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>np1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (np1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> a))</span>
<span id="cb15-8">    mu_sum <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb15-9">    mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu[:, :max_power]</span>
<span id="cb15-10">    idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.add.outer(np.arange(max_power), np.arange(max_power))</span>
<span id="cb15-11">    c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu_sum[idx] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mu.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> mu</span>
<span id="cb15-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> mu, c</span>
<span id="cb15-13"></span>
<span id="cb15-14"></span>
<span id="cb15-15">augmentation_moments.append((age_cols, age_moments))</span></code></pre></div>
</div>
<p>And finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.</p>
<div id="cell-31" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> geo_cluster_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb16-2">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb16-3">    adj_mat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans_transformer.clusters_adjacency_matrix()</span>
<span id="cb16-4">    P <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.eye(adj_mat.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (adj_mat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> adj_mat.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (</span>
<span id="cb16-5">        <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> p</span>
<span id="cb16-6">    )  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># transition probabilities matrix</span></span>
<span id="cb16-7">    M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.csr_array(X.values) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> P</span>
<span id="cb16-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation</span></span>
<span id="cb16-9">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.diags(M.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> M.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> M</span>
<span id="cb16-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> M.toarray(), R.toarray()</span>
<span id="cb16-11"></span>
<span id="cb16-12"></span>
<span id="cb16-13">augmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))</span></code></pre></div>
</div>
<p>Le’t fit the augmented model and see how we did:</p>
<div id="cell-33" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">augmented_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AugmentedLinearModel(augmentation_moments)</span>
<span id="cb17-2">evaluate_model(augmented_linear)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>r2_train=0.903, r2_test=0.882</code></pre>
</div>
</div>
<p>We managed to improve the test accuracy, and reduce overfit.</p>
</section>
<section id="beyond-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="beyond-least-squares">Beyond least squares</h2>
<p>In this section we will extend the result to models that use a non-quadratic loss (e.g.&nbsp;logistic regression). The proof above doesn’t work for non-quadratic losses, but we can get an approximate result using a 2nd order taylor approximation for the loss.</p>
<p>The goal is to (approximately) express <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cmathrm%7BE%7D%20%20%0A%20%20%20%20%5Cleft%5B%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20l%20%5Cleft(%0A%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20%5C,;%5C,%20y_i%0A%20%20%20%20%5Cright)%0A%20%20%20%20%5Cright%5D,%0A%5Cend%7Balign*%7D"> as a sum of a non-augmented loss term, and a regularization term. Here, <img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i%5C,;%5C,y_i)"> is the loss function that measures how bad is the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">, given the true <img src="https://latex.codecogs.com/png.latex?y_i">.<br>
For example, for logistic regression we use the logistic loss <img src="https://latex.codecogs.com/png.latex?%0Al(%5Chat%7By%7D;%20y)%20=%20%5Clog%20%5Cleft(%201%20+%20%5Cexp%20%5Cleft(-y%20%5C,%20%5Chat%7By%7D%20%5Cright)%20%5Cright)%0A"> (with <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5C%7B%20-1,%201%20%5C%7D">).<br>
</p>
<p>First, we expand each loss term around <img src="https://latex.codecogs.com/png.latex?%5Cmu_i%20%5ET%20%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Al(%5Chat%7By%7D_i;%20y_i)%20%5Capprox%0Al(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%0A+%20l'(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%20%5Cleft(%20%5Chat%7By%7D_i%20-%20%5Cmu_i%20%5ET%20%5Ctheta%20%5Cright)%0A+%20%5Cfrac%7B1%7D%7B2%7D%20l''('mu_i%5ET%20%5Ctheta%5C,;%5C,y_i)%20%5Cleft(%20%5Chat%7By%7D_i%20-%20%5Cmu_i%20%5ET%20%5Ctheta%20%5Cright)%5E2%0A%5Cend%7Balign*%7D"> (The derivatives here are with respect to the first argument of <img src="https://latex.codecogs.com/png.latex?l">).</p>
<p>Substitute this into the expectation, we get: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathrm%7BE%7D%20%20%5Cleft%5B%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%0A%20%20%20%20%20%20%20%20l%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20%5C,;%5C,%20y_i%0A%20%20%20%20%20%20%20%20%5Cright)%0A%5Cright%5D%0A&amp;%5Capprox%0A%5Cmathrm%7BE%7D%20%20%5Cleft%5B%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%0A%20%20%20%20%20%20%20%20l(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%0A%20%20%20%20%20%20%20%20+%20l'(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20-%20%5Cmu_i%20%5ET%20%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%0A%20%20%20%20%20%20%20%20+%20%5Cfrac%7B1%7D%7B2%7D%20l''(%5Cmu_i%5ET%20%5Ctheta%5C,;%5C,y_i)%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20a%20%5Cleft(x_i,%20p_i%20%5Cright)%20%5ET%20%5Ctheta%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20%5Cmu_i%20%5ET%20%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%5E2%0A%5Cright%5D%0A%5C%5C&amp;=%0A%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%0A%20%20%20%20l(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%0A%20%20%20%20+%20l'(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%20%5Cmathrm%7BE%7D%5Cleft%5B%0A%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%0A%20%20%20%20%20%20%20%20-%20%5Cmu_i%0A%20%20%20%20%5Cright%5D%20%5ET%20%5Ctheta%0A%20%20%20%20+%20%5Cfrac%7B1%7D%7B2%7D%20l''(%5Cmu_i%5ET%20%5Ctheta%5C,;%5C,y_i)%20%5Cmathrm%7BE%7D%20%5Cleft%5B%0A%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20%5Cmu_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%20%5ET%20%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%5E2%0A%20%20%20%20%5Cright%5D%0A%5C%5C&amp;=%0A%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%0A%20%20%20%20l(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%0A%20%20%20%20+%20%5Cfrac%7B1%7D%7B2%7D%20l''(%5Cmu_i%5ET%20%5Ctheta%5C,;%5C,y_i)%0A%20%20%20%20%20%20%5Ctheta%5ET%20R_i%20%5Ctheta%0A%5Cend%7Balign*%7D"> So like in the least squares case, in the loss term we just replace each <img src="https://latex.codecogs.com/png.latex?x_i"> with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic).</p>
<p>I use this result to tell myself that it is reasonable to design a quadratic regularizer based on augmentation covariances, even for non-least squares models, as long as the covariances are small, and <img src="https://latex.codecogs.com/png.latex?l''"> is bounded (like in logistic regression).</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>I actually don’t know of any reference that discusses this connection between augmentation and regularization, although I’m sure it’s out there. Let me know if you know of one!</p>


</section>

 ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/augmentation_is_regularization/augmentation_is_regularization.html</guid>
  <pubDate>Sun, 14 Jan 2024 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
