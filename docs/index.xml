<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tom Shlomo&#39;s Blog</title>
<link>https://tomshlomo.github.io/blog/</link>
<atom:link href="https://tomshlomo.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Tom Shlomo&#39;s blog</description>
<generator>quarto-1.4.543</generator>
<lastBuildDate>Mon, 26 Feb 2024 22:00:00 GMT</lastBuildDate>
<item>
  <title>Efficient leave one out cross validation - part 1</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/loocv/loocv_part1.html</link>
  <description><![CDATA[ 





<p>Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.</p>
<p>Leave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.</p>
<p>For linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it’s <code>RidgeCV</code> estimator. Notably, this same trick extends beyond these linear models to any quadratically regularized least squares regression — a fact not widely recognized.</p>
<p>Taking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.</p>
<p>In this initial segment, we derive efficient LOOCV for the quadratic scenario and demonstrate its implementation in Python.</p>
<p>In part 2, we will build upon this derivation to cover non-quadratic scenarios and showcase these findings with a practical example dataset.</p>
<section id="notation" class="level1">
<h1>Notation</h1>
<p>We denote the number of samples in the training dataset as <img src="https://latex.codecogs.com/png.latex?n">.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?m">-dimensional feature vectors are represented as <img src="https://latex.codecogs.com/png.latex?x_1"> to <img src="https://latex.codecogs.com/png.latex?x_n">, forming the rows of matrix <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>Targets are denoted as <img src="https://latex.codecogs.com/png.latex?y_1"> to <img src="https://latex.codecogs.com/png.latex?y_n">, forming the vector <img src="https://latex.codecogs.com/png.latex?y">. The model’s prediction for the <img src="https://latex.codecogs.com/png.latex?i">-th training sample is <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%20=%20x_i%5ET%20%5Ctheta">, where <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the coefficients vector. <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20=%20X%20%5Ctheta"> represents the vector containing all predictions.</p>
<p>We fit <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to the training data by minimizing the combined loss and regularization terms: <span id="eq-theta-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20:=%20%5Carg%5Cmin_%7B%5Ctheta'%7D%20f(%5Ctheta').%0A%5Ctag%7B1%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%0Af(%5Ctheta')%20:=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20l(x_i%5ET%20%5Ctheta';%20y_i)%20+%20r(%5Ctheta').%0A"> Here, <img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)"> represents the loss function, quantifying the difference between the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> and the true target <img src="https://latex.codecogs.com/png.latex?y_i">, while <img src="https://latex.codecogs.com/png.latex?r"> is the regularization function. We assume <img src="https://latex.codecogs.com/png.latex?l"> (as a function of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">) and <img src="https://latex.codecogs.com/png.latex?r"> are convex and twice differentiable. Special cases of this model include ordinary least squares (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2">, <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta')%20=%200">), ridge regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2">, <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta')%20=%20%5Calpha%20%5C%7C%20%5Ctheta'%20%5C%7C%5E2">), logistic regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;y_i)%20=%20%5Clog%20%5Cleft(%201%20+%20e%5E%7B-y_i%20%5Chat%7By%7D_i%7D%5Cright)"> with <img src="https://latex.codecogs.com/png.latex?y_i%20%5Cin%20%5C%7B%20-1,%201%5C%7D">), and Poisson regression (<img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D_i;y_i)%20=%20y_i%20%5Chat%7By%7D_i%20-%20e%5E%7B%5Chat%7By%7D_i%7D">).</p>
<p>To denote the coefficients obtained by excluding the <img src="https://latex.codecogs.com/png.latex?j">-th example, we use <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%5E%7B(j)%7D%20=%20%5Carg%5Cmin_%7B%5Ctheta'%7D%20f%5E%7B(j)%7D%20(%5Ctheta')%0A"> where <img src="https://latex.codecogs.com/png.latex?%20f%5E%7B(j)%7D(%5Ctheta')%20:=%20%5Csum_%7Bi%20%5Cneq%20j%7D%20l(x_i%5ET%20%5Ctheta';%20y_i)%20+%20r(%5Ctheta')%20"> Similarly, <img src="https://latex.codecogs.com/png.latex?X%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?y%5E%7B(j)%7D">, represent <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> with the <img src="https://latex.codecogs.com/png.latex?j">-th row removed, respectively. We denote by <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> the predicted label for sample <img src="https://latex.codecogs.com/png.latex?j"> when it is left out: <span id="eq-y-tilde-j-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7By%7D_j%20:=%20x_j%20%5ET%20%5Ctheta%5E%7B(j)%7D%0A%5Ctag%7B2%7D"></span> Our goal is calculating <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">, for all <img src="https://latex.codecogs.com/png.latex?j">, efficiently.</p>
</section>
<section id="deriving-efficient-loocv-for-the-quadratic-case" class="level1">
<h1>Deriving efficient LOOCV for the quadratic case</h1>
<p>In scenarios where the loss function is the sum of squares loss, <img src="https://latex.codecogs.com/png.latex?%0Al(%5Chat%7By%7D_i;%20y_i)%20=%20(%5Chat%7By%7D_i%20-%20y_i)%5E2,%0A"> and the regularizer is quadratic <img src="https://latex.codecogs.com/png.latex?%0Ar(%5Ctheta')%20=%20%5Ctheta'%5ET%20R%20%5Ctheta'%0A"> where <img src="https://latex.codecogs.com/png.latex?R"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20m"> semi-positive definite matrix, the solution to the optimization problem Equation&nbsp;1 is obtained by solving the linear equation <sup>1</sup>: <span id="eq-theta-solve"><img src="https://latex.codecogs.com/png.latex?%0AA%20%5Ctheta%20=%20b.%0A%5Ctag%7B3%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20A%20&amp;:=%20X%5ET%20X%20+%20R%20%5C%5C%0A%20%20%20%20b%20&amp;:=%20X%5ET%20y.%0A%5Cend%7Balign*%7D"></p>
<p>Similarly, obtaining <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> requires solving <span id="eq-theta-j-solve"><img src="https://latex.codecogs.com/png.latex?%0AA%5E%7B(j)%7D%20%5Ctheta%5E%7B(j)%7D%20=%20b%5E%7B(j)%7D.%0A%5Ctag%7B4%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20A%5E%7B(j)%7D%20&amp;:=%20X%5E%7B(j)T%7D%20X%5E%7B(j)%7D%20+%20R%20%5C%5C%0A%20%20%20%20b%5E%7B(j)%7D%20&amp;:=%20X%5E%7B(j)T%7D%20y%5E%7B(j)%7D.%0A%5Cend%7Balign*%7D"> Forming and solving Equation&nbsp;4 for each <img src="https://latex.codecogs.com/png.latex?j"> has a time complexity of <img src="https://latex.codecogs.com/png.latex?O(m%5E3%20+%20n%20m%5E2)">. Thus, in a naive implementation, the overall complexity of LOOCV becomes <img src="https://latex.codecogs.com/png.latex?O(n%20m%5E3%20+%20n%5E2%20m%5E2)">, posing a significant computational challenge, particularly when <img src="https://latex.codecogs.com/png.latex?n"> is large.</p>
<p>Efficient LOOCV leverages the solution for Equation&nbsp;3 to calculate the solution for Equation&nbsp;4. We exploit the idea from computational linear algebra that solving multiple <img src="https://latex.codecogs.com/png.latex?m"> by <img src="https://latex.codecogs.com/png.latex?m"> equations with the same matrix has a time complexity similar to solving a single such equation. Thus, we solve, in addition to Equation&nbsp;3, the following <img src="https://latex.codecogs.com/png.latex?n"> equations: <img src="https://latex.codecogs.com/png.latex?%0AA%20t_j%20=%20x_j.%0A"></p>
<!-- The key idea behind efficient LOOCV lies in leveraging the solution for @eq-theta-solve to calculate the solution for @eq-theta-j-solve.
We will utilize an important idea from computational linear algebra: 
even though the complexity of solving a single $m$ by $m$ equation is $O(m^3)$, the complexity of solving $n$ such equations is not $O(nm^3)$, but $O(m^3 + nm^2)$, if all the equations share the same matrix. -->
<!-- the time required to solve multiple $m$ by $m$ equations that share the same matrix is almost identical to the time it takes to solve a single $m$ by $m$ equation. -->
<!-- Specifically, we will solve, in additional to @eq-theta-solve, the following $n$ equations: -->
<p>We start by noting that <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AX%5ETX%20&amp;=%20X%5E%7B(j)%5ET%7D%20X%5E%7B(j)%7D%20+%20x_j%20x_j%5ET%20%20%20%20%5C%5C%0AX%5ETy%20&amp;=%20X%5E%7B(j)%5ET%7D%20y%5E%7B(j)%7D%20+%20x_j%20y_j,%0A%5Cend%7Balign*%7D"> so we can write Equation&nbsp;4 like so: <img src="https://latex.codecogs.com/png.latex?%0A(A%20-%20x_j%20x_j%5ET)%20%5Ctheta%5E%7B(j)%7D%20=%20b%20-%20x_j%20y_j.%0A"> The usual way forward involves employing Sherman-Morrison formula, solving for <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> and substituting it in Equation&nbsp;2 to obtain an expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D">. However, there’s a better approach <sup>2</sup>: We rewrite Equation&nbsp;4 as <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20A%20%5Ctheta%5E%7B(j)%7D%20-%20x_j%20%5Ctilde%7By%7D_j%20&amp;=%20b%20-%20x_j%20y_j%20%5C%5C%0A%20%20%20%20%5Ctilde%7By%7D_j%20&amp;=%20x_j%20%5ET%20%5Ctheta%5E%7B(j)%7D%0A%5Cend%7Balign*%7D"> so instead of a single equation with one unknown (<img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D">), we now have two equations with two unknowns (<img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">). At first this seems more complicated, but notice that since the coefficient of <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> in the first equation is <img src="https://latex.codecogs.com/png.latex?A">, we can eliminate it: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Ctheta%5E%7B(j)%7D%20%20&amp;=%20A%5E%7B-1%7D%20(%20b%20-%20x_j%20y_j%20+%20x_j%20%5Ctilde%7By%7D_j%20)%20%5C%5C%0A&amp;=%20%5Ctheta%20-%20t_j%20(%20%20y_j%20-%20%5Ctilde%7By%7D_j%20)%0A%5Cend%7Balign*%7D"> substituting in the bottom equation, we can solve for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Ctilde%7By%7D_j%20&amp;=%20x_j%20%5ET%20%5Cleft(%20%5Ctheta%20-%20t_j%20(%20%20y_j%20-%20%5Ctilde%7By%7D_j%20)%20%5Cright)%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;=%20%5Chat%7By%7D_j%20-%20h_j%20(y_j%20-%20%5Ctilde%7By%7D_j)%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;=%20%5Cfrac%7B%5Chat%7By%7D_j%20-%20h_j%20y_j%7D%7B1-h_j%7D%0A%25%20%5C%5C%0A%25%20%5Ctilde%7By%7D_j%20&amp;=%20%5Cfrac%7B%5Chat%7By%7D_j%20-h_j%20%5Chat%7By%7D_j%20+%20h_j%20%5Chat%7By%7D_j%20-%20h_j%20y_j%7D%7B1-h_j%7D%0A%5C%5C%0A%5Ctilde%7By%7D_j%20&amp;=%20%5Chat%7By%7D_j%20+%20%5Cfrac%7Bh_j%20%7D%7B1-h_j%7D%20%5Cleft(%20%5Chat%7By%7D_j%20-%20y_j%20%5Cright)%0A%25%20%5C%5C%0A%25%20%5Ctilde%7By%7D_j%20&amp;=%20%5Cfrac%7B%5Chat%7By%7D_j%20-%20y_j%7D%7B1-h_j%7D%20+%20y_j%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?%0Ah_j%20:=%20x_j%20%5ET%20t_j.%0A"></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminder
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?y_j"> is the true label.<br>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_j"> is the prediction using all the data.<br>
<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> is the leave-one-out prediction.</p>
</div>
</div>
<p>That’s it! we got an expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> that doesn’t require inverting any matrix other than <img src="https://latex.codecogs.com/png.latex?A">. It also has a nice interpretation: the difference between the prediction and the LOO prediction is the difference between the prediction an the true label, “amplified” by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bh_j%20%7D%7B1-h_j%7D">.</p>
</section>
<section id="python-implementation" class="level1">
<h1>Python implementation</h1>
<p>The approach outlined above adapts seamlessly into code. We’ll construct an estimator resembling the sklearn style, featuring standard fit and predict methods, alongside a function to compute <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D">, the leave-one-out predictions:</p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Self</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scipy</span>
<span id="cb1-5"></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> LinearRegressionWithQuadraticRegularization:</span>
<span id="cb1-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> R</span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb1-12">        A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R</span>
<span id="cb1-13">        b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb1-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.solve(</span>
<span id="cb1-15">            A,</span>
<span id="cb1-16">            b,</span>
<span id="cb1-17">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-18">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-19">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb1-20">        )</span>
<span id="cb1-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb1-22"></span>
<span id="cb1-23">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb1-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_</span>
<span id="cb1-25"></span>
<span id="cb1-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit_loocv_predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb1-27">        A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R</span>
<span id="cb1-28">        b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb1-29">        temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.solve(</span>
<span id="cb1-30">            A,</span>
<span id="cb1-31">            np.vstack([b, X]).T,</span>
<span id="cb1-32">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-33">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-34">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb1-35">        )</span>
<span id="cb1-36">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-37">        t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb1-38">        h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ij,ji-&gt;i"</span>, X, t)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h[i] = np.dot(X[i, :], t[:, i])</span></span>
<span id="cb1-39">        y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict(X)</span>
<span id="cb1-40">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> h)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y)</span></code></pre></div>
</div>
<p>Let’s check that our method for calculating the leave-one-out predictions is correct on random data, and compare it’s run time to the usual leave-one-out procedure.</p>
<div id="cell-7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LeaveOneOut</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> time <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb2-3"></span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> standard_loocv(model, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb2-6">    y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty_like(y)</span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i, (train_index, test_index) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(LeaveOneOut().split(X)):</span>
<span id="cb2-8">        X_loo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[train_index, :]</span>
<span id="cb2-9">        y_loo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[train_index]</span>
<span id="cb2-10">        model.fit(X_loo, y_loo)</span>
<span id="cb2-11">        y_tilde[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X[test_index, :])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_tilde</span>
<span id="cb2-13"></span>
<span id="cb2-14"></span>
<span id="cb2-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> gen_random_data(n: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, m: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb2-16">    rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb2-17">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.standard_normal((n, m))</span>
<span id="cb2-18">    L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.standard_normal((m, m))</span>
<span id="cb2-19">    theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> rng.standard_normal(m)</span>
<span id="cb2-20">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rng.standard_normal(n)</span>
<span id="cb2-21">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> L.T  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># random positive definite matrix</span></span>
<span id="cb2-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X, y, R</span>
<span id="cb2-23"></span>
<span id="cb2-24"></span>
<span id="cb2-25">X, y, R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gen_random_data(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-26">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegressionWithQuadraticRegularization(R<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>R)</span>
<span id="cb2-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(</span>
<span id="cb2-28">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"max absolute error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(model.fit_loocv_predict(X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> standard_loocv(model, X, y)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb2-29">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>max absolute error: 1.243e-14</code></pre>
</div>
</div>
<p>Good, the two methods to calculate <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D"> give the same result. Let’s also compare the runtime:</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit model.fit_loocv_predict(X, y) </span>
<span id="cb4-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit standard_loocv(model, X, y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>34.6 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
2.39 ms ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</code></pre>
</div>
</div>
<p>Nice, a significant speedup. But that’s quite fast to begin with. Let’s increase <code>n</code> and <code>m</code>:</p>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">X, y, R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gen_random_data(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb6-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegressionWithQuadraticRegularization(R<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>R)</span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'max absolute error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(model.fit_loocv_predict(X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> standard_loocv(model, X, y)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb6-4"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit model.fit_loocv_predict(X, y) </span>
<span id="cb6-5"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit standard_loocv(model, X, y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>max absolute error: 8.527e-14
138 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
The slowest run took 4.24 times longer than the fastest. This could mean that an intermediate result is being cached.
822 ms ± 461 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p>Hmm… Much less impressive. In theory the speedup should improve as the problem size increases. This is likely due to some python inefficiencies, not the algorithm itself. Let’s try to improve by using JAX’s just-in-time compilation feature:</p>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> jax</span>
<span id="cb8-2"></span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> JitLinearRegressionWithQuadraticRegularization:</span>
<span id="cb8-5">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb8-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> R</span>
<span id="cb8-7"></span>
<span id="cb8-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb8-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._fit(X, y, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R)</span>
<span id="cb8-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb8-11"></span>
<span id="cb8-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._predict(X, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_)</span>
<span id="cb8-14"></span>
<span id="cb8-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit_loocv_predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-16">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_, y_tilde <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._fit_loocv_predict(X, y, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R)</span>
<span id="cb8-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_tilde</span>
<span id="cb8-18">    </span>
<span id="cb8-19">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb8-20">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@jax.jit</span></span>
<span id="cb8-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _fit(X, y, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> jax.scipy.linalg.solve(</span>
<span id="cb8-23">            X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> R, </span>
<span id="cb8-24">            X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y,</span>
<span id="cb8-25">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-26">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-27">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb8-28">        )</span>
<span id="cb8-29"></span>
<span id="cb8-30">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb8-31">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@jax.jit</span></span>
<span id="cb8-32">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _predict(X, theta) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta</span>
<span id="cb8-34"></span>
<span id="cb8-35">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb8-36">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@jax.jit</span></span>
<span id="cb8-37">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _fit_loocv_predict(X, y, R) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb8-38">        temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.scipy.linalg.solve(</span>
<span id="cb8-39">            X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> R,</span>
<span id="cb8-40">            jax.numpy.vstack([X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y, X]).T,</span>
<span id="cb8-41">            overwrite_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-42">            overwrite_b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-43">            assume_a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pos"</span>,</span>
<span id="cb8-44">        )</span>
<span id="cb8-45">        theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-46">        t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb8-47">        h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.numpy.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ij,ji-&gt;i"</span>, X, t)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h[i] = np.dot(X[i, :], t[:, i])</span></span>
<span id="cb8-48">        y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> theta</span>
<span id="cb8-49">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> theta, y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> h)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (y_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y)</span>
<span id="cb8-50">    </span>
<span id="cb8-51">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> JitLinearRegressionWithQuadraticRegularization(R<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>R)</span>
<span id="cb8-52"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'max absolute error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(model.fit_loocv_predict(X, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> standard_loocv(model, X, y)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3e}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb8-53"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit model.fit_loocv_predict(X, y).block_until_ready()</span>
<span id="cb8-54"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>timeit standard_loocv(model, X, y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>max absolute error: 4.780e-05
1.75 ms ± 232 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
353 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p>Much better!</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I am deliberately avoiding writing <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20A%5E%7B-1%7D%20b">, as <img src="https://latex.codecogs.com/png.latex?A"> does not have to be invertible for this equation to have a solution, and it allows me to avoid the usual “assuming full rank” caveats people tend to use here. Furthermore, it can mislead people into implementations like <code>np.linalg.inv(A) @ b</code>, which are less stable and efficient than implementations like <code>np.linalg.solve(A, b)</code>.↩︎</p></li>
<li id="fn2"><p>This approach translates better into code, as we get the expression for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D_j"> directly, without going through an expression for <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B(j)%7D"> first. I also think Sherman-Morisson is a bit too strong here and can obscure some insights, so it’s nice to avoid it. But actually the other approach is just halfway it’s proof (see for example <a href="https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf">here</a>).↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/loocv/loocv_part1.html</guid>
  <pubDate>Mon, 26 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>MUSIC as a sparse decomposition method</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/music/music.html</link>
  <description><![CDATA[ 





<p>MUSIC (MUltiple SIgnal Classification) is a popular algorithm used to estimating the directions of arrival (DOA) of waves recorded by an array of sensors.<br>
While very useful for this task, MUSIC is actually a more general parameters estimation method. However, conventional introductions to MUSIC often delve into the intricacies of equations tailored specifically for DOA estimation. These equations, laden with complex exponents or trigonometric identities, not only risk overwhelming readers but also obscure the fundamental insights that form the backbone of the method.<br>
An assumption most derivations of MUSIC rely on is access to the signals autocorrelation matrix. In practice, only it’s estimate is available (usually from very few samples), and in many cases the signals are not stationary (e.g.&nbsp;speech) so it is not even well defined. Furthermore, most derivations of the algorithm rely on the noise being white, which is often not realistic.<br>
Nevertheless, MUSIC can perform extremely well even when all these assumptions do not hold, which implies the existence of an alternative derivation. In this post I want to address the issues above by introducing MUSIC as a general method to (approximately) solve the multi-snapshot sparse decomposition problem.</p>
<section id="a-quick-introduction-to-sparse-decompositions" class="level3">
<h3 class="anchored" data-anchor-id="a-quick-introduction-to-sparse-decompositions">A quick introduction to sparse decompositions</h3>
<p>You obtained an <img src="https://latex.codecogs.com/png.latex?n">-dimensional vector <img src="https://latex.codecogs.com/png.latex?y">, and you know that it is a linear combination of several “atoms”. You don’t know which atoms, but you do know that they come from a given set of atoms <img src="https://latex.codecogs.com/png.latex?a_1,%20%5Cdots,%20a_m"> known as the dictionary. The goal is to decompose <img src="https://latex.codecogs.com/png.latex?y"> to it’s atoms, that is, find the atoms that participate in the linear combination. In matrix notation: <img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20Ax%0A"> where <img src="https://latex.codecogs.com/png.latex?A"> is the (known) dictionary matrix, with columns <img src="https://latex.codecogs.com/png.latex?a_1,%20%5Cdots,%20a_m">, and <img src="https://latex.codecogs.com/png.latex?x"> contains the (unknown) coefficient for each atom. The set of non-zero indices of <img src="https://latex.codecogs.com/png.latex?x">, which we also call the support, correspond to the atoms that participate in the linear combination.<br>
It might be tempting to simply solve for <img src="https://latex.codecogs.com/png.latex?x"> as both <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?y"> are known, but (at least for the interesting cases) <img src="https://latex.codecogs.com/png.latex?m%20%3E%20n"> and the system is under determined, that is, there are infinite ways to decompose <img src="https://latex.codecogs.com/png.latex?y"> as a linear combination of atoms.</p>
<p>In the setting of sparse decompositions, we add an additional prior to the problem: <img src="https://latex.codecogs.com/png.latex?y"> is composed of at most <img src="https://latex.codecogs.com/png.latex?k%20%3C%20n"> atoms, which means <img src="https://latex.codecogs.com/png.latex?x"> is <img src="https://latex.codecogs.com/png.latex?k">-sparse (has at most <img src="https://latex.codecogs.com/png.latex?k"> non zeros).</p>
<p>For example, in DOA estimation problems, we can use <img src="https://latex.codecogs.com/png.latex?y"> to represent a signal recorded by an array of <img src="https://latex.codecogs.com/png.latex?n">-sensors, <img src="https://latex.codecogs.com/png.latex?a_i"> the response of the array to a unit wave signal coming from the <img src="https://latex.codecogs.com/png.latex?i">’th direction, and <img src="https://latex.codecogs.com/png.latex?x_i"> the amplitude of the wave at coming from the <img src="https://latex.codecogs.com/png.latex?i">’th direction. <img src="https://latex.codecogs.com/png.latex?k">-sparsity of <img src="https://latex.codecogs.com/png.latex?x"> is equivalent to having at most <img src="https://latex.codecogs.com/png.latex?k"> waves active simultaneously, and decomposing <img src="https://latex.codecogs.com/png.latex?y"> into it’s atoms reveals their directions.</p>
<p>There are 2 important extensions to the basic sparse decomposition problem. The first is increasing robustness to noise or modeling errors, by looking for an approximate sparse decomposition instead of an exact one.<br>
For example, in machine learning, approximate sparse decomposition can be used for automatic feature selection in linear regression problems. Here <img src="https://latex.codecogs.com/png.latex?y"> contains the training data labels, <img src="https://latex.codecogs.com/png.latex?A"> contains the training data features, <img src="https://latex.codecogs.com/png.latex?x"> is the coefficient of each feature, and <img src="https://latex.codecogs.com/png.latex?k"> is the number of features to select.</p>
<p>The second extension is the multisnapshot (aka joint sparsity) problem, where instead of observing a single data vector <img src="https://latex.codecogs.com/png.latex?y">, we get <img src="https://latex.codecogs.com/png.latex?p"> vectors <img src="https://latex.codecogs.com/png.latex?y_1,%20%5Cdots,%20y_p">. In matrix notation: <img src="https://latex.codecogs.com/png.latex?%0AY%20=%20AX%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20Y%20&amp;:=%20%5Cbegin%7Bbmatrix%7D%20y_1%20&amp;&amp;%20%5Ccdots%20&amp;&amp;%20y_p%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D"> is the data matrix, and <img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D"> is the (unknown) coefficient of atom <img src="https://latex.codecogs.com/png.latex?a_i"> in <img src="https://latex.codecogs.com/png.latex?y_j">. Here, not only the columns of <img src="https://latex.codecogs.com/png.latex?X"> are <img src="https://latex.codecogs.com/png.latex?k">-sparse, they also share the same support. This means that the matrix <img src="https://latex.codecogs.com/png.latex?X"> is <img src="https://latex.codecogs.com/png.latex?k">-row-sparse, that is, has up to <img src="https://latex.codecogs.com/png.latex?k"> non-zero rows.<br>
In DOA estimation, the multisnapshot problem can be obtained by observing the signals at <img src="https://latex.codecogs.com/png.latex?p"> different (usually consecutive) times.<br>
In the feature selection for linear regression example, the multisnapshot problem is obtained when we have multiple labels to predict, and we want to select the same <img src="https://latex.codecogs.com/png.latex?k"> feature for each.</p>
<p>Solving sparse decomposition problems is in general a hard problem. It turns out that you can’t do much better than enumerating over all <img src="https://latex.codecogs.com/png.latex?m%20%5Cchoose%20k"> possibilities for the support, so in practice approximation methods are often used, e.g. Matching Pursuit, Orthogonal Matching Pursuit, Basis Pursuit, and LASSO. Sometimes, under additional assumptions, they provide some exactness guarantees. Although usually not presented as such, MUSIC is also an approximation method for noisy multisnapshot sparse decomposition, with some guarantees under additional assumptions.</p>
</section>
<section id="solving-the-noiseless-multisnapshot-case" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-noiseless-multisnapshot-case">Solving the noiseless multisnapshot case</h3>
<p>We will start by describing a method that can, under several assumptions, efficiently solve the noiseless joint sparsity problem. MUSIC can be viewed as an extension of this method for the noisy case.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?S"> denote the (unknown) support of <img src="https://latex.codecogs.com/png.latex?X">. We will denote by <img src="https://latex.codecogs.com/png.latex?X_S"> the matrix obtained by keeping only the rows in <img src="https://latex.codecogs.com/png.latex?S">, and by <img src="https://latex.codecogs.com/png.latex?A_S"> the matrix obtained by keeping only the columns in <img src="https://latex.codecogs.com/png.latex?S">. Note that with this notation, we have <img src="https://latex.codecogs.com/png.latex?%0AY%20=%20AX%20=%20A_S%20X_S.%0A"></p>
<p>MUSIC is based one two assumptions:</p>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D%20%5Cleft(X%20%5Cright)%20=%20%5Cleft%7C%20S%20%5Cright%7C"> (or equivalently, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D%20%5Cleft(X_S%20%5Cright)%20=%20%5Cleft%7C%20S%20%5Cright%7C">, as the two matrices obviously have the same row space).</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?a_i%20%5Cin%20%5Ctext%7BRange%7D%20%5Cleft(A_S%20%5Cright)"> if and only if <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20S">.</p></li>
</ol>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminder
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our goal is to find <img src="https://latex.codecogs.com/png.latex?S"> from <img src="https://latex.codecogs.com/png.latex?Y">.</p>
</div>
</div>
<p>Assumption 1 implies that <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRange%7D(Y)%0A=%0A%5Ctext%7BRange%7D(A_S%20X_S)%0A=%0A%5Ctext%7BRange%7D(A_S),%0A"> so we can get <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_S)"> from <img src="https://latex.codecogs.com/png.latex?Y">. Assumption 2 means that once we have <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(A_s)">, we can reconstruct <img src="https://latex.codecogs.com/png.latex?S"> simply by checking which atoms are in it. The implied algorithm is simple:</p>
<ol type="1">
<li><p>Calculate <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(Y)">.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?S=%5Cemptyset">,</p></li>
<li><p>for each <img src="https://latex.codecogs.com/png.latex?i">, if <img src="https://latex.codecogs.com/png.latex?a_i%20%5Cin%20%5Ctext%7BRange%7D(Y)">, add <img src="https://latex.codecogs.com/png.latex?i"> to <img src="https://latex.codecogs.com/png.latex?S">.</p></li>
</ol>
<p>Although correct and efficient, this is a terrible algorithm. Calculating the range of a matrix is numerically unstable, and even the slightest perturbation (e.g.&nbsp;a roundoff error) can change it drastically. But before we continue to the more noise-robust MUSIC, let’s discuss the implications of our two assumptions.</p>
<p>Assumption 2 means that to build an atom from a linear combination of other atoms, you need more than <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> atoms. This is related to something called the <a href="https://en.wikipedia.org/wiki/Spark_(mathematics)">spark</a> of <img src="https://latex.codecogs.com/png.latex?A">. We won’t get into it here, but conditions on the dictionary spark are elementary in basically every sparse decomposition method. For certain dictionaries, it can be shown that assumption 2 holds for any <img src="https://latex.codecogs.com/png.latex?S"> of size less than <img src="https://latex.codecogs.com/png.latex?n">. Specifically, this holds for the dictionary in DOA estimation <sup>1</sup>.</p>
<p>Assumption 2 is more restrictive. It means that no row of <img src="https://latex.codecogs.com/png.latex?X_S"> is a linear combination of the other rows. A necessary (but not sufficient) condition is <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C%20%5Cleq%20p">.<br>
In the DOA estimation, each rows of <img src="https://latex.codecogs.com/png.latex?X_S"> contains the samples of a different source. If the sources are uncorrelated (e.g.&nbsp;different speakers) and <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C%20%5Cleq%20p">, it is very unlikely that one is a linear combination of the others. If the sources are correlated, this doesn’t hold, and MUSIC can not be applied. This happens, for example, when one source is an echo of another, due to multi-path propagation.</p>
</section>
<section id="music" class="level3">
<h3 class="anchored" data-anchor-id="music">MUSIC</h3>
<p>The method above relies on the equation <span id="eq-range"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRange%7D(Y)%20=%20%5Ctext%7BRange%7D(A_S)%0A%5Ctag%7B1%7D"></span> which is true if <img src="https://latex.codecogs.com/png.latex?Y=AX">, but in practice the best we can hope for is <img src="https://latex.codecogs.com/png.latex?Y=AX+W">, where the noise term <img src="https://latex.codecogs.com/png.latex?W"> is very small compared to <img src="https://latex.codecogs.com/png.latex?AX">. Unfortunately, no matter how small <img src="https://latex.codecogs.com/png.latex?W"> is, due to the discontinuity of <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D">, Equation&nbsp;1 won’t even hold approximately. In fact, if <img src="https://latex.codecogs.com/png.latex?p%20%5Cgeq%20n">, we will almost surely have <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D(Y)%20=%20%5Cmathbb%7BR%7D%5En">, and the algorithm above would just yield <img src="https://latex.codecogs.com/png.latex?S=%5Cleft%5C%7B1,%20%5Cdots,%20%20m%20%5Cright%5C%7D">.</p>
<p>MUSIC makes 2 modifications the the algorithm above.<br>
First, we replace <img src="https://latex.codecogs.com/png.latex?Y"> with <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D">, a rank-<img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation of <img src="https://latex.codecogs.com/png.latex?Y">.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> is assumed known is MUSIC. It can be avoided, sometimes, using model selection methods.</p>
</div>
</div>
<p>Since <img src="https://latex.codecogs.com/png.latex?AX"> has rank <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C">, taking a rank <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation of <img src="https://latex.codecogs.com/png.latex?Y"> has a denoising effect<sup>2</sup>. Indeed, unlike <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20Y%20%5Cright)">, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)"> is a good estimate for <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20A_S%20%5Cright)"> when <img src="https://latex.codecogs.com/png.latex?W"> is small, but it is not exact: almost surely, none of the atoms would lie exactly in it. So the second modification soften the requirement that <img src="https://latex.codecogs.com/png.latex?a_i%20%5Cin%20%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)"> to add <img src="https://latex.codecogs.com/png.latex?i"> to <img src="https://latex.codecogs.com/png.latex?S">. Instead, we will require that <img src="https://latex.codecogs.com/png.latex?a_i"> is “almost in” <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)">, by checking if it looses little magnitude when projected onto it: <img src="https://latex.codecogs.com/png.latex?%0Ac_i%20:=%20%5Cfrac%7B%5C%7C%20%5Ctext%7BProj%7D_%7B%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)%7D(a_i)%20%5C%7C%5E2%7D%0A%7B%5C%7C%20a_i%20%5C%7C%5E2%20%7D%0A%5Ctext%7B%20is%20close%20to%201%7D%0A%5Cimplies%0A%5Ctext%7B%20add%20$i$%20to%20$S$%7D%0A"> (what “is close” means exactly differs between implementations. When the atoms can be ordered, like in DOA estimation, it is common to use a peak selection algorithm).</p>
<p>As we said above, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D"> is a rank-<img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation to <img src="https://latex.codecogs.com/png.latex?Y">. In MUSIC, we use the best rank-<img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> approximation in the least squares sense, which is given by the truncated singular value decomposition (SVD) of <img src="https://latex.codecogs.com/png.latex?Y">. Note that we don’t really need to calculate <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D"> itself, all we really need is it’s range projection operator. Well, a nice about the SVD is that we can get it directly: <img src="https://latex.codecogs.com/png.latex?%0A%5Clabel%7Bmusic_final%7D%0Ac_i%20=%20%5Cfrac%7B%5C%7C%20U%5ET%20a_i%5C%7C%5E2%7D%7B%5C%7C%20a_i%5C%7C%5E2%7D.%0A"> where the columns of <img src="https://latex.codecogs.com/png.latex?U"> are the first <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C"> left singular vectors (which form an orthonormal basis for <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20%5Ctilde%7BY%7D%20%5Cright)">).</p>
<p>To wrap things up, a few notes to connect the above to the “usual” MUSIC derivation:</p>
<ul>
<li><p>The left singular vectors of <img src="https://latex.codecogs.com/png.latex?Y"> are the eigenvectors of <img src="https://latex.codecogs.com/png.latex?p%5E%7B-1%7D%20YY%5ET">, which, in a stochastic setting, can be viewed as an estimate of the autocorrelation matrix.</p></li>
<li><p>The usual MUSIC formula use the last <img src="https://latex.codecogs.com/png.latex?n-%5Cleft%7C%20S%20%5Cright%7C"> left singular vectors (which we stack to the columns of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BU%7D">) instead of the first <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%20S%20%5Cright%7C">. From the Pythagorean theorem <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%20a_i%20%5C%7C%20%5E2%20=%20%5C%7CU%5ET%20a_i%20%5C%7C%5E2%20+%20%5C%7C%20%5Cbar%7BU%7D%5ET%20a_i%20%5C%7C%5E2,%0A"> so we can write <img src="https://latex.codecogs.com/png.latex?c_i"> as follows: <img src="https://latex.codecogs.com/png.latex?%0Ac_i%20=%201%20-%5Cfrac%7B%0A%5C%7C%20%5Cbar%7BU%7D%5ET%20a_i%20%5C%7C%5E2%0A%7D%7B%5C%7C%20a_i%5C%7C%5E2%7D.%0A"></p></li>
<li><p>In MUSIC for DOA/spectral estimation, it is common to plot <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B1-c_i%7D">, and call it the “pseudo-spectrum”. The 1-over-1-minus transform maps numbers close to 1 to very large numbers, which often results in very beautiful and pointy (but somewhat misleading) plots.</p></li>
</ul>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>With linear, equally spaced array of sensors, if the usual anti-aliasing conditions hold: the spacing between the sensors is smaller than half the wavelength, and no 2 directions lie on the same cone who’s axis contains the array.↩︎</p></li>
<li id="fn2"><p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRange%7D%20%5Cleft(%20A_S%20%5Cright)"> is sometimes called the signal subspace, and the subspace orthogonal to it the noise subspace.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/music/music.html</guid>
  <pubDate>Mon, 29 Jan 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>A practical interpertation of the Pearson correlation coefficient</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/pearson_correlation/pearson_correlation.html</link>
  <description><![CDATA[ 





<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crenewcommand%7B%5CE%7D%5B1%5D%7B%5Coperatorname%7BE%7D%5Cleft%5B#1%5Cright%5D%7D%0A%5Crenewcommand%7B%5Cvar%7D%5B1%5D%7B%5Coperatorname%7BVar%7D%20%5Cleft%5B#1%20%5Cright%5D%7D%0A%5Crenewcommand%7B%5Ccov%7D%5B1%5D%7B%5Coperatorname%7BCov%7D%20%5Cleft%5B#1%20%5Cright%5D%20%7D%0A"> My goal is to explain the Pearson correlation coefficient without using the word correlation, which is often used to describe it.<br>
The Pearson correlation coefficient of two random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Crho%20:=%20%5Cfrac%7B%5Csigma_%7BXY%7D%7D%7B%5Csigma_X%20%5Csigma_Y%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Csigma_X"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_Y"> are the standard deviation of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> respectively, and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7BXY%7D"> is their covariance.</p>
<p>A motivation for the definition <img src="https://latex.codecogs.com/png.latex?%5Crho"> comes from the problem of estimating <img src="https://latex.codecogs.com/png.latex?Y"> from an observation of <img src="https://latex.codecogs.com/png.latex?X">. It turns out that in the optimal (lowest MSE) linear estimation, <em>the number of standard deviations <img src="https://latex.codecogs.com/png.latex?Y"> is above it’s mean is <img src="https://latex.codecogs.com/png.latex?%5Crho"> times the number of standard deviations <img src="https://latex.codecogs.com/png.latex?X"> is above it’s mean.</em><br>
For example, consider a population of people where height and weight are correlated with <img src="https://latex.codecogs.com/png.latex?%5Crho=0.72">, heights are distributed with mean <img src="https://latex.codecogs.com/png.latex?170">cm and a standard deviation of <img src="https://latex.codecogs.com/png.latex?10">cm, weights are distributed with mean <img src="https://latex.codecogs.com/png.latex?70">Kg and a standard deviation of <img src="https://latex.codecogs.com/png.latex?20">Kg. If we know that the height of a certain person is <img src="https://latex.codecogs.com/png.latex?190">cm, a good guess for it’s weight is <img src="https://latex.codecogs.com/png.latex?70%20+%202%20%5Ccdot%200.72%20%5Ccdot%2020%20=%2098.8">Kg.</p>
<p>The proof is very simple. Since we are dealing with linear (actually, affine) estimators, we need to show that the <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?b"> that would minimize <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BMSE%7D%20:=%20%5CE%7B%20%5Cleft(%20%5Chat%7BY%7D%20-%20Y%20%5Cright)%20%5E2%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20:=%20a%20(X%20-%20%5Cmu_x)%20+%20b">, are <img src="https://latex.codecogs.com/png.latex?%5Crho%20%5Csigma_Y%20/%20%5Csigma_X"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu_Y">.<br>
The MSE is the sum of bias squared and variance. The variance doesn’t depend on <img src="https://latex.codecogs.com/png.latex?b">, and the bias is <img src="https://latex.codecogs.com/png.latex?%5CE%7B%20%20%5Chat%7BY%7D%20-%20Y%20%7D%20=%20b%20-%20%5Cmu_Y"> which doesn’t depend on <img src="https://latex.codecogs.com/png.latex?a">, so <img src="https://latex.codecogs.com/png.latex?b=%5Cmu_Y">. To minimize the variance, we simplify: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cvar%7B%5Chat%7BY%7D%20-%20Y%7D%0A&amp;=%20%5Cvar%7B%5Chat%7BY%7D%7D%20+%20%5Cvar%7BY%7D%20-%202%20%5Ccov%7B%5Chat%7BY%7D,%20Y%7D%0A%5C%5C&amp;=%20%5Csigma_x%20%5E%202%20a%5E2%0A%20%20%20+%20%5Csigma_Y%20%5E2%0A%20%20%20-2%20%20%5Csigma_%7BXY%7D%20a.%0A%5Cend%7Balign*%7D%0A"> This is just a parabola in <img src="https://latex.codecogs.com/png.latex?a">, so the optimal <img src="https://latex.codecogs.com/png.latex?a"> is <img src="https://latex.codecogs.com/png.latex?%0Aa=%5Cfrac%7B2%20%5Csigma_%7BXY%7D%7D%20%7B2%20%5Csigma_X%20%5E2%7D%0A=%0A%5Crho%20%5Cfrac%7B%5Csigma_Y%20%7D%20%7B%5Csigma_X%20%7D%0A"> (which is what we wanted to show).</p>
<p>The estimator is unbiased, so it’s MSE is equal to it’s variance: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BMSE%7D%20=%20%5Csigma_Y%20%5E2%20(1%20-%20%5Crho%20%5E%202).%0A"> This equation gives another concrete interpretation of <img src="https://latex.codecogs.com/png.latex?%5Crho">: <em>If <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are correlated with coefficient <img src="https://latex.codecogs.com/png.latex?%5Crho">, observing <img src="https://latex.codecogs.com/png.latex?X"> will decrease the standard deviation of a <img src="https://latex.codecogs.com/png.latex?Y"> estimate by a factor of at least <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B1%20-%20%5Crho%5E2%7D">.</em><br>
“at least” since the the optimal linear estimator is equal or worse than the optimal estimator.<br>
In the example above, knowing the height decreases weight estimation standard deviation from 20Kg to <img src="https://latex.codecogs.com/png.latex?20%20%20(1%20-%200.72%5E2)%20=%209.6">Kg.</p>
<p>Randomly ordered notes:</p>
<ol type="1">
<li><p>If <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are jointly Gaussian, the optimal linear estimator is also the optimal estimator.</p></li>
<li><p>The “mean” in “MSE” is an average over the joint distribution of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, which is different than over the distribution of <img src="https://latex.codecogs.com/png.latex?Y"> given <img src="https://latex.codecogs.com/png.latex?X">, for which our estimator is not the optimal linear estimator (and biased).<br>
In our example, we estimated the weight to be <img src="https://latex.codecogs.com/png.latex?98.8">Kg with variance <img src="https://latex.codecogs.com/png.latex?9.6%5E2">. It doesn’t mean that if we will sample random people with height <img src="https://latex.codecogs.com/png.latex?190">cm, we would get a mean weight of <img src="https://latex.codecogs.com/png.latex?98.8">Kg and variance smaller than <img src="https://latex.codecogs.com/png.latex?9.6%5E2">. It means that if we sample random people, and estimate their weight from their height using the optimal linear estimator, our error will be zero on average, and with variance <img src="https://latex.codecogs.com/png.latex?9.6%5E2">. If we use the optimal estimator, the <img src="https://latex.codecogs.com/png.latex?9.6%5E2"> is an upper bound on the variance.</p></li>
<li><p>The sentence “<img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are not correlated” now has a concrete meaning: it means that the optimal linear estimator of <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X"> will be the mean of <img src="https://latex.codecogs.com/png.latex?Y">, ignoring <img src="https://latex.codecogs.com/png.latex?X"> completely.</p></li>
<li><p>The discussion above is “Bayesian”, in the sense that it assumes you have some knowledge about the distribution of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">. In practice we usually get <img src="https://latex.codecogs.com/png.latex?n"> samples of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> pairs, and we use plug-in estimators to estimate the means, variances, and covariance, which we will then use build our <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X"> linear estimator.<br>
Machine learning people would say: we can use the samples to train a linear regression model to predict <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X"> directly. Sounds better, more “end-to-end”y, but actually it gives exactly the same result<sup>1</sup>. Proof:<br>
We denote by <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> be the vectors of samples of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B1%7D"> a vector of ones, and by <img src="https://latex.codecogs.com/png.latex?A"> the matrix whose first column is <img src="https://latex.codecogs.com/png.latex?x"> and the second column is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B1%7D">. The coefficients of the linear model are given by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%5Ctheta_%7B%5Ctext%7Bslope%7D%7D%20%5C%5C%0A%20%20%20%20%20%5Ctheta_%7B%5Ctext%7Bintercept%7D%7D%0A%5Cend%7Bbmatrix%7D%0A&amp;:=%0A%5Ctext%7Bargmin%7D_%5Ctheta%20%5C%7C%20A%20%5Ctheta%20-%20y%20%5C%7C%5E2%0A%5C%5C&amp;=%0A%5Cleft(%20A%20%5ET%20A%20%5Cright)%5E%7B-1%7D%20A%5ET%20y%0A%5C%5C&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%5C%7Cx%5C%7C%5E2%20&amp;&amp;%20%5Cmathbf%7B1%7D%5ETx%20%5C%5C%0A%20%20%20%20%20%5Cmathbf%7B1%7D%5ET%20x%20%20&amp;&amp;%20%5Cmathbf%7B1%7D%5ET%20%5Cmathbf%7B1%7D%0A%5Cend%7Bbmatrix%7D%0A%5E%7B-1%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20x%5ET%20y%20%5C%5C%0A%20%20%20%20%20%5Cmathbf%7B1%7D%20%5ET%20y%0A%5Cend%7Bbmatrix%7D%0A%5C%5C&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%5Csigma_X%5E2%20+%20%5Cmu_X%5E2%20&amp;&amp;%20%5Cmu_X%20%5C%5C%0A%20%20%20%20%20%5Cmu_X%20%20&amp;&amp;%201%0A%5Cend%7Bbmatrix%7D%0A%5E%7B-1%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%5Csigma_%7BXY%7D%20+%20%5Cmu_X%20%5Cmu_Y%20%5C%5C%0A%20%20%20%20%20%5Cmu_Y%0A%5Cend%7Bbmatrix%7D%0A%5C%5C&amp;=%0A%5Cfrac%7B1%7D%7B%5Csigma_X%20%5E2%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%201%20&amp;&amp;%20-%5Cmu_X%20%5C%5C%0A%20%20%20%20%20-%5Cmu_X%20%20&amp;&amp;%20%5Csigma_X%5E2%20+%20%5Cmu_X%5E2%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%5Csigma_%7BXY%7D%20+%20%5Cmu_X%20%5Cmu_Y%20%5C%5C%0A%20%20%20%20%20%5Cmu_Y%0A%5Cend%7Bbmatrix%7D%0A%5C%5C&amp;=%0A%5Cfrac%7B1%7D%7B%5Csigma_X%20%5E2%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%5Csigma_%7BXY%7D%20%5C%5C%0A%20%20%20%20%20-%5Cmu_X%20%5Csigma_%7BXY%7D%20+%20%5Csigma_X%5E2%20%5Cmu_Y%0A%5Cend%7Bbmatrix%7D%0A%5C%5C&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20a%20%5C%5C%0A%20%20%20%20%20-%5Cmu_X%20a%20+%20b%0A%5Cend%7Bbmatrix%7D.%0A%5Cend%7Balign*%7D%0A"> Note also that the r2-score of this fit is equal to <img src="https://latex.codecogs.com/png.latex?%5Crho%5E2">: <img src="https://latex.codecogs.com/png.latex?%0Ar%5E2%20:=%201%20-%20%5Cfrac%7B%5Ctext%7BMSE%7D%7D%7B%5Csigma_Y%5E2%7D%20=%201%20-%20%5Cfrac%7B%5Csigma_Y%20%5E2%20%5Cleft(1-%5Crho%5E2%5Cright)%7D%7B%5Csigma_Y%20%5E2%7D%20=%20%5Crho%5E2.%0A"></p></li>
</ol>
<!-- 
The optima
The optimal $a=\frac{2 \sigma_{XY}} {2 \sigma_X ^ 2}$

$$
\begin{align*}
a 
&= \text{argmin}_{a'} \var{\hat{Y} - Y\right] \
&= \text{argmin}_{a'} \var{a (X - \mu_X) - Y\right] \
&= \text{argmin}_{a'} 
    \var{a \left(X - \mu_X\right) \right] 
    + \var{ Y\right] 
    -2 \mathrm{Cov}\left[a \left(X - \mu_X\right), Y\right] \
&= \text{argmin}_{a'} 
    a^2 \sigma_x ^ 2
    + \sigma_Y ^2
    -2 a \sigma_{XY}
\end{align*}
$$
and the variance do
We with expanding the MSE as the sum of the squared bias and variance
$$
\begin{align*}
\text{MSE} &=
\left(\E{ \left[\hat{Y} - Y \right] \right)^2
+ \mathrm{Var} \left[\hat{Y} - Y \right]
\&=
b ^ 2
+ \mathrm{Var} \left[a (x - \mu_X) - Y \right]
\end{align*}
$$ -->
<!-- 
Suppose the Pearson correlation coefficient is $\rho$, 
and you wish to estimate $Y$ based on a given observation of $X$
that is $n$ standard deviations away from the mean.
The optimal linear estimate is $n \rho$ standard deviations away from the mean.

If the observation of $X$ is $n$
It turns out that the optimal linear estimation is $\rho$ 
then the optimal linear estimation of $Y$ given a sample of $X$ is
then the optimal linear estimation of $Y$ from $X$ is obtained by 
1. Calculate by how many standard deviations the sampled $X$ is above it's mean.
2. multiply by $\rho$.
3. This is by how many standard deviations the estimate of $Y$ is above it's mean. -->
<!-- $$
\begin{align*}
\E{ \left[ \left(a (x - \mu_x) + b - y \right) ^ 2 \right]
&= 
a ^ 2 \E{ \left[ \left( x - \mu_x \right) ^ 2 \right]
+
\E{ \left[ \left(b - y \right) ^ 2 \right]
+
a \E{ \left[ \left(x - \mu_x\right) \left(b - y \right) \right]
\&=
a ^ 2 \sigma_x ^2
+
\sigma_y ^ 2
+
a \, \sigma_{xy}
\end{align*}
$$ -->




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Assuming we don’t use <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/pearson_correlation/pearson_correlation.html</guid>
  <pubDate>Fri, 19 Jan 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Augmentation is Regularization</title>
  <dc:creator>Tom Shlomo</dc:creator>
  <link>https://tomshlomo.github.io/blog/posts/augmentation_is_regularization/augmentation_is_regularization.html</link>
  <description><![CDATA[ 





<p>Training data augmentation enhances the training dataset by applying transformations to existing training data instances. The specific transformations vary depending on the type of data involved, and this flexibility allows to leverage domain knowledge, such as known invariants, effectively. The goal is to introduce variability and increase the diversity of the training set, allowing the model to better generalize to unseen data and exhibit improved robustness. Despite the advantages, training data augmentation introduces an inherent computational cost: the increased volume of data requires additional computational resources, impacting both training time and memory requirements.</p>
<p>As we will show below, for linear models with the sum of squares loss, training data augmentation is equivalent to adding quadratic regularization term, which implies that the computational cost of fitting a model to an augmented dataset is the same as using no augmentation at all!</p>
<p>This link between augmentation and regularization is useful in the other direction as well: it gives a concrete interpretation to the value of regularization hyperparameters, and can be used to avoid costly hyperparameters tuning (<code>np.logspace(-6, 6, 100)</code> much?), and to design regularizers that are more appropriate to the data than the simple ones (i.e sum of squares regularization used in ridge regression).</p>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<p>Suppose we have a training data set comprised of <img src="https://latex.codecogs.com/png.latex?n"> pairs <img src="https://latex.codecogs.com/png.latex?x_i,%5C,y_i"> for <img src="https://latex.codecogs.com/png.latex?i=0,%20%5Cdots,%20n-1">, where <img src="https://latex.codecogs.com/png.latex?x_i"> is the <img src="https://latex.codecogs.com/png.latex?d"> dimensional feature vector of the <img src="https://latex.codecogs.com/png.latex?i">’th training data, and <img src="https://latex.codecogs.com/png.latex?y_i"> is the corresponding label. Here we will assume <img src="https://latex.codecogs.com/png.latex?y_i%20%5Cin%20%5Cmathrm%7BR%7D">, however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by <img src="https://latex.codecogs.com/png.latex?X"> the <img src="https://latex.codecogs.com/png.latex?n">-by-<img src="https://latex.codecogs.com/png.latex?d"> matrix with rows <img src="https://latex.codecogs.com/png.latex?x_0%5ET,%20%5Cdots,%20x_%7Bn-1%7D%5ET"> and by <img src="https://latex.codecogs.com/png.latex?y"> the <img src="https://latex.codecogs.com/png.latex?n">-vector with entries <img src="https://latex.codecogs.com/png.latex?y_0,%20%5Cdots,%20y_%7Bn-1%7D">.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?a:%5Cmathrm%7BR%7D%5Ed%20%5Ctimes%20%5Cmathcal%7BP%7D%20%20%5Cmapsto%20%5Cmathrm%7BR%7D%5Ed"> denote the augmentation function that given the augmentation params <img src="https://latex.codecogs.com/png.latex?p%20%5Cin%20%5Cmathcal%7BP%7D">, maps a feature vector <img src="https://latex.codecogs.com/png.latex?x"> to a transformed feature vector. The augmentation parameters <img src="https://latex.codecogs.com/png.latex?p"> are usually sampled randomly from a given distribution. For example, for image data, <img src="https://latex.codecogs.com/png.latex?a"> is often a composition of small shifts, rotations, brightness changes, etc. while <img src="https://latex.codecogs.com/png.latex?p"> specifies the amount of shifting, rotation and brightness change.</p>
</section>
<section id="ordinary-least-squares-ols" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-least-squares-ols">Ordinary least squares (OLS)</h2>
<p>Let’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.<br>
To fit an OLS model, we find a vector of coefficients <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%5Ctext%7BOLS%7D"> that minimizes the sum of squared training errors: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BOLS%7D%20&amp;:=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20%5Cleft(%0A%20%20%20%20x_i%20%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%5Cright)%20%5E2%0A%20%20%20%20%5C%5C&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5C%7C%20X%20%5Ctheta%20-%20y%20%5C%7C%5E2%20%5Ctag%7B1%7D%0A%5Cend%7Balign*%7D"> To solve the optimization problem (1), we solve the equation <img src="https://latex.codecogs.com/png.latex?X%5ETX%20%5Ctheta_%5Ctext%7BOLS%7D%20=%20X%5ET%20y">, which has time complexity <img src="https://latex.codecogs.com/png.latex?O(n%20d%5E2)">.</p>
</section>
<section id="augmented-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="augmented-least-squares">Augmented least squares</h2>
<p>We will now fit a model by finding coefficients <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%5Ctext%7BALS%7D"> that minimize the expected error over the augmented training dataset: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BALS%7D%20&amp;:=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5Cmathrm%7BE%7D%20%20%0A%20%20%20%20%5Cleft%5B%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20%5Cleft(%0A%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%5Cright)%20%5E2%0A%20%20%20%20%5Cright%5D,%20%5Ctag%7B2%7D%0A%5Cend%7Balign*%7D"> where the expectation is over <img src="https://latex.codecogs.com/png.latex?p_0,%5Cdots,%20p_%7Bn-1%7D">, the random augmentation parameters. As we will see below, <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%5Ctext%7BALS%7D"> depends on <img src="https://latex.codecogs.com/png.latex?a"> and the distribution of <img src="https://latex.codecogs.com/png.latex?p"> only through the 2nd order moments, which we denote by <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cmu_i%20&amp;:=%20%5Cmathrm%7BE%7D%20%5Cleft%5Ba(x_i,%20p_i)%20%5Cright%5D%5C%5C%0A%20%20%20%20R_i%20&amp;:=%20%5Cmathrm%7BC%7D%5Ctext%7Bov%7D%20%5Cleft%5B%20a(x_i,%20p_i)%20%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>Continuing from (2), we use the standard trick of subtracting and adding the mean: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BALS%7D%20&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%20%5Cmathrm%7BE%7D%20%20%0A%20%20%20%20%5Cleft%5B%0A%20%20%20%20%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%0A%20%20%20%20%20%20%20%20%20%20%20%20+%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20-%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%5ET%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%20%5E2%0A%20%20%20%20%5Cright%5D%0A%5Cend%7Balign*%7D"> Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Ctheta_%5Ctext%7BALS%7D%20&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%0A%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%5ET%20%5Ctheta%20-%20y_i%0A%20%20%20%20%20%20%20%20%5Cright)%0A%20%20%20%20%20%20%20%20+%0A%20%20%20%20%20%20%20%20%5Cmathrm%7BE%7D%20%5Cleft%5B%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cleft(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20-%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cmu_i%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cright)%5ET%5Ctheta%0A%20%20%20%20%20%20%20%20%5Cright)%5E2%20%5Cright%5D%20%5C%5C%0A%20%20%20%20&amp;=%20%5Ctext%7Bargmin%7D%20_%5Ctheta%0A%20%20%20%20%5C%7C%20M%20%5Ctheta%20-%20y%5C%7C%5E2%20+%20%5Ctheta%20%5ET%20R%20%5Ctheta,%0A%20%20%20%20%5Ctag%7B3%7D%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?M"> is the <img src="https://latex.codecogs.com/png.latex?n">-by-<img src="https://latex.codecogs.com/png.latex?d"> matrix whose rows are <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%5ET,%20%5Cdots,%20%5Cmu_%7Bn-1%7D%5ET">, and <img src="https://latex.codecogs.com/png.latex?%0AR%20:=%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20R_i.%0A"> Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace <img src="https://latex.codecogs.com/png.latex?X"> with it’s mean, and use the sum of all covariances as the regularization matrix.</p>
<p>To solve the optimization problem (3), we solve the equation <img src="https://latex.codecogs.com/png.latex?(X%5ET%20X%20+%20R)%20%5Ctheta_%5Ctext%7BALS%7D%20=%20X%5ET%20y">, which has the same <img src="https://latex.codecogs.com/png.latex?O(n%20d%5E2)"> complexity as OLS.</p>
</section>
<section id="ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression">Ridge regression</h2>
<p>Ride regression (aka Tykhonov regularization) has the form (3) with <img src="https://latex.codecogs.com/png.latex?M=X"> and <img src="https://latex.codecogs.com/png.latex?R=%5Clambda%20I">. As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance <img src="https://latex.codecogs.com/png.latex?%5Clambda/n">, uncorrelated across features.<br>
This interpretation of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> can be used to set it (at least roughly): just think what level of perturbation <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is reasonable for your features, and set <img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20n%20%5Csigma%5E2">.<br>
This also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal <img src="https://latex.codecogs.com/png.latex?R">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20R=%20n%20%5C,%20%5Ctext%7Bdiag%7D%20%5Cleft(%0A%20%20%20%20%20%20%20%20%5Csigma_0%5E2,%20%5Cdots,%20%5Csigma_%7Bd-1%7D%5E2%0A%20%20%20%20%5Cright)%0A%5Cend%7Balign*%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Csigma_i"> is the standard deviation of the perturbation of feature <img src="https://latex.codecogs.com/png.latex?i">.</p>
<p>Another option is to scale the transformations before fit, e.g using sklearn’s <code>StandardScaler</code>. With all features scaled to have unit variance, setting <img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20n%20%5C,%2010%20%5E%7B-6%7D"> is a sensible rule of thumb, as it is often reasonable to assume a <img src="https://latex.codecogs.com/png.latex?0.1%5C%25"> perturbation.</p>
<p>Note that often the model includes an intercept (aka constant) term by adding a column of ones to <img src="https://latex.codecogs.com/png.latex?X">. Since this column remain unchanged through any augmenting transformation, the corresponding row and column of <img src="https://latex.codecogs.com/png.latex?R"> should be all zeros.</p>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>For the example we are gonna use the <a href="https://www.kaggle.com/datasets/harlfoxem/housesalesprediction/data">House Sales in King County, USA dataset</a>. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.</p>
<p>Note: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.</p>
<p>Let’s begin by importing everything we will need, loading our data, and adding some columns.</p>
<div id="cell-7" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Callable, Hashable, Self</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> numpy.typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> NDArray</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scipy</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.base <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseEstimator</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KMeans</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.compose <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ColumnTransformer</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression, RidgeCV</span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> r2_score</span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-15"></span>
<span id="cb1-16">Array <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NDArray[np.float64]</span>
<span id="cb1-17"></span>
<span id="cb1-18">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data/kc_house_data.csv.zip"</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date"</span>])</span>
<span id="cb1-19">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"long_scaled"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"long"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(</span>
<span id="cb1-20">    np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(np.cos(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lat"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">180</span>))</span>
<span id="cb1-21">)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># earth-curvature correction for (approximate) distance calculations</span></span>
<span id="cb1-22">df.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">id</th>
<th data-quarto-table-cell-role="th">date</th>
<th data-quarto-table-cell-role="th">price</th>
<th data-quarto-table-cell-role="th">bedrooms</th>
<th data-quarto-table-cell-role="th">bathrooms</th>
<th data-quarto-table-cell-role="th">sqft_living</th>
<th data-quarto-table-cell-role="th">sqft_lot</th>
<th data-quarto-table-cell-role="th">floors</th>
<th data-quarto-table-cell-role="th">waterfront</th>
<th data-quarto-table-cell-role="th">view</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">sqft_above</th>
<th data-quarto-table-cell-role="th">sqft_basement</th>
<th data-quarto-table-cell-role="th">yr_built</th>
<th data-quarto-table-cell-role="th">yr_renovated</th>
<th data-quarto-table-cell-role="th">zipcode</th>
<th data-quarto-table-cell-role="th">lat</th>
<th data-quarto-table-cell-role="th">long</th>
<th data-quarto-table-cell-role="th">sqft_living15</th>
<th data-quarto-table-cell-role="th">sqft_lot15</th>
<th data-quarto-table-cell-role="th">long_scaled</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>2.161300e+04</td>
<td>21613</td>
<td>2.161300e+04</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>2.161300e+04</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>...</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
<td>21613.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>4.580302e+09</td>
<td>2014-10-29 04:38:01.959931648</td>
<td>5.400881e+05</td>
<td>3.370842</td>
<td>2.114757</td>
<td>2079.899736</td>
<td>1.510697e+04</td>
<td>1.494309</td>
<td>0.007542</td>
<td>0.234303</td>
<td>...</td>
<td>1788.390691</td>
<td>291.509045</td>
<td>1971.005136</td>
<td>84.402258</td>
<td>98077.939805</td>
<td>47.560053</td>
<td>-122.213896</td>
<td>1986.552492</td>
<td>12768.455652</td>
<td>-82.471784</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">min</td>
<td>1.000102e+06</td>
<td>2014-05-02 00:00:00</td>
<td>7.500000e+04</td>
<td>0.000000</td>
<td>0.000000</td>
<td>290.000000</td>
<td>5.200000e+02</td>
<td>1.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>290.000000</td>
<td>0.000000</td>
<td>1900.000000</td>
<td>0.000000</td>
<td>98001.000000</td>
<td>47.155900</td>
<td>-122.519000</td>
<td>399.000000</td>
<td>651.000000</td>
<td>-82.677673</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">25%</td>
<td>2.123049e+09</td>
<td>2014-07-22 00:00:00</td>
<td>3.219500e+05</td>
<td>3.000000</td>
<td>1.750000</td>
<td>1427.000000</td>
<td>5.040000e+03</td>
<td>1.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>1190.000000</td>
<td>0.000000</td>
<td>1951.000000</td>
<td>0.000000</td>
<td>98033.000000</td>
<td>47.471000</td>
<td>-122.328000</td>
<td>1490.000000</td>
<td>5100.000000</td>
<td>-82.548783</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">50%</td>
<td>3.904930e+09</td>
<td>2014-10-16 00:00:00</td>
<td>4.500000e+05</td>
<td>3.000000</td>
<td>2.250000</td>
<td>1910.000000</td>
<td>7.618000e+03</td>
<td>1.500000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>1560.000000</td>
<td>0.000000</td>
<td>1975.000000</td>
<td>0.000000</td>
<td>98065.000000</td>
<td>47.571800</td>
<td>-122.230000</td>
<td>1840.000000</td>
<td>7620.000000</td>
<td>-82.482651</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">75%</td>
<td>7.308900e+09</td>
<td>2015-02-17 00:00:00</td>
<td>6.450000e+05</td>
<td>4.000000</td>
<td>2.500000</td>
<td>2550.000000</td>
<td>1.068800e+04</td>
<td>2.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>2210.000000</td>
<td>560.000000</td>
<td>1997.000000</td>
<td>0.000000</td>
<td>98118.000000</td>
<td>47.678000</td>
<td>-122.125000</td>
<td>2360.000000</td>
<td>10083.000000</td>
<td>-82.411796</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">max</td>
<td>9.900000e+09</td>
<td>2015-05-27 00:00:00</td>
<td>7.700000e+06</td>
<td>33.000000</td>
<td>8.000000</td>
<td>13540.000000</td>
<td>1.651359e+06</td>
<td>3.500000</td>
<td>1.000000</td>
<td>4.000000</td>
<td>...</td>
<td>9410.000000</td>
<td>4820.000000</td>
<td>2015.000000</td>
<td>2015.000000</td>
<td>98199.000000</td>
<td>47.777600</td>
<td>-121.315000</td>
<td>6210.000000</td>
<td>871200.000000</td>
<td>-81.865195</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">std</td>
<td>2.876566e+09</td>
<td>NaN</td>
<td>3.671272e+05</td>
<td>0.930062</td>
<td>0.770163</td>
<td>918.440897</td>
<td>4.142051e+04</td>
<td>0.539989</td>
<td>0.086517</td>
<td>0.766318</td>
<td>...</td>
<td>828.090978</td>
<td>442.575043</td>
<td>29.373411</td>
<td>401.679240</td>
<td>53.505026</td>
<td>0.138564</td>
<td>0.140828</td>
<td>685.391304</td>
<td>27304.179631</td>
<td>0.095033</td>
</tr>
</tbody>
</table>

<p>8 rows × 22 columns</p>
</div>
</div>
</div>
<p>We will want a polynomial (rather than linear) dependency on the age of the house:</p>
<div id="cell-9" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date"</span>].dt.year <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"yr_built"</span>]</span>
<span id="cb2-2">age_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>]</span>
<span id="cb2-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> power <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb2-4">    col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"age ^ </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>power<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb2-5">    df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> power</span>
<span id="cb2-6">    age_cols.append(col)</span></code></pre></div>
</div>
<p>We do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.</p>
<div id="cell-11" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"price"</span>])</span>
<span id="cb3-2">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"price"</span>])</span>
<span id="cb3-3">x_train, x_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(</span>
<span id="cb3-4">    X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span></span>
<span id="cb3-5">)</span>
<span id="cb3-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x_train<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x_train.shape=(2161, 25), x_test.shape=(19452, 25)</code></pre>
</div>
</div>
<p>There is no reason to expect a linear relationship between the house geographical coordinates and it’s price.<br>
However, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.<br>
One way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:</p>
<div id="cell-13" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.</span></span>
<span id="cb5-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> OneHotEncodedKMeansTransformer:</span>
<span id="cb5-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, k: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, columns: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>], name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb5-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> columns</span>
<span id="cb5-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k</span>
<span id="cb5-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> name</span>
<span id="cb5-7"></span>
<span id="cb5-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb5-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k, n_init<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"auto"</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb5-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_.fit(X[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.columns])</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb5-12"></span>
<span id="cb5-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> column_names(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]:</span>
<span id="cb5-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> [<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k)]</span>
<span id="cb5-15"></span>
<span id="cb5-16">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> transform(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame):</span>
<span id="cb5-17">        cluster_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_.predict(X[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.columns])</span>
<span id="cb5-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> pd.concat(</span>
<span id="cb5-19">            [</span>
<span id="cb5-20">                X,</span>
<span id="cb5-21">                pd.DataFrame(</span>
<span id="cb5-22">                    np.eye(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k)[cluster_index],</span>
<span id="cb5-23">                    columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.column_names(),</span>
<span id="cb5-24">                    index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X.index,</span>
<span id="cb5-25">                ),</span>
<span id="cb5-26">            ],</span>
<span id="cb5-27">            axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb5-28">        )</span>
<span id="cb5-29"></span>
<span id="cb5-30">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> clusters_adjacency_matrix(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb5-31">        edges <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(</span>
<span id="cb5-32">            scipy.spatial.Voronoi(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.kmeans_.cluster_centers_).ridge_points</span>
<span id="cb5-33">        ).T</span>
<span id="cb5-34">        a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.coo_matrix(</span>
<span id="cb5-35">            (np.ones(edges.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), (edges[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], edges[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])),</span>
<span id="cb5-36">            shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.k),</span>
<span id="cb5-37">        )</span>
<span id="cb5-38">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> a.T</span>
<span id="cb5-39"></span>
<span id="cb5-40"></span>
<span id="cb5-41">kmeans_transformer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OneHotEncodedKMeansTransformer(</span>
<span id="cb5-42">    k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>,</span>
<span id="cb5-43">    columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lat"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"long_scaled"</span>],</span>
<span id="cb5-44">    name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"geo_cluster"</span>,</span>
<span id="cb5-45">)</span>
<span id="cb5-46">x_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans_transformer.fit(x_train).transform(x_train)</span>
<span id="cb5-47">x_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans_transformer.transform(x_test)</span></code></pre></div>
</div>
<p>We will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.</p>
<div id="cell-15" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> evaluate_model(model) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-2">    y_train_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(x_train, y_train).predict(x_train)</span>
<span id="cb6-3">    r2_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_train, y_train_pred)</span>
<span id="cb6-4">    y_test_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(x_test)</span>
<span id="cb6-5">    r2_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_test, y_test_pred)</span>
<span id="cb6-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<p>Let’s start with a vanilla linear model, without any regularization/augmentations.</p>
<div id="cell-17" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-2">    [</span>
<span id="cb7-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bedrooms"</span>,</span>
<span id="cb7-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bathrooms"</span>,</span>
<span id="cb7-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"floors"</span>,</span>
<span id="cb7-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"waterfront"</span>,</span>
<span id="cb7-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"view"</span>,</span>
<span id="cb7-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"condition"</span>,</span>
<span id="cb7-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grade"</span>,</span>
<span id="cb7-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living"</span>,</span>
<span id="cb7-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot"</span>,</span>
<span id="cb7-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_above"</span>,</span>
<span id="cb7-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_basement"</span>,</span>
<span id="cb7-14">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot15"</span>,</span>
<span id="cb7-15">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living15"</span>,</span>
<span id="cb7-16">    ]</span>
<span id="cb7-17">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> age_cols</span>
<span id="cb7-18">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> kmeans_transformer.column_names()</span>
<span id="cb7-19">)</span>
<span id="cb7-20">columns_selector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ColumnTransformer(</span>
<span id="cb7-21">    [(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selector"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"passthrough"</span>, columns)],</span>
<span id="cb7-22">    remainder<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"drop"</span>,</span>
<span id="cb7-23">    verbose_feature_names_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb7-24">).set_output(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pandas"</span>)</span>
<span id="cb7-25"></span>
<span id="cb7-26">simple_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline(</span>
<span id="cb7-27">    [</span>
<span id="cb7-28">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selector"</span>, columns_selector),</span>
<span id="cb7-29">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"linear"</span>, LinearRegression(fit_intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)),</span>
<span id="cb7-30">    ]</span>
<span id="cb7-31">)</span>
<span id="cb7-32">evaluate_model(simple_linear)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>r2_train=0.918, r2_test=0.862</code></pre>
</div>
</div>
<p>Not bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:</p>
<div id="cell-19" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline(</span>
<span id="cb9-2">    [</span>
<span id="cb9-3">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selector"</span>, columns_selector),</span>
<span id="cb9-4">        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scale"</span>, StandardScaler()),</span>
<span id="cb9-5">        (</span>
<span id="cb9-6">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"linear"</span>,</span>
<span id="cb9-7">            RidgeCV(</span>
<span id="cb9-8">                fit_intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb9-9">                alphas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.logspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>),</span>
<span id="cb9-10">            ),</span>
<span id="cb9-11">        ),</span>
<span id="cb9-12">    ]</span>
<span id="cb9-13">)</span>
<span id="cb9-14">evaluate_model(ridge)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>r2_train=0.918, r2_test=0.863</code></pre>
</div>
</div>
<p>That didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.<br>
Let’s see if we can do better by using augmentations that are more appropriate for our data.</p>
<p>First let’s build a class for linear models with augmentation via regularization.<br>
We will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.<br>
Since often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).</p>
<div id="cell-21" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> AugmentedLinearModel:</span>
<span id="cb11-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(</span>
<span id="cb11-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb11-4">        augmentation_moments: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># one item for each group of features</span></span>
<span id="cb11-5">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[</span>
<span id="cb11-6">                <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[Hashable],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># column names of features in the group</span></span>
<span id="cb11-7">                Callable[[pd.DataFrame], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># maps X to M and R</span></span>
<span id="cb11-8">            ]</span>
<span id="cb11-9">        ],</span>
<span id="cb11-10">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb11-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.augmentation_moments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> augmentation_moments</span>
<span id="cb11-12"></span>
<span id="cb11-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame, y: pd.Series) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Self:</span>
<span id="cb11-14">        means, covs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(</span>
<span id="cb11-15">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(</span>
<span id="cb11-16">                moments(X.loc[:, columns])</span>
<span id="cb11-17">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> columns, moments <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.augmentation_moments</span>
<span id="cb11-18">            )</span>
<span id="cb11-19">        )</span>
<span id="cb11-20">        M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.hstack(means)</span>
<span id="cb11-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://scikit-learn.org/stable/developers/develop.html#estimated-attributes</span></span>
<span id="cb11-22">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.linalg.block_diag(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>covs)</span>
<span id="cb11-23">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.solve(M.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.R_, M.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y)</span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb11-25"></span>
<span id="cb11-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> pd.Series:</span>
<span id="cb11-27">        cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [col <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> cols, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.augmentation_moments <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> cols]</span>
<span id="cb11-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X.loc[:, cols] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.theta_</span></code></pre></div>
</div>
<p>Here are the augmentations we are gonna use:<br>
With 10% probability, a bathroom is counted as half a bedroom.</p>
<div id="cell-23" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> bedrooms_bathrooms_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb12-2">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb12-3">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>])</span>
<span id="cb12-4">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bathrooms"</span>]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-5">    M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(mask, X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> v, X)</span>
<span id="cb12-6">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> p) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.outer(v, v) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask.values.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb12-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> M, R</span>
<span id="cb12-8"></span>
<span id="cb12-9"></span>
<span id="cb12-10">augmentation_moments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bedrooms"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bathrooms"</span>], bedrooms_bathrooms_moments)]</span></code></pre></div>
</div>
<p>A 5% perturbation for the features<br>
<code>sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15</code>, uncorrelated across the features.</p>
<div id="cell-25" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> relative_perturbation_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb13-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X.values, np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(X.values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb13-3"></span>
<span id="cb13-4"></span>
<span id="cb13-5">augmentation_moments.extend(</span>
<span id="cb13-6">    ([column], relative_perturbation_moments)</span>
<span id="cb13-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> [</span>
<span id="cb13-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living"</span>,</span>
<span id="cb13-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot"</span>,</span>
<span id="cb13-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_above"</span>,</span>
<span id="cb13-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_basement"</span>,</span>
<span id="cb13-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_lot15"</span>,</span>
<span id="cb13-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sqft_living15"</span>,</span>
<span id="cb13-14">    ]</span>
<span id="cb13-15">)</span></code></pre></div>
</div>
<p>A perturbation of 0.01 for the features <code>floors, waterfront, view, condition, grade</code>, uncorrelated across the features</p>
<div id="cell-27" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> absolute_perturbation_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb14-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> X.values, X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb14-3"></span>
<span id="cb14-4"></span>
<span id="cb14-5">augmentation_moments.extend(</span>
<span id="cb14-6">    ([column], absolute_perturbation_moments)</span>
<span id="cb14-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"floors"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"waterfront"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"view"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"condition"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grade"</span>]</span>
<span id="cb14-8">)</span></code></pre></div>
</div>
<p>perturbing <code>age</code> with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.</p>
<div id="cell-29" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> age_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb15-2">    a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>]].values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb15-3">    b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>]].values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb15-4">    max_power <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb15-5">    np1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> max_power <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb15-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments</span></span>
<span id="cb15-7">    mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>np1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>np1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (np1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> a))</span>
<span id="cb15-8">    mu_sum <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb15-9">    mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu[:, :max_power]</span>
<span id="cb15-10">    idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.add.outer(np.arange(max_power), np.arange(max_power))</span>
<span id="cb15-11">    c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu_sum[idx] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mu.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> mu</span>
<span id="cb15-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> mu, c</span>
<span id="cb15-13"></span>
<span id="cb15-14"></span>
<span id="cb15-15">augmentation_moments.append((age_cols, age_moments))</span></code></pre></div>
</div>
<p>And finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.</p>
<div id="cell-31" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> geo_cluster_moments(X: pd.DataFrame) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[Array, Array]:</span>
<span id="cb16-2">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb16-3">    adj_mat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans_transformer.clusters_adjacency_matrix()</span>
<span id="cb16-4">    P <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.eye(adj_mat.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (adj_mat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> adj_mat.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (</span>
<span id="cb16-5">        <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> p</span>
<span id="cb16-6">    )  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># transition probabilities matrix</span></span>
<span id="cb16-7">    M <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.csr_array(X.values) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> P</span>
<span id="cb16-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation</span></span>
<span id="cb16-9">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scipy.sparse.diags(M.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> M.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> M</span>
<span id="cb16-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> M.toarray(), R.toarray()</span>
<span id="cb16-11"></span>
<span id="cb16-12"></span>
<span id="cb16-13">augmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))</span></code></pre></div>
</div>
<p>Le’t fit the augmented model and see how we did:</p>
<div id="cell-33" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">augmented_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AugmentedLinearModel(augmentation_moments)</span>
<span id="cb17-2">evaluate_model(augmented_linear)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>r2_train=0.903, r2_test=0.882</code></pre>
</div>
</div>
<p>We managed to improve the test accuracy, and reduce overfit.</p>
</section>
<section id="beyond-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="beyond-least-squares">Beyond least squares</h2>
<p>Is it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.</p>
<p>The goal is to (approximately) express <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cmathrm%7BE%7D%20%20%0A%20%20%20%20%5Cleft%5B%0A%20%20%20%20%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20l%20%5Cleft(%0A%20%20%20%20a%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20%5C,;%5C,%20y_i%0A%20%20%20%20%5Cright)%0A%20%20%20%20%5Cright%5D,%0A%5Cend%7Balign*%7D"> as a sum of a non-augmented loss term, and a regularization term. Here, <img src="https://latex.codecogs.com/png.latex?l(%5Chat%7By%7D%5C,;%5C,y)"> measures how bad is the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D">, given the true value <img src="https://latex.codecogs.com/png.latex?y"> (the loss).<br>
For example, for logistic regression we use the logistic loss <img src="https://latex.codecogs.com/png.latex?%0Al(%5Chat%7By%7D;%20y)%20=%20%5Clog%20%5Cleft(%201%20+%20%5Cexp%20%5Cleft(-y%20%5C,%20%5Chat%7By%7D%20%5Cright)%20%5Cright)%0A"> (with <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5C%7B%20-1,%201%20%5C%7D">).<br>
Let’s expand <img src="https://latex.codecogs.com/png.latex?l%20%5Cleft(%0Aa%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%5C,;%5C,y_i%0A%5Cright)"> around <img src="https://latex.codecogs.com/png.latex?%5Cmu_i%20%5ET%20%5Ctheta"> and simplify: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathrm%7BE%7D%20%20%0A%5Cleft%5B%0A%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20l%20%5Cleft(%0Aa%5Cleft(x_i,%20p_i%5Cright)%20%5ET%20%5Ctheta%20%5C,;%5C,%20y_i%0A%5Cright)%0A%5Cright%5D%0A%5Capprox%0A%5Csum_%7Bi=0%7D%20%5E%7Bn-1%7D%20l(%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20l''%20%5Cleft(%20%5Cmu_i%20%5ET%20%5Ctheta%5C,;%5C,y_i%20%5Cright)%20%5Ctheta%5ET%20R%20%5Ctheta%0A%5Cend%7Balign*%7D"> (the order-1 term vanishes as it has zero mean, similar to <a href="https://en.wikipedia.org/wiki/Delta_method#">the delta method</a>).<br>
So like in the least squares case, in the loss term we just replace each <img src="https://latex.codecogs.com/png.latex?x"> with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).</p>
<p>I use this result to tell myself that it is ok to select an <img src="https://latex.codecogs.com/png.latex?R"> for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and <img src="https://latex.codecogs.com/png.latex?l''"> is bounded (correct for logistic regression).</p>


</section>

 ]]></description>
  <guid>https://tomshlomo.github.io/blog/posts/augmentation_is_regularization/augmentation_is_regularization.html</guid>
  <pubDate>Sun, 14 Jan 2024 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
