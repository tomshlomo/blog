[
  {
    "objectID": "posts/jupyter-post/augmentation_example.html",
    "href": "posts/jupyter-post/augmentation_example.html",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Training data augmentation enhances the training dataset by applying transformations to existing training data instances. The specific transformations vary depending on the type of data involved, and this flexibility allows to leverage domain knowledge, such as known invariants, effectively. The goal is to introduce variability and increase the diversity of the training set, allowing the model to better generalize to unseen data and exhibit improved robustness. Despite the advantages, training data augmentation introduces an inherent computational cost: the increased volume of data requires additional computational resources, impacting both training time and memory requirements.\nAs we will show below, for linear models with the sum of squares loss, training data augmentation is equivalent to adding quadratic regularization term, which implies that the computational cost of fitting a model to an augmented dataset is the same as using no augmentation at all!\nThis link between augmentation and regularization is useful in the other direction as well: it gives a concrete interpretation to the value of regularization hyperparameters, and can be used to avoid costly hyperparameters tuning (np.logspace(-6, 6, 100) much?), and to design regularizers that are more appropriate to the data than the simple ones (i.e sum of squares regularization used in ridge regression).\n\n\nSuppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change.\n\n\n\nLet’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\).\n\n\n\nWe will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS.\n\n\n\nRide regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros.\n\n\n\nFor the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit.\n\n\n\nIs it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  },
  {
    "objectID": "posts/jupyter-post/augmentation_example.html#notation",
    "href": "posts/jupyter-post/augmentation_example.html#notation",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Suppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change."
  },
  {
    "objectID": "posts/jupyter-post/augmentation_example.html#ordinary-least-squares-ols",
    "href": "posts/jupyter-post/augmentation_example.html#ordinary-least-squares-ols",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Let’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\)."
  },
  {
    "objectID": "posts/jupyter-post/augmentation_example.html#augmented-least-squares",
    "href": "posts/jupyter-post/augmentation_example.html#augmented-least-squares",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "We will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS."
  },
  {
    "objectID": "posts/jupyter-post/augmentation_example.html#ridge-regression",
    "href": "posts/jupyter-post/augmentation_example.html#ridge-regression",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Ride regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros."
  },
  {
    "objectID": "posts/jupyter-post/augmentation_example.html#example",
    "href": "posts/jupyter-post/augmentation_example.html#example",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "For the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit."
  },
  {
    "objectID": "posts/jupyter-post/augmentation_example.html#beyond-least-squares",
    "href": "posts/jupyter-post/augmentation_example.html#beyond-least-squares",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Is it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "A practical interpertation of the Pearson correlation coefficient\n\n\n\n\n\n\nmachine learning\n\n\n\n\\(\\rho=1\\) means perfect positive correlation, \\(\\rho=-1\\) means perfect negative correlation, \\(\\rho=0\\) means no correlation. But what does \\(\\rho=0.72\\) mean?\n\n\n\n\n\nJan 20, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\n\n\n\n\n\n\nAugmentation is Regularization\n\n\n\n\n\n\nmachine learning\n\n\nregularization\n\n\n\nOn the equivalence of training data augmentation and quadratic regularization for linear models - a very useful (but not well known) result.\n\n\n\n\n\nJan 15, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pearson_correlation/pearson_correlation.html",
    "href": "posts/pearson_correlation/pearson_correlation.html",
    "title": "A practical interpertation of the Pearson correlation coefficient",
    "section": "",
    "text": "My goal is to explain the Pearson correlation coefficient without using the word correlation, which is often used to describe it. \\ One way is to just give the definition: the Pearson correlation coefficient of two random variables \\(X\\) and \\(Y\\) is \\[\n\\rho = \\frac{\\sigma_{XY}}{\\sigma_X \\sigma_Y},\n\\] where \\(\\sigma_X^2 =\\mathrm{E} (X - \\mu_X)^2\\) is the variance of \\(X\\), \\(\\sigma_Y^2 =\\mathrm{E} (Y - \\mu_Y)^2\\) is the variance of \\(Y\\), \\(\\sigma_{XY}=\\mathrm{E} (X - \\mu_X)(Y - \\mu_Y)\\) is the covariance of \\(X\\) and \\(Y\\), \\(\\mu_x = \\mathrm{E} X\\) is the mean of \\(X\\), and \\(\\mu_Y = \\mathrm{E} Y\\) is the mean of \\(Y\\). \\ But this is unsatisfying: why is this definition useful?\nConsider the problem of estimating \\(Y\\) from an observation of \\(X\\). It turns out that in the optimal linear estimator, the number of standard deviations \\(Y\\) is above it’s mean is \\(\\rho\\) times the number of standard deviations \\(X\\) is above it’s mean.  \\ In other words, \\(\\rho\\) is the factor by which we shrink (and possibly flip) the deviation from the mean in one variable when we estimate the other. \\ So for example, if \\(\\rho=0.72, \\mu_x = 4, \\sigma_x = 0.5, \\mu_y = 100, \\sigma_y = 10\\) and we observe \\(X=5\\), the estimate of \\(Y\\) is \\(100 + 0.72 \\cdot 2 \\cdot 10 =114.4\\)\nThe proof is very simple. Since we are dealing with linear (actually, affine) estimators, we need to show that the \\(a\\) and \\(b\\) that would minimize \\[\n\\text{MSE} := \\mathrm{E} \\left( \\hat{Y} - Y )^2 \\right) ^2,\n\\] where \\(\\hat{Y} := a (X - \\mu_x) + b\\),  are \\(\\rho \\sigma_Y / \\sigma_X\\) and \\(\\mu_Y\\).\nThe MSE is the sum of bias squared and variance. The variance doesn’t depend on \\(b\\), and the bias is \\(\\mathrm{E} \\left[ \\hat{Y} - Y \\right] = b - \\mu_Y\\) which doesn’t depend on \\(a\\). So we already know that \\(b=\\mu_Y\\). To minimize the variance, we simplify \\[\n\\begin{align*}\n\\mathrm{Var}\\left[\\hat{Y} - Y\\right]\n&= \\mathrm{Var}\\left[a (X - \\mu_X) - Y\\right] \\\\\n&= \\mathrm{Var}\\left[a \\left(X - \\mu_X\\right) \\right]\n    + \\mathrm{Var}\\left[ Y\\right]\n    -2 \\mathrm{Cov}\\left[a \\left(X - \\mu_X\\right), Y\\right] \\\\\n&= \\sigma_x ^ 2 a^2\n   + \\sigma_Y ^2\n   -2  \\sigma_{XY} a\n\\end{align*}\n\\] This is just a parabola in \\(a\\), so the optimal \\(a\\) is \\[\n\\frac{2 \\sigma_{XY}} {2 \\sigma_X ^2}\n=\n\\rho \\frac{\\sigma_Y } {\\sigma_X }\n\\] (which is what we wanted to show). The optima The optimal \\(a=\\frac{2 \\sigma_{XY}} {2 \\sigma_X ^ 2}\\)\n\\[\n\\begin{align*}\na\n&= \\text{argmin}_{a'} \\mathrm{Var}\\left[\\hat{Y} - Y\\right] \\\\\n&= \\text{argmin}_{a'} \\mathrm{Var}\\left[a (X - \\mu_X) - Y\\right] \\\\\n&= \\text{argmin}_{a'}\n    \\mathrm{Var}\\left[a \\left(X - \\mu_X\\right) \\right]\n    + \\mathrm{Var}\\left[ Y\\right]\n    -2 \\mathrm{Cov}\\left[a \\left(X - \\mu_X\\right), Y\\right] \\\\\n&= \\text{argmin}_{a'}\n    a^2 \\sigma_x ^ 2\n    + \\sigma_Y ^2\n    -2 a \\sigma_{XY}\n\\end{align*}\n\\] and the variance do We with expanding the MSE as the sum of the squared bias and variance \\[\n\\begin{align*}\n\\text{MSE} &=\n\\left(\\mathrm{E} \\left[\\hat{Y} - Y \\right] \\right)^2\n+ \\mathrm{Var} \\left[\\hat{Y} - Y \\right]\n\\\\&=\nb ^ 2\n+ \\mathrm{Var} \\left[a (x - \\mu_X) - Y \\right]\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\mathrm{E} \\left[ \\left(a (x - \\mu_x) + b - y \\right) ^ 2 \\right]\n&=\na ^ 2 \\mathrm{E} \\left[ \\left( x - \\mu_x \\right) ^ 2 \\right]\n+\n\\mathrm{E} \\left[ \\left(b - y \\right) ^ 2 \\right]\n+\na \\mathrm{E} \\left[ \\left(x - \\mu_x\\right) \\left(b - y \\right) \\right]\n\\\\&=\na ^ 2 \\sigma_x ^2\n+\n\\sigma_y ^ 2\n+\na \\, \\sigma_{xy}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Training data augmentation enhances the training dataset by applying transformations to existing training data instances. The specific transformations vary depending on the type of data involved, and this flexibility allows to leverage domain knowledge, such as known invariants, effectively. The goal is to introduce variability and increase the diversity of the training set, allowing the model to better generalize to unseen data and exhibit improved robustness. Despite the advantages, training data augmentation introduces an inherent computational cost: the increased volume of data requires additional computational resources, impacting both training time and memory requirements.\nAs we will show below, for linear models with the sum of squares loss, training data augmentation is equivalent to adding quadratic regularization term, which implies that the computational cost of fitting a model to an augmented dataset is the same as using no augmentation at all!\nThis link between augmentation and regularization is useful in the other direction as well: it gives a concrete interpretation to the value of regularization hyperparameters, and can be used to avoid costly hyperparameters tuning (np.logspace(-6, 6, 100) much?), and to design regularizers that are more appropriate to the data than the simple ones (i.e sum of squares regularization used in ridge regression).\n\n\nSuppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change.\n\n\n\nLet’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\).\n\n\n\nWe will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS.\n\n\n\nRide regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros.\n\n\n\nFor the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit.\n\n\n\nIs it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#notation",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#notation",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Suppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ordinary-least-squares-ols",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ordinary-least-squares-ols",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Let’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\)."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#augmented-least-squares",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#augmented-least-squares",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "We will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ridge-regression",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ridge-regression",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Ride regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#example",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#example",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "For the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#beyond-least-squares",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#beyond-least-squares",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Is it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  }
]