[
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html",
    "title": "Augmentation is Regularization",
    "section": "",
    "text": "Training data augmentation enhances the training dataset by applying transformations to existing training data instances. The specific transformations vary depending on the type of data involved, and this flexibility allows to leverage domain knowledge, such as known invariants, effectively. The goal is to introduce variability and increase the diversity of the training set, allowing the model to better generalize to unseen data and exhibit improved robustness. Despite the advantages, training data augmentation introduces an inherent computational cost: the increased volume of data requires additional computational resources, impacting both training time and memory requirements.\nAs we will show below, for linear models with the sum of squares loss, training data augmentation is equivalent to adding quadratic regularization term, which implies that the computational cost of fitting a model to an augmented dataset is the same as using no augmentation at all!\nThis link between augmentation and regularization is useful in the other direction as well: it gives a concrete interpretation to the value of regularization hyperparameters, and can be used to avoid costly hyperparameters tuning (np.logspace(-6, 6, 100) much?), and to design regularizers that are more appropriate to the data than the simple ones (i.e sum of squares regularization used in ridge regression)."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#notation",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#notation",
    "title": "Augmentation is Regularization",
    "section": "Notation",
    "text": "Notation\nSuppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ordinary-least-squares-ols",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ordinary-least-squares-ols",
    "title": "Augmentation is Regularization",
    "section": "Ordinary least squares (OLS)",
    "text": "Ordinary least squares (OLS)\nLet’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\)."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#augmented-least-squares",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#augmented-least-squares",
    "title": "Augmentation is Regularization",
    "section": "Augmented least squares",
    "text": "Augmented least squares\nWe will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ridge-regression",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#ridge-regression",
    "title": "Augmentation is Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\nRide regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#example",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#example",
    "title": "Augmentation is Regularization",
    "section": "Example",
    "text": "Example\nFor the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit."
  },
  {
    "objectID": "posts/augmentation_is_regularization/augmentation_is_regularization.html#beyond-least-squares",
    "href": "posts/augmentation_is_regularization/augmentation_is_regularization.html#beyond-least-squares",
    "title": "Augmentation is Regularization",
    "section": "Beyond least squares",
    "text": "Beyond least squares\nIs it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  },
  {
    "objectID": "posts/pearson_correlation/pearson_correlation.html",
    "href": "posts/pearson_correlation/pearson_correlation.html",
    "title": "A practical interpertation of the Pearson correlation coefficient",
    "section": "",
    "text": "\\[\n\\renewcommand{\\E}[1]{\\operatorname{E}\\left[#1\\right]}\n\\renewcommand{\\var}[1]{\\operatorname{Var} \\left[#1 \\right]}\n\\renewcommand{\\cov}[1]{\\operatorname{Cov} \\left[#1 \\right] }\n\\]\nMy goal is to explain the Pearson correlation coefficient without using the word “correlation,” which is often used to describe it.\nThe Pearson correlation coefficient of two random variables \\(X\\) and \\(Y\\) is \\[\n\\rho := \\frac{\\sigma_{XY}}{\\sigma_X \\sigma_Y},\n\\] where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\) respectively, and \\(\\sigma_{XY}\\) is their covariance.\nA motivation for this definition stems from the problem of estimating \\(Y\\) from an observation of \\(X\\). It turns out that in the optimal (lowest MSE) linear estimation, the number of standard deviations \\(Y\\) is above its mean is \\(\\rho\\) times the number of standard deviations \\(X\\) is above its mean.\nFor example, consider a population where height and weight are correlated with \\(\\rho=0.72\\), heights are distributed with a mean of \\(170\\)cm and a standard deviation of \\(10\\)cm, weights are distributed with a mean of \\(70\\)Kg and a standard deviation of \\(20\\)Kg. If we know that a certain person’s height is \\(190\\)cm, a good estimate for their weight would be \\(70 + 2 \\cdot 0.72 \\cdot 20 = 98.8\\)Kg.\nThe proof is straightforward. Since we are dealing with linear (actually, affine) estimators, we need to show that the \\(a\\) and \\(b\\) that would minimize \\[\n\\text{MSE} := \\E{ \\left( \\hat{Y} - Y \\right) ^2},\n\\] where \\(\\hat{Y} := a (X - \\mu_x) + b\\), are \\(\\rho \\sigma_Y / \\sigma_X\\) and \\(\\mu_Y\\).\nThe MSE is the sum of the square of the bias and the variance. The variance doesn’t depend on \\(b\\), and the bias is \\(\\E{  \\hat{Y} - Y } = b - \\mu_Y\\) which doesn’t depend on \\(a\\), so \\(b=\\mu_Y\\). To minimize the variance, we simplify: \\[\n\\begin{align*}\n\\var{\\hat{Y} - Y}\n&= \\var{\\hat{Y}} + \\var{Y} - 2 \\cov{\\hat{Y}, Y}\n\\\\&= \\sigma_x ^ 2 a^2\n   + \\sigma_Y ^2\n   -2  \\sigma_{XY} a.\n\\end{align*}\n\\] This is simply a parabola in \\(a\\), so the optimal \\(a\\) is \\[\na=\\frac{2 \\sigma_{XY}} {2 \\sigma_X ^2}\n=\n\\rho \\frac{\\sigma_Y } {\\sigma_X }\n\\] (which is what we wanted to show).\nThe estimator is unbiased, so its MSE is equal to its variance: \\[\n\\text{MSE} = \\sigma_Y ^2 (1 - \\rho ^ 2).\n\\] This equation provides another concrete interpretation of \\(\\rho\\): *If \\(X\\) and \\(Y\\) are correlated with coefficient \\(\\rho\\), observing \\(X\\) will decrease the standard deviation of a \\(Y\\) estimate by a factor of at least \\(\\sqrt{1 - \\rho^2}\\) (“At least” since the optimal linear estimator is equal to or worse than the optimal estimator). In the example above, knowing the height decreases weight estimation standard deviation from 20Kg to \\(20  \\sqrt{1 - 0.72^2} = 13.9\\)Kg.\nRandomly ordered notes:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "All posts",
    "section": "",
    "text": "Efficient leave one out cross validation - part 2\n\n\n\n\n\nThe non quadratic case\n\n\n\n\n\nMar 30, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient leave one out cross validation - part 1\n\n\n\n\n\nThe derivation and implementation of a method for leave one out cross validation with neglible extra runtime compared to fitting alone.\n\n\n\n\n\nFeb 27, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\n\n\n\n\n\n\nMUSIC as a sparse decomposition method\n\n\n\n\n\nA unique introduction to the MUSIC algorithm, as a general method to solve the multisnapshot sparse decomposition problem.\n\n\n\n\n\nJan 30, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\n\n\n\n\n\n\nA practical interpertation of the Pearson correlation coefficient\n\n\n\n\n\n\\(\\rho=1\\) means perfect positive correlation, \\(\\rho=-1\\) means perfect negative correlation, \\(\\rho=0\\) means no correlation. But what does \\(\\rho=0.72\\) mean?\n\n\n\n\n\nJan 20, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\n\n\n\n\n\n\nAugmentation is Regularization\n\n\n\n\n\nOn the equivalence of training data augmentation and quadratic regularization for linear models - a very useful (but not well known) result.\n\n\n\n\n\nJan 15, 2024\n\n\nTom Shlomo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/music/music.html",
    "href": "posts/music/music.html",
    "title": "MUSIC as a sparse decomposition method",
    "section": "",
    "text": "MUSIC (MUltiple SIgnal Classification) is a popular algorithm used to estimating the directions of arrival (DOA) of waves recorded by an array of sensors.\nWhile very useful for this task, MUSIC is actually a more general parameters estimation method. However, conventional introductions to MUSIC often delve into the intricacies of equations tailored specifically for DOA estimation. These equations, laden with complex exponents or trigonometric identities, not only risk overwhelming readers but also obscure the fundamental insights that form the backbone of the method.\nAn assumption most derivations of MUSIC rely on is access to the signals autocorrelation matrix. In practice, only it’s estimate is available (usually from very few samples), and in many cases the signals are not stationary (e.g. speech) so it is not even well defined. Furthermore, most derivations of the algorithm rely on the noise being white, which is often not realistic.\nNevertheless, MUSIC can perform extremely well even when all these assumptions do not hold, which implies the existence of an alternative derivation. In this post I want to address the issues above by introducing MUSIC as a general method to (approximately) solve the multi-snapshot sparse decomposition problem."
  },
  {
    "objectID": "posts/music/music.html#a-quick-introduction-to-sparse-decompositions",
    "href": "posts/music/music.html#a-quick-introduction-to-sparse-decompositions",
    "title": "MUSIC as a sparse decomposition method",
    "section": "A quick introduction to sparse decompositions",
    "text": "A quick introduction to sparse decompositions\nYou obtained an \\(n\\)-dimensional vector \\(y\\), and you know that it is a linear combination of several “atoms”. You don’t know which atoms, but you do know that they come from a given set of atoms \\(a_1, \\dots, a_m\\) known as the dictionary. The goal is to decompose \\(y\\) to it’s atoms, that is, find the atoms that participate in the linear combination. In matrix notation: \\[\ny = Ax\n\\] where \\(A\\) is the (known) dictionary matrix, with columns \\(a_1, \\dots, a_m\\), and \\(x\\) contains the (unknown) coefficient for each atom. The non-zero indices of \\(x\\) correspond to the atoms that participate in the linear combination.\nIt might be tempting to simply solve for \\(x\\) as both \\(A\\) and \\(y\\) are known, but (at least for the interesting cases) \\(m &gt; n\\) and the system is under determined, that is, there are infinite ways to decompose \\(y\\) to a linear combination of atoms.\nIn the setting of sparse decompositions, we add an additional prior to the problem: \\(y\\) is composed of at most \\(k &lt; m\\) atoms, which means \\(x\\) is \\(k\\)-sparse (has at most \\(k\\) non zeros). The indices of the non-zeros entries of \\(x\\) are called the support of \\(x\\).\nFor example, in DOA estimation problems, we can use \\(y\\) to represent a signal recorded by an array of sensors at different locations, \\(a_i\\) is the response of the array to a unit amplitude signal coming from the \\(i\\)’th direction, and \\(x_i\\) the amplitude of the signal at the \\(i\\)’th direction. Assuming \\(x\\) is \\(k\\)-sparse is the same as assuming there are at most \\(k\\) signals active simultaneously. Decomposing \\(y\\) into it’s atoms reveals the directions of the recorded signals.\nThere are 2 important extensions to the basic sparse decomposition problem. The first is increasing robustness to noise or modeling errors, by looking for an approximate sparse decomposition instead of an exact one.\nFor example, in machine learning, sparse decomposition can be used for automatic feature selection in linear regression problems. Here \\(y\\) contains the training data labels, \\(A\\) contains the training data features, \\(x\\) is the coefficient of each feature, and \\(k\\) is the number of features to select.\nThe second extension is the joint sparsity problem (aka multi-snapshot), where instead of having a single vector \\(y\\), we get \\(p\\) vectors \\(y_1, \\dots, y_p\\). In matrix notation: \\[\nY = AX\n\\] where \\[\\begin{align*}\n    Y &:= \\begin{bmatrix} y_1 && \\cdots && y_p \\end{bmatrix}\n\\end{align*}\\] is the data matrix, and \\(X_{ij}\\) is the (unknown) coefficient of atom \\(a_i\\) in \\(y_j\\). Here, not only the columns of \\(X\\) are \\(k\\)-sparse, they also share the same support. This means that the matrix \\(X\\) is \\(k\\)-row-sparse, that is, has up to \\(k\\) non-zero rows.\nIn the direction of arrival estimation example, the joint sparsity problem can be obtained by observing the signals at \\(p\\) different (usually consecutive) times.\nIn the feature selection for linear regression example, the joint sparsity problem is obtained when we have multiple labels to predict, and we want to select the same \\(k\\) feature for each.\nSolving sparse decomposition problems is in general a hard problem. It turns out that you can’t do much better than enumerating over all \\(m \\choose k\\) possibilities for the support. Popular methods that yield approximate solutions are Matching Pursuit, Orthogonal Matching Pursuit, Basis Pursuit, and LASSO. In some cases, under additional assumptions, we can get some guarantees that the solution is exact, or close to exact. Although usually not presented as such, MUSIC is also a sparse decomposition method for the joint sparsity case, that under additional assumptions can provide exactness guarantees.\nWe will start by describing a method that can, under several assumptions, efficiently solve the noiseless joint sparsity problem. As we will see below, MUSIC can be viewed as an extension of this method for the noisy case.\nLet \\(S\\) denote the (unknown) support of \\(X\\). We will denote by \\(X_S\\) the matrix obtained by keeping only the rows in \\(S\\), and by \\(A_S\\) the matrix obtained by keeping only the columns in \\(S\\). Note that with this notation, we have \\[\nY = AX = A_S X_S.\n\\]\nMUSIC is based 2 assumptions:\nOur goal is to find \\(S\\) from \\(Y\\). Assumption 2 implies that \\[\n\\text{Range}(Y)\n=\n\\text{Range}(A_S X_S)\n=\n\\text{Range}(A_S),\n\\] so we can get \\(\\text{Range}(A_S)\\) from \\(Y\\). Assumption 2 means that once we have \\(\\text{Range}(A_s)\\), we can reconstruct \\(S\\) simply by checking which atoms are in it. The implied algorithm is simple:\nAlthough correct and efficient, this is a terrible algorithm. Calculating the range of a matrix is numerically unstable, and even the slightest perturbation (e.g. a roundoff error) can change it drastically. But before we continue to the more noise-robust MUSIC, let’s discuss the implications of the 2 assumptions.\nAssumption 1 means that to build an atom from a linear combination of other atoms, you need more than \\(\\left| S \\right|\\) atoms. This is related to something called the spark of \\(A\\). We won’t get into it here, but condition on the dictionary spark are elementary in basically every sparse decomposition method. For certain dictionaries, it can be shown that assumption 1 holds for any \\(S\\) of size less than \\(n\\). Specifically, this holds for the dictionary in DOA estimation problems with linear, equally spaced array of sensors, if the usual anti-aliasing conditions hold: the spacing between the sensors is smaller than half the wavelength, and no 2 directions lie on the same cone who’s axis contains the array.\nAssumption 2 is more restrictive. It means that no row of \\(X_S\\) is a linear combination of the other rows. A necessary (but not sufficient) condition is \\(\\left| S \\right| \\leq p\\). In the DOA estimation problem, each rows of \\(X_S\\) contains the samples of a different source. If the sources are uncorrelated (e.g. different speakers) and \\(\\left| S \\right| \\leq p\\), it is very unlikely that one is a linear combination of the others. If the sources are correlated, this doesn’t hold, and MUSIC can not be applied. This happens, for example, when one source is an echo of another, due to multi-path propagation.\nThe method above relies on the equation \\(\\text{Range}(Y) = \\text{Range}(A_S)\\) which is true if \\(Y=AX\\). However, in real life, the best we can hope for is \\(Y=AX+W\\), where \\(W\\), the noise term, is very small compared to \\(AX\\). Unfortunately, no matter how small \\(W\\) is, due to the discontinuity of \\(\\text{Range}\\), it won’t even hold approximately. In fact, if \\(p \\geq n\\), we will almost surely have \\(\\text{Range}(Y) = \\reals ^n\\), and the algorithm above would just yield \\(S=\\left\\{1, \\dots,  m \\right\\}\\).\nMUSIC makes 2 modifications the the algorithm above.\nFirst, we replace \\(Y\\) with \\(\\tilde{Y}\\), a rank-\\(\\left| S \\right|\\) approximation of \\(Y\\).\nNote that this requires us to know \\(\\left| S \\right|\\). This is another assumption MUSIC makes (although it can be avoided, sometimes, using model selection methods).\nSince \\(AX\\) has rank \\(\\left| S \\right|\\), taking a rank \\(\\left| S \\right|\\) approximation has a denoising effect. Indeed, unlike \\(\\text{Range} \\left( Y \\right)\\), \\(\\text{Range} \\left( \\tilde{Y} \\right)\\) is a good estimate for \\(\\text{Range} \\left( A_S \\right)\\) when \\(W\\) is small, but it is not exact: almost surely, none of the atoms would lie exactly in it. So the second modification is to soften our requirement that \\(a_i \\in \\text{Range} \\left( \\tilde{Y} \\right)\\) to add \\(i\\) to \\(S\\). Instead, we will require that \\(a_i\\) is almost in \\(\\text{Range} \\left( \\tilde{Y} \\right)\\), by checking if it looses little magnitude when projected onto it: \\[\nc_i := \\frac{\\| \\text{Proj}_{\\text{Range} \\left( \\tilde{Y} \\right)}(a_i) \\|^2}\n{\\| a_i \\|^2 }\n\\text{ is close to 1}\n\\implies\n\\text{ add $i$ to $S$}\n\\] (what “is close” means exactly differs between implementations. When the atoms can be ordered, like in DOA estimation, it is common to use a peak selection algorithm).\nAs we said above, \\(\\tilde{Y}\\) is a rank-\\(\\left| S \\right|\\) approximation to \\(Y\\). In MUSIC, we use the best rank-\\(\\left| S \\right|\\) approximation in the least squares sense, which is given by the truncated singular value decomposition (SVD) of \\(Y\\). Note that we don’t really need to calculate \\(\\tilde{Y}\\) itself, all we really need is the projection to it’s range. Well, a nice about the SVD is that we can get it directly: \\[\n\\label{music_final}\nc_i = \\frac{\\| U^T a_i\\|^2}{\\| a_i\\|^2}.\n\\] where the columns of \\(U\\) are the first \\(\\left| S \\right|\\) left singular vectors (which form an orthonormal basis for \\(\\text{Range} \\left( \\tilde{Y} \\right)\\)).\nTo wrap things up, a few notes to connect the above to the “usual” MUSIC derivation:"
  },
  {
    "objectID": "posts/music/music.html#footnotes",
    "href": "posts/music/music.html#footnotes",
    "title": "MUSIC as a sparse decomposition method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith linear, equally spaced array of sensors, if the usual anti-aliasing conditions hold: the spacing between the sensors is smaller than half the wavelength, and no 2 directions lie on the same cone who’s axis contains the array.↩︎\n\\(\\text{Range} \\left( A_S \\right)\\) is sometimes called the signal subspace, and the subspace orthogonal to it the noise subspace.↩︎"
  },
  {
    "objectID": "posts/music/music.html#music",
    "href": "posts/music/music.html#music",
    "title": "MUSIC as a sparse decomposition method",
    "section": "MUSIC",
    "text": "MUSIC\nThe method above relies on the equation \\(\\text{Range}(Y) = \\text{Range}(A_S)\\) which is true if \\(Y=AX\\). However, in real life, the best we can hope for is \\(Y=AX+W\\), where \\(W\\), the noise term, is very small compared to \\(AX\\). Unfortunately, no matter how small \\(W\\) is, due to the discontinuity of \\(\\text{Range}\\), it won’t even hold approximately. In fact, if \\(p \\geq n\\), we will almost surely have \\(\\text{Range}(Y) = \\reals ^n\\), and the algorithm above would just yield \\(S=\\left\\{1, \\dots,  m \\right\\}\\).\nMUSIC makes 2 modifications the the algorithm above.\nFirst, we replace \\(Y\\) with \\(\\tilde{Y}\\), a rank-\\(\\left| S \\right|\\) approximation of \\(Y\\).\nNote that this requires us to know \\(\\left| S \\right|\\). This is another assumption MUSIC makes (although it can be avoided, sometimes, using model selection methods).\nSince \\(AX\\) has rank \\(\\left| S \\right|\\), taking a rank \\(\\left| S \\right|\\) approximation has a denoising effect. Indeed, unlike \\(\\text{Range} \\left( Y \\right)\\), \\(\\text{Range} \\left( \\tilde{Y} \\right)\\) is a good estimate for \\(\\text{Range} \\left( A_S \\right)\\) when \\(W\\) is small, but it is not exact: almost surely, none of the atoms would lie exactly in it. So the second modification is to soften our requirement that \\(a_i \\in \\text{Range} \\left( \\tilde{Y} \\right)\\) to add \\(i\\) to \\(S\\). Instead, we will require that \\(a_i\\) is almost in \\(\\text{Range} \\left( \\tilde{Y} \\right)\\), by checking if it looses little magnitude when projected onto it: \\[\nc_i := \\frac{\\| \\text{Proj}_{\\text{Range} \\left( \\tilde{Y} \\right)}(a_i) \\|^2}\n{\\| a_i \\|^2 }\n\\text{ is close to 1}\n\\implies\n\\text{ add $i$ to $S$}\n\\] (what “is close” means exactly differs between implementations. When the atoms can be ordered, like in DOA estimation, it is common to use a peak selection algorithm).\nAs we said above, \\(\\tilde{Y}\\) is a rank-\\(\\left| S \\right|\\) approximation to \\(Y\\). In MUSIC, we use the best rank-\\(\\left| S \\right|\\) approximation in the least squares sense, which is given by the truncated singular value decomposition (SVD) of \\(Y\\). Note that we don’t really need to calculate \\(\\tilde{Y}\\) itself, all we really need is the projection to it’s range. Well, a nice about the SVD is that we can get it directly: \\[\n\\label{music_final}\nc_i = \\frac{\\| U^T a_i\\|^2}{\\| a_i\\|^2}.\n\\] where the columns of \\(U\\) are the first \\(\\left| S \\right|\\) left singular vectors (which form an orthonormal basis for \\(\\text{Range} \\left( \\tilde{Y} \\right)\\)).\nTo wrap things up, a few notes to connect the above to the “usual” MUSIC derivation:"
  },
  {
    "objectID": "posts/loocv/loocv.html",
    "href": "posts/loocv/loocv.html",
    "title": "Efficient Leave One Out Cross Validation - Part 1",
    "section": "",
    "text": "Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.\nLeave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.\nFor linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it’s RidgeCV estimator. Notably, this same trick extends beyond these linear models to any problem involving quadratically regularized least squares regression — a fact not widely recognized.\nTaking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.\nIn this first part, we derive efficient LOOCV for the quadratic case, and implement it python using JAX.\nIn part 2, we will extend the derivation for the non-quadratic case, and demonstrate these results on a practical example."
  },
  {
    "objectID": "posts/loocv/loocv.html#notation",
    "href": "posts/loocv/loocv.html#notation",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "",
    "text": "Suppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change."
  },
  {
    "objectID": "posts/loocv/loocv.html#ordinary-least-squares-ols",
    "href": "posts/loocv/loocv.html#ordinary-least-squares-ols",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "",
    "text": "Let’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\)."
  },
  {
    "objectID": "posts/loocv/loocv.html#augmented-least-squares",
    "href": "posts/loocv/loocv.html#augmented-least-squares",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "",
    "text": "We will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS."
  },
  {
    "objectID": "posts/loocv/loocv.html#ridge-regression",
    "href": "posts/loocv/loocv.html#ridge-regression",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "",
    "text": "Ride regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros."
  },
  {
    "objectID": "posts/loocv/loocv.html#example",
    "href": "posts/loocv/loocv.html#example",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "",
    "text": "For the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit."
  },
  {
    "objectID": "posts/loocv/loocv.html#beyond-least-squares",
    "href": "posts/loocv/loocv.html#beyond-least-squares",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "",
    "text": "Is it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  },
  {
    "objectID": "posts/loocv/loocv.html#footnotes",
    "href": "posts/loocv/loocv.html#footnotes",
    "title": "Efficient Leave One Out Cross Validation - Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am deliberately avoiding writing \\(\\theta = A^{-1} b\\), as \\(A\\) does not have to be invertible for this equation to have a solution, and it allows me to avoid the usual “assuming full rank” caveats people tend to use here. Furthermore, it can mislead people into implementations like np.linalg.inv(A) @ b, which are less stable and efficient than implementations like np.linalg.solve(A, b).↩︎\nThis approach translates better into code, as we get the expression for \\(\\tilde{y}_j\\) directly, without going through an expression for \\(\\theta^{(j)}\\) first. I also think Sherman-Morisson is a bit too strong here and can obscure some insights, so it’s nice to avoid it. But actually the other approach is just halfway it’s proof (see for example here).↩︎"
  },
  {
    "objectID": "posts/loocv/loocv.html#python-implementation",
    "href": "posts/loocv/loocv.html#python-implementation",
    "title": "Efficient Leave One Out Cross Validation",
    "section": "Python implementation",
    "text": "Python implementation\nThe approach outlined above adapts seamlessly into code. We’ll construct an estimator resembling the sklearn style, featuring standard fit and predict methods, alongside a function to compute \\(\\tilde{y}\\), the leave-one-out predictions:\n\nfrom typing import Self\n\nimport numpy as np\nimport scipy\n\n\nclass LinearRegressionWithQuadraticRegularization:\n    def __init__(self, R) -&gt; None:\n        self.R = R\n\n    def fit(self, X, y) -&gt; Self:\n        A = X.T @ X + self.R\n        b = X.T @ y\n        self.theta_ = scipy.linalg.solve(\n            A,\n            b,\n            overwrite_a=True,\n            overwrite_b=True,\n            assume_a=\"pos\",\n        )\n        return self\n\n    def predict(self, X) -&gt; np.ndarray:\n        return X @ self.theta_\n\n    def fit_loocv_predict(self, X, y) -&gt; np.ndarray:\n        A = X.T @ X + self.R\n        b = X.T @ y\n        temp = scipy.linalg.solve(\n            A,\n            np.vstack([b, X]).T,\n            overwrite_a=True,\n            overwrite_b=True,\n            assume_a=\"pos\",\n        )\n        self.theta_ = temp[:, 0]\n        t = temp[:, 1:]\n        h = np.einsum(\"ij,ji-&gt;i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n        y_hat = self.predict(X)\n        return y_hat + (h / (1 - h)) * (y_hat - y)\n\nLet’s check that our method for calculating the leave-one-out predictions is correct on random data, and compare it’s run time to the usual leave-one-out procedure.\n\nfrom sklearn.model_selection import LeaveOneOut\nfrom time import time\n\n\ndef standard_loocv(model, X, y) -&gt; np.ndarray:\n    y_tilde = np.empty_like(y)\n    for i, (train_index, test_index) in enumerate(LeaveOneOut().split(X)):\n        X_loo = X[train_index, :]\n        y_loo = y[train_index]\n        model.fit(X_loo, y_loo)\n        y_tilde[i] = model.predict(X[test_index, :])[0]\n    return y_tilde\n\n\ndef gen_random_data(n: int, m: int) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    rng = np.random.default_rng(42)\n    X = rng.standard_normal((n, m))\n    L = rng.standard_normal((m, m))\n    theta = L @ rng.standard_normal(m)\n    y = X @ theta + rng.standard_normal(n)\n    R = L @ L.T  # random positive definite matrix\n    return X, y, R\n\n\nX, y, R = gen_random_data(n=100, m=10)\nmodel = LinearRegressionWithQuadraticRegularization(R=R)\nprint(\n    f\"max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}\"\n)\n\nmax absolute error: 1.243e-14\n\n\nGood, the two methods to calculate \\(\\tilde{y}\\) give the same result. Let’s also compare the runtime:\n\n%timeit model.fit_loocv_predict(X, y) \n%timeit standard_loocv(model, X, y)\n\n34.6 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n2.39 ms ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nNice, a significant speedup. But that’s quite fast to begin with. Let’s increase n and m:\n\nX, y, R = gen_random_data(n=1000, m=50)\nmodel = LinearRegressionWithQuadraticRegularization(R=R)\nprint(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')\n%timeit model.fit_loocv_predict(X, y) \n%timeit standard_loocv(model, X, y)\n\nmax absolute error: 8.527e-14\n138 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nThe slowest run took 4.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n822 ms ± 461 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nHmm… Much less impressive. In theory the speedup should improve as the problem size increases. This is likely due to some python inefficiencies, not the algorithm itself. Let’s try to improve by using JAX’s just-in-time compilation feature:\n\nimport jax\n\n\nclass JitLinearRegressionWithQuadraticRegularization:\n    def __init__(self, R) -&gt; None:\n        self.R = R\n\n    def fit(self, X, y) -&gt; Self:\n        self.theta_ = self._fit(X, y, self.R)\n        return self\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._predict(X, self.theta_)\n\n    def fit_loocv_predict(self, X, y) -&gt; np.ndarray:\n        self.theta_, y_tilde = self._fit_loocv_predict(X, y, self.R)\n        return y_tilde\n    \n    @staticmethod\n    @jax.jit\n    def _fit(X, y, R) -&gt; np.ndarray:\n        return jax.scipy.linalg.solve(\n            X.T @ X + R, \n            X.T @ y,\n            overwrite_a=True,\n            overwrite_b=True,\n            assume_a=\"pos\",\n        )\n\n    @staticmethod\n    @jax.jit\n    def _predict(X, theta) -&gt; np.ndarray:\n        return X @ theta\n\n    @staticmethod\n    @jax.jit\n    def _fit_loocv_predict(X, y, R) -&gt; np.ndarray:\n        temp = jax.scipy.linalg.solve(\n            X.T @ X + R,\n            jax.numpy.vstack([X.T @ y, X]).T,\n            overwrite_a=True,\n            overwrite_b=True,\n            assume_a=\"pos\",\n        )\n        theta = temp[:, 0]\n        t = temp[:, 1:]\n        h = jax.numpy.einsum(\"ij,ji-&gt;i\", X, t)  # h[i] = np.dot(X[i, :], t[:, i])\n        y_hat = X @ theta\n        return theta, y_hat + (h / (1 - h)) * (y_hat - y)\n    \nmodel = JitLinearRegressionWithQuadraticRegularization(R=R)\nprint(f'max absolute error: {np.max(np.abs(model.fit_loocv_predict(X, y) - standard_loocv(model, X, y))):.3e}')\n%timeit model.fit_loocv_predict(X, y).block_until_ready()\n%timeit standard_loocv(model, X, y)\n\nmax absolute error: 4.780e-05\n1.75 ms ± 232 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n353 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nMuch better!"
  },
  {
    "objectID": "posts/loocv/loocv.html#python-implementation-1",
    "href": "posts/loocv/loocv.html#python-implementation-1",
    "title": "Efficient Leave One Out Cross Validation - Part 1",
    "section": "Python implementation",
    "text": "Python implementation\nOnce more, we’ll turn to jax, leveraging its automatic differentiation capabilities. Our estimator will take as inputs the loss and regularization functions, along with an optional “inverse link” function. This function can be employed to transform the predicted labels (e.g. a sigmoid to convert log-odds to probabilities in logistic regression)."
  },
  {
    "objectID": "posts/pearson_correlation/pearson_correlation.html#footnotes",
    "href": "posts/pearson_correlation/pearson_correlation.html#footnotes",
    "title": "A practical interpertation of the Pearson correlation coefficient",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAssuming we don’t use Bessel’s correction↩︎"
  },
  {
    "objectID": "posts/loocv/loocv_part1.html",
    "href": "posts/loocv/loocv_part1.html",
    "title": "Efficient leave one out cross validation - part 1",
    "section": "",
    "text": "Cross-validation is a crucial technique in assessing the performance of machine learning models. K-fold cross-validation, a widely-used method, involves dividing the dataset into K subsets, training the model K times, each time using a different subset as the testing set. This helps us gauge how well our model generalizes to unseen data. However, as K increases so does the computational time. This becomes painfully evident, particularly during hyperparameter tuning, where sluggish fits can be a major bottleneck.\nLeave-one-out cross-validation (LOOCV), a special case of K-fold cross-validation where K equals the number of training samples, can offer accurate evaluation but comes at a hefty computational cost, making it less practical for larger datasets and hyperparameter tuning.\nFor linear models like ordinary least squares and ridge regression, a little-known trick exists to efficiently calculate LOOCV scores. scikit-learn even implements this in it’s RidgeCV estimator. Notably, this same trick extends beyond these linear models to any quadratically regularized least squares regression — a fact not widely recognized.\nTaking it a step further, even for non-least-squares models like logistic and Poisson regression, a similar trick can be employed to approximate LOOCV scores efficiently. Intriguingly, the accuracy of this approximation improves with larger datasets, addressing the need for speedup in precisely those scenarios.\nIn this initial segment, we derive efficient LOOCV for the quadratic scenario and demonstrate its implementation in Python.\nIn part 2, we will build upon this derivation to cover non-quadratic scenarios and showcase these findings with a practical example dataset."
  },
  {
    "objectID": "posts/loocv/loocv_part1.html#python-implementation-1",
    "href": "posts/loocv/loocv_part1.html#python-implementation-1",
    "title": "Efficient Leave One Out Cross Validation - Part 1",
    "section": "Python implementation",
    "text": "Python implementation\nOnce more, we’ll turn to jax, leveraging its automatic differentiation capabilities. Our estimator will take as inputs the loss and regularization functions, along with an optional “inverse link” function. This function can be employed to transform the predicted labels (e.g. a sigmoid to convert log-odds to probabilities in logistic regression)."
  },
  {
    "objectID": "posts/loocv/loocv_part1.html#footnotes",
    "href": "posts/loocv/loocv_part1.html#footnotes",
    "title": "Efficient leave one out cross validation - part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am deliberately avoiding writing \\(\\theta = A^{-1} b\\), as \\(A\\) does not have to be invertible for this equation to have a solution, and it allows me to avoid the usual “assuming full rank” caveats people tend to use here. Furthermore, it can mislead people into implementations like np.linalg.inv(A) @ b, which are less stable and efficient than implementations like np.linalg.solve(A, b).↩︎\nThis approach translates better into code, as we get the expression for \\(\\tilde{y}_j\\) directly, without going through an expression for \\(\\theta^{(j)}\\) first. I also think Sherman-Morisson is a bit too strong here and can obscure some insights, so it’s nice to avoid it. But actually the other approach is just halfway it’s proof (see for example here).↩︎"
  },
  {
    "objectID": "posts/loocv_part2/loocv_part2.html",
    "href": "posts/loocv_part2/loocv_part2.html",
    "title": "Efficient Leave One Out Cross Validation - Part 2",
    "section": "",
    "text": "In the first part, we developed a method for performing efficient leave-one-out cross-validation (LOOCV). This method was precise but mandated that the loss and regularization functions be quadratic. Here, we’ll introduce a similar technique that provides an approximation, but eliminates the necessity for the loss and regularization to be quadratic. Additionally, we’ll code this method in Python using JAX and showcase its application on a sample dataset.\n\nNotation (same as part 1)\nWe denote the number of samples in the training dataset as \\(n\\).\nThe \\(m\\)-dimensional feature vectors are represented as \\(x_1\\) to \\(x_n\\), forming the rows of matrix \\(X\\).\nTargets are denoted as \\(y_1\\) to \\(y_n\\), forming the vector \\(y\\). The model’s prediction for the \\(i\\)-th training sample is \\(\\hat{y}_i = x_i^T \\theta\\), where \\(\\theta\\) is the coefficients vector. \\(\\hat{y} = X \\theta\\) represents the vector containing all predictions.\nWe fit \\(\\theta\\) to the training data by minimizing the combined loss and regularization terms: \\[\n\\theta := \\arg\\min_{\\theta'} f(\\theta').\n\\tag{1}\\] where \\[\nf(\\theta') := \\sum_{i=1}^{n} l(x_i^T \\theta'; y_i) + r(\\theta').\n\\] Here, \\(l(\\hat{y}_i; y_i)\\) represents the loss function, quantifying the difference between the prediction \\(\\hat{y}\\) and the true target \\(y_i\\), while \\(r\\) is the regularization function. We assume \\(l\\) (as a function of \\(\\hat{y}_i\\)) and \\(r\\) are convex and twice differentiable. Special cases of this model include ordinary least squares (\\(l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2\\), \\(r(\\theta') = 0\\)), ridge regression (\\(l(\\hat{y}_i; y_i) = (\\hat{y}_i - y_i)^2\\), \\(r(\\theta') = \\alpha \\| \\theta' \\|^2\\)), logistic regression (\\(l(\\hat{y}_i;y_i) = \\log \\left( 1 + e^{-y_i \\hat{y}_i}\\right)\\) with \\(y_i \\in \\{ -1, 1\\}\\)), and Poisson regression (\\(l(\\hat{y}_i;y_i) = y_i \\hat{y}_i - e^{\\hat{y}_i}\\)).\nTo denote the coefficients obtained by excluding the \\(j\\)-th example, we use \\(\\theta^{(j)}\\): \\[\n\\theta^{(j)} = \\arg\\min_{\\theta'} f^{(j)} (\\theta')\n\\] where \\[ f^{(j)}(\\theta') := \\sum_{i \\neq j} l(x_i^T \\theta'; y_i) + r(\\theta') \\] Similarly, \\(X^{(j)}\\) and \\(y^{(j)}\\), represent \\(X\\) and \\(y\\) with the \\(j\\)-th row removed, respectively. We denote by \\(\\tilde{y}_j\\) the predicted label for sample \\(j\\) when it is left out: \\[\n\\tilde{y}_j := x_j ^T \\theta^{(j)}\n\\tag{2}\\] Our goal is calculating \\(\\tilde{y}_j\\), for all \\(j\\), efficiently.\n\n\nDeriving efficient LOOCV for the non-quadratic case\nIn this section, we extend our approach to scenarios where \\(l\\) or \\(r\\) are not quadratic. Although solving equation Equation 1 is not simplified to solving a linear equation as it did in part 1, we can resort to the following approximation: \\[\nH^{(j)} (\\theta^{(j)} - \\theta) \\approx -g^{(j)}\n\\tag{3}\\] where \\(H^{(j)}\\) and \\(g^{(j)}\\) represent the Hessian and gradient of \\(f^{(j)}\\) at \\(\\theta\\), respectively. The rationale here is that \\(\\theta\\) and \\(\\theta^{(j)}\\) should be relatively close (and closer as \\(n\\) increases), making it likely that Newton’s method on \\(f^{(j)}\\) converges in a single iteration when initialized on \\(\\theta\\).\nSimilar to the quadratic case, we can relate \\(H^{(j)}\\) and \\(g^{(j)}\\) to \\(H\\) and \\(g\\), the Hessian and gradient of \\(f\\) at \\(\\theta\\): \\[\\begin{align*}\nH^{(j)} &= H - x_j l''(\\hat{y}_i ; y_i) x_j^T\n\\\\\ng^{(j)} &= g - x_j l'(\\hat{y}_i ; y_i) = - x_j l'(\\hat{y}_i ; y_i)\n\\end{align*}\\] allowing us to rewrite Equation 3 as: \\[\n\\left(\n    H - x_j l''\\left(\\hat{y}_i ; y_i\\right) x_j^T\n\\right)\n\\left( \\theta^{(j)} - \\theta \\right)\n\\approx  x_j l'(\\hat{y}_i ; y_i).\n\\] Next, we introduce the second equation: \\[\\begin{align*}\nH \\theta^{(j)}\n    - x_j l''(\\hat{y}_i ; y_i) \\tilde{y}_j\n    - H \\theta\n    + x_j l''(\\hat{y}_i ; y_i) \\hat{y}_j\n    &\\approx\n    x_j l'(\\hat{y}_i ; y_i)\n    \\\\\n    \\tilde{y}_j &= x_j ^T \\theta^{(j)}.\n\\end{align*}\\] Now, we can eliminate \\(\\theta^{(j)}\\) and solve for \\(\\tilde{y}_j\\): \\[\\begin{align*}\n\\theta^{(j)} &\\approx \\theta + t_j (l'(\\hat{y}_i ; y_i) +  l''(\\hat{y}_i ; y_i) (\\tilde{y}_j - \\hat{y}_j))\n\\\\\n\\tilde{y}_j &\\approx x_j ^T \\left(\n    \\theta + t_j (l'(\\hat{y}_i ; y_i) +  l''(\\hat{y}_i ; y_i) (\\tilde{y}_j - \\hat{y}_j))\n    \\right)\n\\\\\n\\tilde{y}_j &\\approx\n     \\hat{y}_j\n    + \\frac{h_j}{1 - h_j l''(\\hat{y}_i ; y_i)}  l'(\\hat{y}_i ; y_i)\n\\end{align*}\\] where \\(t_j := H^{-1} x_j\\) and \\(h_j := x_j^T t_j\\).\nIt’s worth noting the resemblance between the expression for \\(\\tilde{y}_j\\) here and the expression obtained for the quadratic case.\n\n\nPython implementation\nOnce more, we’ll turn to JAX, leveraging its automatic differentiation capabilities. Our estimator will take as inputs the loss and regularization functions, along with an optional “inverse link” function. This function can be employed to transform the predicted labels (e.g. a sigmoid to convert log-odds to probabilities in logistic regression, or an exponent to convert log-rate to rate in Poisson regression).\n\nfrom typing import Callable\n\nimport jax\nimport numpy as np\nimport numpy.typing as npt\nimport scipy\n\nArray = npt.NDArray[np.float64]\n\n\nclass GLMWithLOOCV:\n    def __init__(\n        self,\n        loss: Callable[[Array, Array], Array],\n        reg: Callable[[Array], float],\n        inverse_link: Callable[[Array], Array],\n    ) -&gt; None:\n        self.loss = loss\n        self.reg = reg\n        self.inverse_link = inverse_link\n\n    def f(self, theta: Array, X: Array, y: Array) -&gt; float:\n        y_hat = X @ theta\n        return self.loss(y_hat, y).sum() + self.reg(theta)\n\n    def fit(self, X: Array, y: Array):\n        # We optimize f with L-BFGS-B as it has reasonable performance with the data below,\n        #  but any other convex optimization algorithm can be used here.\n        result = scipy.optimize.minimize(\n            jax.value_and_grad(lambda theta: self.f(theta, X, y)),\n            x0=np.zeros(X.shape[1]),\n            method=\"L-BFGS-B\",\n            jac=True,\n        )\n        self.theta_ = result.x\n        return self\n\n    def predict(self, X: Array) -&gt; Array:\n        return self.inverse_link(X @ self.theta_)\n\n    def fit_loocv_predict(self, X: Array, y: Array) -&gt; Array:\n        self.fit(X, y)\n        y_hat = X @ self.theta_\n        l_prime = jax.vmap(jax.grad(self.loss, argnums=0))(y_hat, y)\n        l_prime_prime = jax.vmap(jax.hessian(self.loss, argnums=0))(y_hat, y)\n        H = jax.hessian(self.f, argnums=0)(self.theta_, X, y)\n        t = scipy.linalg.solve(\n            H,\n            X.T,\n            overwrite_a=True,\n            assume_a=\"pos\",\n        )\n        h = np.einsum(\"ij,ji-&gt;i\", X, t)\n        return self.inverse_link(y_hat + (h / (1 - h * l_prime_prime)) * l_prime)\n\n\n\nExample\nTo illustrate the concepts discussed above, we train a classifier on a dataset for predicting heart disease events. From a quick glance over Kaggle, it appears that achieving an AUC of approximately 0.9 is feasible.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer\nfrom sklearn import set_config\nfrom sklearn.metrics import roc_auc_score\n\ndf = pd.read_csv(\"data/heart.csv\")\ndf\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n45\nM\nTA\n110\n264\n0\nNormal\n132\nN\n1.2\nFlat\n1\n\n\n914\n68\nM\nASY\n144\n193\n1\nNormal\n141\nN\n3.4\nFlat\n1\n\n\n915\n57\nM\nASY\n130\n131\n0\nNormal\n115\nY\n1.2\nFlat\n1\n\n\n916\n57\nF\nATA\n130\n236\n0\nLVH\n174\nN\n0.0\nFlat\n1\n\n\n917\n38\nM\nNAP\n138\n175\n0\nNormal\n173\nN\n0.0\nUp\n0\n\n\n\n\n918 rows × 12 columns\n\n\n\n\ny = df[\"HeartDisease\"]\nX = df.drop(columns=[\"HeartDisease\"])\nX[\"one\"] = 1.0  # an all-ones column to implicitly fit an intercept term\nX = pd.get_dummies(X, drop_first=True)\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(642, 16), x_test.shape=(276, 16)\n\n\nOur approach involves using stratified logistic regression with a combination of Laplacian and sum of squares regularization. While this may not be the optimal model for this specific problem, it serves well for demonstrating the concepts.\nIn our model, we stratify over the sex of the patient, meaning we fit two coefficient vectors: one for males and one for females. The Laplacian regularization promotes similarity between the coefficient vectors for females and males. You can read more about stratified models with Laplacian regularization here.\n\ndef stratify(X: Array) -&gt; Array:\n    z = X[\"Sex_M\"].values[:, np.newaxis]\n    X = X.drop(columns=[\"Sex_M\"]).astype(float).values\n    return np.hstack([X * z, X * ~z])\n\n\ntransformer = Pipeline(\n    [\n        (\n            \"scale\",\n            ColumnTransformer(\n                [\n                    (\"none\", \"passthrough\", [\"one\", \"Sex_M\"]),\n                    (\n                        \"scale\",\n                        StandardScaler(),\n                        list(set(x_train.columns) - {\"one\", \"Sex_M\"}),\n                    ),\n                ],\n                verbose_feature_names_out=False,\n            ).set_output(transform=\"pandas\"),\n        ),\n        (\"stratify\", FunctionTransformer(stratify)),\n    ]\n)\n\nx_train = transformer.fit_transform(x_train)\nx_test = transformer.transform(x_test)\n\nNext we define the regularization matrices:\n\nm = x_train.shape[1]\nlaplacian = np.array([[1, -1], [-1, 1]])\nlaplacian = np.kron(laplacian, np.eye(m // 2))\nridge = np.eye(m)\nridge[0] = 0  # no penalty on the intercept\nridge[m // 2] = 0\n\nWe have two hyperparameters in our model: the strength of the sum of squares (ridge) regularization and the strength of the Laplacian regularization.\nFor hyperparameter optimization, we utilize Optuna.\n\nimport optuna\n\n\ndef model_factory(alpha: float, beta: float) -&gt; GLMWithLOOCV:\n    R = alpha * ridge + beta * laplacian\n    return GLMWithLOOCV(\n        loss=lambda y_hat, y: -jax.nn.log_sigmoid((y * 2 - 1) * y_hat),\n        reg=lambda theta: theta.T @ R @ theta,\n        inverse_link=jax.nn.sigmoid,\n    )\n\n\ndef objective(trial: optuna.Trial):\n    alpha = trial.suggest_float(\"alpha\", 1e-6, 1e3, log=True)\n    beta = trial.suggest_float(\"beta\", 1e-6, 1e3, log=True)\n    model = model_factory(alpha, beta)\n    y_tilde = model.fit_loocv_predict(x_train, y_train.values)\n    return -roc_auc_score(\n        y_train, y_tilde\n    )  # minus since optuna minimizes the objective and we need to maximize\n\n\nstudy: optuna.Study = optuna.create_study()\nstudy.optimize(objective, n_trials=50)\nmodel = model_factory(**study.best_params)\nroc_auc_score(\n    y_test,\n    model.fit(x_train, y_train.values).predict(x_test),\n)\n\n[I 2024-03-24 08:56:29,175] A new study created in memory with name: no-name-61410be4-4d1d-4a9b-8cc6-068004336d5c\n[I 2024-03-24 08:56:30,700] Trial 0 finished with value: -0.9092301389105666 and parameters: {'alpha': 0.012775258780126265, 'beta': 0.002191596541067304}. Best is trial 0 with value: -0.9092301389105666.\n[I 2024-03-24 08:56:31,058] Trial 1 finished with value: -0.909025284844701 and parameters: {'alpha': 0.0006298752554559054, 'beta': 2.1509611015314225e-05}. Best is trial 0 with value: -0.9092301389105666.\n[I 2024-03-24 08:56:31,605] Trial 2 finished with value: -0.9118639769002653 and parameters: {'alpha': 4.938878297009436e-06, 'beta': 780.3122728275399}. Best is trial 2 with value: -0.9118639769002653.\n[I 2024-03-24 08:56:31,926] Trial 3 finished with value: -0.914858748244108 and parameters: {'alpha': 3.1356031871802617, 'beta': 30.256480198543056}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:32,239] Trial 4 finished with value: -0.9104885281723115 and parameters: {'alpha': 0.005919567636794085, 'beta': 0.21420343169280254}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:32,557] Trial 5 finished with value: -0.9091325893553925 and parameters: {'alpha': 0.0029083107541165135, 'beta': 0.003468355333742003}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:32,947] Trial 6 finished with value: -0.9103422038395502 and parameters: {'alpha': 0.25663606336322276, 'beta': 2.027770483000284e-06}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:33,339] Trial 7 finished with value: -0.912537068830966 and parameters: {'alpha': 1.0538711667069038, 'beta': 0.0003360283526189701}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:33,622] Trial 8 finished with value: -0.9089862650226315 and parameters: {'alpha': 1.079098816654311e-05, 'beta': 1.3846427704434615e-06}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:34,060] Trial 9 finished with value: -0.9089667551115967 and parameters: {'alpha': 1.2646277638647537e-06, 'beta': 5.240465554229351e-05}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:34,378] Trial 10 finished with value: -0.8929003433744341 and parameters: {'alpha': 521.943351068861, 'beta': 21.128403850663364}. Best is trial 3 with value: -0.914858748244108.\n[I 2024-03-24 08:56:34,601] Trial 11 finished with value: -0.9151416419541125 and parameters: {'alpha': 7.849754690834663, 'beta': 0.6886856542708458}. Best is trial 11 with value: -0.9151416419541125.\n[I 2024-03-24 08:56:34,839] Trial 12 finished with value: -0.9127614328078664 and parameters: {'alpha': 36.17696304784756, 'beta': 0.9382223092187879}. Best is trial 11 with value: -0.9151416419541125.\n[I 2024-03-24 08:56:35,023] Trial 13 finished with value: -0.9154538005306695 and parameters: {'alpha': 20.819919697832866, 'beta': 18.009933396875475}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:35,430] Trial 14 finished with value: -0.9134150148275324 and parameters: {'alpha': 50.2350037043773, 'beta': 3.819223887756615}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:35,728] Trial 15 finished with value: -0.8607480099890744 and parameters: {'alpha': 768.6568030874997, 'beta': 0.07013747752246598}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:36,291] Trial 16 finished with value: -0.9138930076478852 and parameters: {'alpha': 9.155957878379764, 'beta': 819.8096902730499}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:36,608] Trial 17 finished with value: -0.9130540814733884 and parameters: {'alpha': 0.1485485239581707, 'beta': 38.97815021766497}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:36,858] Trial 18 finished with value: -0.9053866864367098 and parameters: {'alpha': 78.72829138458891, 'beta': 0.016036059061954787}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:37,161] Trial 19 finished with value: -0.9136783986265022 and parameters: {'alpha': 0.1375243107258574, 'beta': 1.8291549312508841}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:37,528] Trial 20 finished with value: -0.9129760418292493 and parameters: {'alpha': 1.0707674807216208, 'beta': 117.83261301793227}. Best is trial 13 with value: -0.9154538005306695.\n[I 2024-03-24 08:56:37,795] Trial 21 finished with value: -0.9159903230841268 and parameters: {'alpha': 4.416371155644641, 'beta': 9.316946205587612}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:38,065] Trial 22 finished with value: -0.9146831590447948 and parameters: {'alpha': 12.426070601603788, 'beta': 0.3853665693475585}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:38,316] Trial 23 finished with value: -0.8677228031840175 and parameters: {'alpha': 319.5534395847857, 'beta': 2.7762991166087905}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:38,620] Trial 24 finished with value: -0.9158342437958482 and parameters: {'alpha': 3.112413409324069, 'beta': 7.37150462979163}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:38,914] Trial 25 finished with value: -0.9150733572654908 and parameters: {'alpha': 0.7868633423237494, 'beta': 8.431547546648925}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:39,290] Trial 26 finished with value: -0.9122541751209613 and parameters: {'alpha': 0.03174734240904159, 'beta': 114.13914385182184}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:39,658] Trial 27 finished with value: -0.9116103480568128 and parameters: {'alpha': 90.62207805800448, 'beta': 202.6257677752164}. Best is trial 21 with value: -0.9159903230841268.\n[I 2024-03-24 08:56:39,917] Trial 28 finished with value: -0.9160781176837834 and parameters: {'alpha': 6.553554396630455, 'beta': 11.167094954503991}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:40,310] Trial 29 finished with value: -0.9098349461526456 and parameters: {'alpha': 0.029887003146111108, 'beta': 0.05116561128441574}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:40,633] Trial 30 finished with value: -0.9140198220696114 and parameters: {'alpha': 2.942224930336697, 'beta': 0.00513193420243311}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:40,934] Trial 31 finished with value: -0.915219681598252 and parameters: {'alpha': 22.919512362299404, 'beta': 12.27623192594196}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:41,189] Trial 32 finished with value: -0.9157464491961917 and parameters: {'alpha': 2.757918964666127, 'beta': 6.038269195534732}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:41,617] Trial 33 finished with value: -0.9142149211799594 and parameters: {'alpha': 2.838682256536785, 'beta': 0.22519746478261113}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:41,949] Trial 34 finished with value: -0.9148392383330732 and parameters: {'alpha': 0.3616241392613928, 'beta': 5.830178783914518}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:42,382] Trial 35 finished with value: -0.9120005462775089 and parameters: {'alpha': 0.0008822564977771567, 'beta': 307.9388398609111}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:42,562] Trial 36 finished with value: -0.9134930544716716 and parameters: {'alpha': 0.04187871349145953, 'beta': 1.8734690287950477}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:42,925] Trial 37 finished with value: -0.9120590760106134 and parameters: {'alpha': 120.58206602832338, 'beta': 38.5688359496023}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:43,277] Trial 38 finished with value: -0.9141856563134072 and parameters: {'alpha': 2.86004817515422, 'beta': 0.14090599669599804}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:43,658] Trial 39 finished with value: -0.9131028562509754 and parameters: {'alpha': 0.46403986565570593, 'beta': 48.64240100580483}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:44,034] Trial 40 finished with value: -0.9092106289995319 and parameters: {'alpha': 3.056882608727012e-05, 'beta': 0.013794522533314474}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:44,311] Trial 41 finished with value: -0.9159513032620571 and parameters: {'alpha': 7.453204156267973, 'beta': 18.41659964949616}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:44,593] Trial 42 finished with value: -0.916068362728266 and parameters: {'alpha': 5.494711869607197, 'beta': 6.6964359014868124}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:44,981] Trial 43 finished with value: -0.9152391915092868 and parameters: {'alpha': 6.789849837478972, 'beta': 0.9088200937340541}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:45,353] Trial 44 finished with value: -0.9095422974871235 and parameters: {'alpha': 272.45143282464653, 'beta': 86.15445718504921}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:45,689] Trial 45 finished with value: -0.9129857967847667 and parameters: {'alpha': 1.245276289906343, 'beta': 494.7421191070618}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:46,090] Trial 46 finished with value: -0.9099032308412675 and parameters: {'alpha': 0.0911593221200086, 'beta': 0.0005329474451518324}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:46,388] Trial 47 finished with value: -0.9154050257530826 and parameters: {'alpha': 19.418957034796815, 'beta': 17.869675293948607}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:46,745] Trial 48 finished with value: -0.9114152489464649 and parameters: {'alpha': 0.004900207869786724, 'beta': 0.5092857463474738}. Best is trial 28 with value: -0.9160781176837834.\n[I 2024-03-24 08:56:47,053] Trial 49 finished with value: -0.9153757608865303 and parameters: {'alpha': 5.6025164558499885, 'beta': 1.4684843094190325}. Best is trial 28 with value: -0.9160781176837834.\n\n\n0.9398954703832751\n\n\n\n\nVerification of implementation\nTo assess the accuracy of our implementation, we compare the approximated leave-one-out predictions with the actual leave-one-out predictions:\n\nfrom time import time\n\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"notebook\"\n\nfrom sklearn.model_selection import LeaveOneOut\n\nt_start = time()\ny_tilde = np.zeros(y_train.shape)\nfor i, (train_index, val_index) in enumerate(LeaveOneOut().split(x_train)):\n    X_loo = x_train[train_index, :]\n    y_loo = y_train.values[train_index]\n    model.fit(X_loo, y_loo)\n    y_tilde[i] = model.predict(x_train[val_index, :])[0]\nstandard_loocv_runtime = time() - t_start\n\nt_start = time()\ny_tilde_approx = model.fit_loocv_predict(x_train, y_train.values)\nefficient_loocv_runtime = time() - t_start\n\nprint(f\"{np.abs(y_tilde - y_tilde_approx).mean()=}\")\npx.scatter(x=y_tilde, y=y_tilde_approx)\n\nnp.abs(y_tilde - y_tilde_approx).mean()=4.465176e-05\n\n\n                                                \n\n\n\npx.histogram(y_tilde - y_tilde_approx)\n\n                                                \n\n\nThe approximation demonstrates high accuracy in this instance. Now, let’s compare the runtimes:\n\nprint(f\"{standard_loocv_runtime = :.1e}\")\nprint(f\"{efficient_loocv_runtime = :.1e}\")\nprint(f\"{standard_loocv_runtime/efficient_loocv_runtime = :.0f}.\")\n\nstandard_loocv_runtime = 4.7e+01\nefficient_loocv_runtime = 1.3e-01\nstandard_loocv_runtime/efficient_loocv_runtime = 357.\n\n\nA significant speedup! However, it’s important to note that there is room for further optimization. For instance, the BFGS iterations could be initialized with a previous solution, or we could utilize JIT compilation as we did in part 1.\n\n\nReferences\nEfficient LOOCV for ordinary least squares and ridge regression is mentioned in several well known books like The Elements of Statistical Learning and An Introduction to Statistical Learning. I first encountered it in a brief mention in All of Statistics.\nThe only reference I am aware of that discusses the general quadratic case, and a similar approach for the non-quadratic approximation, is this theses by Rosa Meijer."
  },
  {
    "objectID": "posts/loocv_part2/loocv_part2.html#python-implementation",
    "href": "posts/loocv_part2/loocv_part2.html#python-implementation",
    "title": "Efficient Leave One Out Cross Validation - Part 2",
    "section": "Python implementation",
    "text": "Python implementation\nOnce more, we’ll turn to jax, leveraging its automatic differentiation capabilities. Our estimator will take as inputs the loss and regularization functions, along with an optional “inverse link” function. This function can be employed to transform the predicted labels (e.g. a sigmoid to convert log-odds to probabilities in logistic regression, or an exponent to convert log-rate to rate in Poisson regression).\n\nfrom typing import Callable, Self\n\nimport jax\nimport numpy as np\nimport numpy.typing as npt\nimport scipy\n\nArray = npt.NDArray[np.float64]\n\n\nclass GLMWithLOOCV:\n    def __init__(\n        self,\n        loss: Callable[[Array, Array], Array],\n        reg: Callable[[Array], float],\n        inverse_link: Callable[[Array], Array],\n    ) -&gt; None:\n        self.loss = loss\n        self.reg = reg\n        self.inverse_link = inverse_link\n\n    def f(self, theta: Array, X: Array, y: Array) -&gt; float:\n        y_hat = X @ theta\n        return self.loss(y_hat, y).sum() + self.reg(theta)\n\n    def fit(self, X: Array, y: Array) -&gt; Self:\n        # We optimize f with L-BFGS-B as it has reasonable performance with the data below,\n        #  but any other convex optimization algorithm can be used here.\n        result = scipy.optimize.minimize(\n            jax.value_and_grad(lambda theta: self.f(theta, X, y)),\n            x0=np.zeros(X.shape[1]),\n            method=\"L-BFGS-B\",\n            jac=True,\n        )\n        self.theta_ = result.x\n        return self\n\n    def predict(self, X: Array) -&gt; Array:\n        return self.inverse_link(X @ self.theta_)\n\n    def fit_loocv_predict(self, X: Array, y: Array) -&gt; Array:\n        self.fit(X, y)\n        y_hat = X @ self.theta_\n        l_prime = jax.vmap(jax.grad(self.loss, argnums=0))(y_hat, y)\n        l_prime_prime = jax.vmap(jax.hessian(self.loss, argnums=0))(y_hat, y)\n        H = jax.hessian(self.f, argnums=0)(self.theta_, X, y)\n        t = scipy.linalg.solve(\n            H,\n            X.T,\n            overwrite_a=True,\n            assume_a=\"pos\",\n        )\n        h = np.einsum(\"ij,ji-&gt;i\", X, t)\n        return self.inverse_link(y_hat + (h / (1 - h * l_prime_prime)) * l_prime)"
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html",
    "title": "Augmentation is Regularization 2",
    "section": "",
    "text": "Training data augmentation enhances the training dataset by applying transformations to existing training data instances. The specific transformations vary depending on the type of data involved, and this flexibility allows to leverage domain knowledge, such as known invariants, effectively. The goal is to introduce variability and increase the diversity of the training set, allowing the model to better generalize to unseen data and exhibit improved robustness. Despite the advantages, training data augmentation introduces an inherent computational cost: the increased volume of data requires additional computational resources, impacting both training time and memory requirements.\nAs we will show below, for linear models with the sum of squares loss, training data augmentation is equivalent to adding quadratic regularization term, which implies that the computational cost of fitting a model to an augmented dataset is the same as using no augmentation at all!\nThis link between augmentation and regularization is useful in the other direction as well: it gives a concrete interpretation to the value of regularization hyperparameters, and can be used to avoid costly hyperparameters tuning (np.logspace(-6, 6, 100) much?), and to design regularizers that are more appropriate to the data than the simple ones (i.e sum of squares regularization used in ridge regression)."
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#notation",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#notation",
    "title": "Augmentation is Regularization 2",
    "section": "Notation",
    "text": "Notation\nSuppose we have a training data set comprised of \\(n\\) pairs \\(x_i,\\,y_i\\) for \\(i=0, \\dots, n-1\\), where \\(x_i\\) is the \\(d\\) dimensional feature vector of the \\(i\\)’th training data, and \\(y_i\\) is the corresponding label. Here we will assume \\(y_i \\in \\mathrm{R}\\), however our results can be easily extended to the vector-labeled (aka multi-output) case. We will also denote by \\(X\\) the \\(n\\)-by-\\(d\\) matrix with rows \\(x_0^T, \\dots, x_{n-1}^T\\) and by \\(y\\) the \\(n\\)-vector with entries \\(y_0, \\dots, y_{n-1}\\).\nLet \\(a:\\mathrm{R}^d \\times \\mathcal{P}  \\mapsto \\mathrm{R}^d\\) denote the augmentation function that given the augmentation params \\(p \\in \\mathcal{P}\\), maps a feature vector \\(x\\) to a transformed feature vector. The augmentation parameters \\(p\\) are usually sampled randomly from a given distribution. For example, for image data, \\(a\\) is often a composition of small shifts, rotations, brightness changes, etc. while \\(p\\) specifies the amount of shifting, rotation and brightness change."
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#ordinary-least-squares-ols",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#ordinary-least-squares-ols",
    "title": "Augmentation is Regularization 2",
    "section": "Ordinary least squares (OLS)",
    "text": "Ordinary least squares (OLS)\nLet’s quickly discuss OLS so we can compare it’s equations with the augmented version we will derive after.\nTo fit an OLS model, we find a vector of coefficients \\(\\theta_\\text{OLS}\\) that minimizes the sum of squared training errors: \\[\\begin{align*}\n    \\theta_\\text{OLS} &:= \\text{argmin} _\\theta \\sum_{i=0} ^{n-1} \\left(\n    x_i ^T \\theta - y_i\n    \\right) ^2\n    \\\\&= \\text{argmin} _\\theta \\| X \\theta - y \\|^2 \\tag{1}\n\\end{align*}\\] To solve the optimization problem (1), we solve the equation \\(X^TX \\theta_\\text{OLS} = X^T y\\), which has time complexity \\(O(n d^2)\\)."
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#augmented-least-squares",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#augmented-least-squares",
    "title": "Augmentation is Regularization 2",
    "section": "Augmented least squares",
    "text": "Augmented least squares\nWe will now fit a model by finding coefficients \\(\\theta_\\text{ALS}\\) that minimize the expected error over the augmented training dataset: \\[\\begin{align*}\n    \\theta_\\text{ALS} &:= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta - y_i\n    \\right) ^2\n    \\right], \\tag{2}\n\\end{align*}\\] where the expectation is over \\(p_0,\\dots, p_{n-1}\\), the random augmentation parameters. As we will see below, \\(\\theta_\\text{ALS}\\) depends on \\(a\\) and the distribution of \\(p\\) only through the 2nd order moments, which we denote by \\[\\begin{align*}\n    \\mu_i &:= \\mathrm{E} \\left[a(x_i, p_i) \\right]\\\\\n    R_i &:= \\mathrm{C}\\text{ov} \\left[ a(x_i, p_i) \\right].\n\\end{align*}\\]\nContinuing from (2), we use the standard trick of subtracting and adding the mean: \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta \\mathrm{E}  \n    \\left[\n        \\sum_{i=0} ^{n-1} \\left(\n            \\left(\n                \\mu_i^T \\theta - y_i\n            \\right)\n            + \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right) ^2\n    \\right]\n\\end{align*}\\] Note that the first term $ ( _i^T - y_i ) $ is deterministic, while the second term $ ( a(x_i, p_i) - _i )^T $ has zero mean. Therefore \\[\\begin{align*}\n    \\theta_\\text{ALS} &= \\text{argmin} _\\theta\n    \\sum_{i=0} ^{n-1}\n        \\left(\n            \\mu_i^T \\theta - y_i\n        \\right)\n        +\n        \\mathrm{E} \\left[\\left(\n            \\left(\n                a\\left(x_i, p_i\\right) -\n                \\mu_i\n            \\right)^T\\theta\n        \\right)^2 \\right] \\\\\n    &= \\text{argmin} _\\theta\n    \\| M \\theta - y\\|^2 + \\theta ^T R \\theta,\n    \\tag{3}\n\\end{align*}\\] where \\(M\\) is the \\(n\\)-by-\\(d\\) matrix whose rows are \\(\\mu_0^T, \\dots, \\mu_{n-1}^T\\), and \\[\nR := \\sum_{i=0} ^{n-1} R_i.\n\\] Equation (3) shows exactly what we set to prove - fitting a model on augmented training dataset, is equivalent to fitting a non-augmented, but quadratically regularized, least squares model. We just replace \\(X\\) with it’s mean, and use the sum of all covariances as the regularization matrix.\nTo solve the optimization problem (3), we solve the equation \\((X^T X + R) \\theta_\\text{ALS} = X^T y\\), which has the same \\(O(n d^2)\\) complexity as OLS."
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#ridge-regression",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#ridge-regression",
    "title": "Augmentation is Regularization 2",
    "section": "Ridge regression",
    "text": "Ridge regression\nRide regression (aka Tykhonov regularization) has the form (3) with \\(M=X\\) and \\(R=\\lambda I\\). As an augmentation, it can be interpreted as follows: perturb each feature vector by a zero mean noise, with variance \\(\\lambda/n\\), uncorrelated across features.\nThis interpretation of \\(\\lambda\\) can be used to set it (at least roughly): just think what level of perturbation \\(\\sigma\\) is reasonable for your features, and set \\(\\lambda = n \\sigma^2\\).\nThis also shows that when different feature are scaled differently, ridge regression is perhaps not the best fit. A standard deviation of 100 might be reasonable for a feature with values in the order of millions, but it is probably not suitable for a feature with values in the order of 1. In these cases, we may use a diagonal \\(R\\): \\[\\begin{align*}\n    R= n \\, \\text{diag} \\left(\n        \\sigma_0^2, \\dots, \\sigma_{d-1}^2\n    \\right)\n\\end{align*}\\] where \\(\\sigma_i\\) is the standard deviation of the perturbation of feature \\(i\\).\nAnother option is to scale the transformations before fit, e.g using sklearn’s StandardScaler. With all features scaled to have unit variance, setting \\(\\lambda = n \\, 10 ^{-6}\\) is a sensible rule of thumb, as it is often reasonable to assume a \\(0.1\\%\\) perturbation.\nNote that often the model includes an intercept (aka constant) term by adding a column of ones to \\(X\\). Since this column remain unchanged through any augmenting transformation, the corresponding row and column of \\(R\\) should be all zeros."
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#example",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#example",
    "title": "Augmentation is Regularization 2",
    "section": "Example",
    "text": "Example\nFor the example we are gonna use the House Sales in King County, USA dataset. Each row describes a house, sold between May 2014 and May 2015. Our goal will be to predict the log price given features like number of rooms, area, and geography.\nNote: several decisions outlined below weren’t necessarily the most effective,;, rather, they were chosen to showcase different modelling techniques in the context of augmentation via regularization.\nLet’s begin by importing everything we will need, loading our data, and adding some columns.\n\nfrom typing import Callable, Hashable, Self\n\nimport numpy as np\nfrom numpy.typing import NDArray\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nArray = NDArray[np.float64]\n\ndf = pd.read_csv(\"data/kc_house_data.csv.zip\", parse_dates=[\"date\"])\ndf[\"long_scaled\"] = df[\"long\"] * np.mean(\n    np.abs(np.cos(df[\"lat\"] * np.pi / 180))\n)  # earth-curvature correction for (approximate) distance calculations\ndf.describe()\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nlong_scaled\n\n\n\n\ncount\n2.161300e+04\n21613\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n2.161300e+04\n21613.000000\n21613.000000\n21613.000000\n...\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n21613.000000\n\n\nmean\n4.580302e+09\n2014-10-29 04:38:01.959931648\n5.400881e+05\n3.370842\n2.114757\n2079.899736\n1.510697e+04\n1.494309\n0.007542\n0.234303\n...\n1788.390691\n291.509045\n1971.005136\n84.402258\n98077.939805\n47.560053\n-122.213896\n1986.552492\n12768.455652\n-82.471784\n\n\nmin\n1.000102e+06\n2014-05-02 00:00:00\n7.500000e+04\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.000000\n0.000000\n0.000000\n...\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.000000\n-82.677673\n\n\n25%\n2.123049e+09\n2014-07-22 00:00:00\n3.219500e+05\n3.000000\n1.750000\n1427.000000\n5.040000e+03\n1.000000\n0.000000\n0.000000\n...\n1190.000000\n0.000000\n1951.000000\n0.000000\n98033.000000\n47.471000\n-122.328000\n1490.000000\n5100.000000\n-82.548783\n\n\n50%\n3.904930e+09\n2014-10-16 00:00:00\n4.500000e+05\n3.000000\n2.250000\n1910.000000\n7.618000e+03\n1.500000\n0.000000\n0.000000\n...\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571800\n-122.230000\n1840.000000\n7620.000000\n-82.482651\n\n\n75%\n7.308900e+09\n2015-02-17 00:00:00\n6.450000e+05\n4.000000\n2.500000\n2550.000000\n1.068800e+04\n2.000000\n0.000000\n0.000000\n...\n2210.000000\n560.000000\n1997.000000\n0.000000\n98118.000000\n47.678000\n-122.125000\n2360.000000\n10083.000000\n-82.411796\n\n\nmax\n9.900000e+09\n2015-05-27 00:00:00\n7.700000e+06\n33.000000\n8.000000\n13540.000000\n1.651359e+06\n3.500000\n1.000000\n4.000000\n...\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.000000\n-81.865195\n\n\nstd\n2.876566e+09\nNaN\n3.671272e+05\n0.930062\n0.770163\n918.440897\n4.142051e+04\n0.539989\n0.086517\n0.766318\n...\n828.090978\n442.575043\n29.373411\n401.679240\n53.505026\n0.138564\n0.140828\n685.391304\n27304.179631\n0.095033\n\n\n\n\n8 rows × 22 columns\n\n\n\nWe will want a polynomial (rather than linear) dependency on the age of the house:\n\ndf[\"age\"] = df[\"date\"].dt.year - df[\"yr_built\"]\nage_cols = [\"age\"]\nfor power in range(2, 5):\n    col = f\"age ^ {power}\"\n    df[col] = df[\"age\"] ** power\n    age_cols.append(col)\n\nWe do a 10-90 train-test split to demonstrate the effectiveness of augmentation when we have little data.\n\ny = np.log(df[\"price\"])\nX = df.drop(columns=[\"price\"])\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.9, random_state=42\n)\nprint(f\"{x_train.shape=}, {x_test.shape=}\")\n\nx_train.shape=(2161, 25), x_test.shape=(19452, 25)\n\n\nThere is no reason to expect a linear relationship between the house geographical coordinates and it’s price.\nHowever, we do expect a strong dependency between price and location in the sense that houses with similar features should share similar prices when located in close geographical proximity.\nOne way to model this is to cluster the data geographically, and tag each house with the cluster it belongs to using one hot encoding:\n\n# We need this class mainly since the transform method of sklearn's k-means class yields cluster centers, and we want one hot encoding.\nclass OneHotEncodedKMeansTransformer:\n    def __init__(self, k: int, columns: list[str], name: str) -&gt; None:\n        self.columns = columns\n        self.k = k\n        self.name = name\n\n    def fit(self, X: pd.DataFrame) -&gt; Self:\n        self.kmeans_ = KMeans(n_clusters=self.k, n_init=\"auto\", random_state=42)\n        self.kmeans_.fit(X[self.columns])\n        return self\n\n    def column_names(self) -&gt; list[str]:\n        return [f\"{self.name}_{i}\" for i in range(self.k)]\n\n    def transform(self, X: pd.DataFrame):\n        cluster_index = self.kmeans_.predict(X[self.columns])\n        return pd.concat(\n            [\n                X,\n                pd.DataFrame(\n                    np.eye(self.k)[cluster_index],\n                    columns=self.column_names(),\n                    index=X.index,\n                ),\n            ],\n            axis=1,\n        )\n\n    def clusters_adjacency_matrix(self):\n        edges = np.array(\n            scipy.spatial.Voronoi(self.kmeans_.cluster_centers_).ridge_points\n        ).T\n        a = scipy.sparse.coo_matrix(\n            (np.ones(edges.shape[1]), (edges[0], edges[1])),\n            shape=(self.k, self.k),\n        )\n        return a + a.T\n\n\nkmeans_transformer = OneHotEncodedKMeansTransformer(\n    k=500,\n    columns=[\"lat\", \"long_scaled\"],\n    name=\"geo_cluster\",\n)\nx_train = kmeans_transformer.fit(x_train).transform(x_train)\nx_test = kmeans_transformer.transform(x_test)\n\nWe will evaluate our models by their R squared score. From a quick glance over Kaggle, it seems that sophisticated and advanced models (e.g XGBoost) can achieve a score of about 0.9. Let’s see if we can get there using a linear model.\n\ndef evaluate_model(model) -&gt; None:\n    y_train_pred = model.fit(x_train, y_train).predict(x_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    y_test_pred = model.predict(x_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    print(f\"{r2_train=:.3f}, {r2_test=:.3f}\")\n\nLet’s start with a vanilla linear model, without any regularization/augmentations.\n\ncolumns = (\n    [\n        \"bedrooms\",\n        \"bathrooms\",\n        \"floors\",\n        \"waterfront\",\n        \"view\",\n        \"condition\",\n        \"grade\",\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n    + age_cols\n    + kmeans_transformer.column_names()\n)\ncolumns_selector = ColumnTransformer(\n    [(\"selector\", \"passthrough\", columns)],\n    remainder=\"drop\",\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nsimple_linear = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"linear\", LinearRegression(fit_intercept=False)),\n    ]\n)\nevaluate_model(simple_linear)\n\nr2_train=0.918, r2_test=0.862\n\n\nNot bad, but we do have some overfitting. Let’s see if we can improve generalization with regularization/augmentation. First we try ridge regression:\n\nridge = Pipeline(\n    [\n        (\"selector\", columns_selector),\n        (\"scale\", StandardScaler()),\n        (\n            \"linear\",\n            RidgeCV(\n                fit_intercept=True,\n                alphas=x_train.shape[0] * np.logspace(-9, -2, 100),\n            ),\n        ),\n    ]\n)\nevaluate_model(ridge)\n\nr2_train=0.918, r2_test=0.863\n\n\nThat didn’t really help. That makes sense since using a diagonal regularization matrix doesn’t make sense for our correlated features.\nLet’s see if we can do better by using augmentations that are more appropriate for our data.\nFirst let’s build a class for linear models with augmentation via regularization.\nWe will pass to the constructor a callable that takes the input features and returns their mean and (sum of) covariance after the augmentation, since as we shown above these are all we need from the augmentations.\nSince often the transformations of different features are uncorrelated, it is convenient to specify features in groups, and assume zero covariance for features that are not in the same group (i.e a block diagonal covariance matrix).\n\nclass AugmentedLinearModel:\n    def __init__(\n        self,\n        augmentation_moments: list[  # one item for each group of features\n            tuple[\n                list[Hashable],  # column names of features in the group\n                Callable[[pd.DataFrame], tuple[Array, Array]],  # maps X to M and R\n            ]\n        ],\n    ) -&gt; None:\n        self.augmentation_moments = augmentation_moments\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; Self:\n        means, covs = zip(\n            *(\n                moments(X.loc[:, columns])\n                for columns, moments in self.augmentation_moments\n            )\n        )\n        M = np.hstack(means)\n        # https://scikit-learn.org/stable/developers/develop.html#estimated-attributes\n        self.R_ = scipy.linalg.block_diag(*covs)\n        self.theta_ = np.linalg.solve(M.T @ M + self.R_, M.T @ y)\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.Series:\n        cols = [col for cols, _ in self.augmentation_moments for col in cols]\n        return X.loc[:, cols] @ self.theta_\n\nHere are the augmentations we are gonna use:\nWith 10% probability, a bathroom is counted as half a bedroom.\n\ndef bedrooms_bathrooms_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.1\n    v = np.array([1, -0.5])\n    mask = X[[\"bathrooms\"]] &gt;= 1\n    M = np.where(mask, X + p * v, X)\n    R = p * (1 - p) * np.outer(v, v) * mask.values.sum()\n    return M, R\n\n\naugmentation_moments = [([\"bedrooms\", \"bathrooms\"], bedrooms_bathrooms_moments)]\n\nA 5% perturbation for the features\nsqft_living, sqft_lot, sqft_above, sqft_basement, sqft_lot15, sqft_living15, uncorrelated across the features.\n\ndef relative_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, np.sum(X.values**2) * 0.05**2\n\n\naugmentation_moments.extend(\n    ([column], relative_perturbation_moments)\n    for column in [\n        \"sqft_living\",\n        \"sqft_lot\",\n        \"sqft_above\",\n        \"sqft_basement\",\n        \"sqft_lot15\",\n        \"sqft_living15\",\n    ]\n)\n\nA perturbation of 0.01 for the features floors, waterfront, view, condition, grade, uncorrelated across the features\n\ndef absolute_perturbation_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    return X.values, X.shape[0] * 0.01**2\n\n\naugmentation_moments.extend(\n    ([column], absolute_perturbation_moments)\n    for column in [\"floors\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n)\n\nperturbing age with a uniform distribution between -1 and 1. We need to calculate the moments for the power of age accordingly.\n\ndef age_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    a = X[[\"age\"]].values - 1\n    b = X[[\"age\"]].values + 1\n    max_power = X.shape[1]\n    np1 = np.arange(2, 2 * max_power + 2)\n    # https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Moments\n    mu = (b**np1 - a**np1) / (np1 * (b - a))\n    mu_sum = mu[:, 1:].sum(axis=0)\n    mu = mu[:, :max_power]\n    idx = np.add.outer(np.arange(max_power), np.arange(max_power))\n    c = mu_sum[idx] - mu.T @ mu\n    return mu, c\n\n\naugmentation_moments.append((age_cols, age_moments))\n\nAnd finally, with probability 50%, the geo cluster is changed to one of it’s neighbors.\n\ndef geo_cluster_moments(X: pd.DataFrame) -&gt; tuple[Array, Array]:\n    p = 0.5\n    adj_mat = kmeans_transformer.clusters_adjacency_matrix()\n    P = scipy.sparse.eye(adj_mat.shape[0]) * p + (adj_mat / adj_mat.sum(axis=1)) * (\n        1 - p\n    )  # transition probabilities matrix\n    M = scipy.sparse.csr_array(X.values) @ P\n    # https://en.wikipedia.org/wiki/Multinomial_distribution#Matrix_notation\n    R = scipy.sparse.diags(M.sum(axis=0)) - M.T @ M\n    return M.toarray(), R.toarray()\n\n\naugmentation_moments.append((kmeans_transformer.column_names(), geo_cluster_moments))\n\nLe’t fit the augmented model and see how we did:\n\naugmented_linear = AugmentedLinearModel(augmentation_moments)\nevaluate_model(augmented_linear)\n\nr2_train=0.903, r2_test=0.882\n\n\nWe managed to improve the test accuracy, and reduce overfit."
  },
  {
    "objectID": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#beyond-least-squares",
    "href": "posts/augmentation_is_regularization copy/augmentation_is_regularization.html#beyond-least-squares",
    "title": "Augmentation is Regularization 2",
    "section": "Beyond least squares",
    "text": "Beyond least squares\nIs it possible to extend the result to models that use a non-quadratic loss (e.g logistic regression)? Well the proof heavily relies on that, so probably not, but let’s if we can at least can an approximate result using a 2nd order taylor approximation for the loss.\nThe goal is to (approximately) express \\[\\begin{align*}\n    \\mathrm{E}  \n    \\left[\n    \\sum_{i=0} ^{n-1} l \\left(\n    a\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n    \\right)\n    \\right],\n\\end{align*}\\] as a sum of a non-augmented loss term, and a regularization term. Here, \\(l(\\hat{y}\\,;\\,y)\\) measures how bad is the prediction \\(\\hat{y}\\), given the true value \\(y\\) (the loss).\nFor example, for logistic regression we use the logistic loss \\[\nl(\\hat{y}; y) = \\log \\left( 1 + \\exp \\left(-y \\, \\hat{y} \\right) \\right)\n\\] (with \\(y \\in \\{ -1, 1 \\}\\)).\nLet’s expand \\(l \\left(\na\\left(x_i, p_i\\right) ^T \\theta\\,;\\,y_i\n\\right)\\) around \\(\\mu_i ^T \\theta\\) and simplify: \\[\\begin{align*}\n\\mathrm{E}  \n\\left[\n\\sum_{i=0} ^{n-1} l \\left(\na\\left(x_i, p_i\\right) ^T \\theta \\,;\\, y_i\n\\right)\n\\right]\n\\approx\n\\sum_{i=0} ^{n-1} l(\\mu_i ^T \\theta\\,;\\,y_i) + \\frac{1}{2}  l'' \\left( \\mu_i ^T \\theta\\,;\\,y_i \\right) \\theta^T R \\theta\n\\end{align*}\\] (the order-1 term vanishes as it has zero mean, similar to the delta method).\nSo like in the least squares case, in the loss term we just replace each \\(x\\) with it’s mean. But, the regularization term is not quadratic, since we have the second derivative factor which is not constant (unless the loss is quadratic…).\nI use this result to tell myself that it is ok to select an \\(R\\) for a quadratic regularization based on the covariance of an augmentation, as long as the covariance is small (usually correct for augmentations), and \\(l''\\) is bounded (correct for logistic regression)."
  }
]